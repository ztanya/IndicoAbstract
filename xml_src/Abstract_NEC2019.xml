<?xml version="1.0" encoding="utf-8"?><AbstractBook>
<Conference>
Symposium on Nuclear Electronics and Computing - NEC'2019
</Conference>
<abstract>
<Id>110</Id>
<Title>
Modernization of neutron Fourier chopper for High-resolution Fourier diffractometer (HRFD)
</Title>
<Content>
The High-resolution Fourier diffractometer (HRFD) is operated at the pulsed reactor IBR-2 of FLNP JINR allowing to carry out precision research on the crystal structure and microstructure of inorganic materials. The use of the fast Fourier chopper both for intensity modulation of the primary neutron beam and the correlation method of diffraction data accumulation is the principal feature of the HRFD design. This allows to obtain extremely high resolution (Δd/d ≈ 0.001) at HRFD in a wide range of interplanar distances at a relatively short flight distance from the chopper to the sample position (L = 20 m). In 2016 the old Fourier-chopper (the operation period ~20 years) was replaced with a new one manufactured by the Mirrotron Ltd company (Hungary). The basic mechanical characteristics of the previous version of the Fourier chopper, particularly, the rotor diameter, the number of slits, the slit length, the slit width at the middle, the absorbing material Gd2O3 and the width of the layer Gd2O3, have been maintained in the new Fourier chopper for HRFD. The rotor is produced from the high-strength Al based alloy and allows maximum rotation speed of 6000 rpm. As compared to the previous version, the rotor and the stator are installed in a hermetic casing, the mechanical design of the stator allows to provide the exact configuration and fixation of the pick-up signal phase, a new type of incremental magnetic pickup sensor of the chopper disk rotation speed is applied with interpolation factor of 2 instead of 8, the rotor vacuum and vibration monitoring sensors are installed, a new control system for stator position is used. The new pick-up signal sensor and control system allowed to decrease the differential nonlinearity of the rotor instant speed up to ~2.5%. The chopper control and monitoring system based on the software logic controller Omron provides the predefined law of change in the Fourier-chopper rotation speed and monitors the readings of vacuum, vibration and temperature control sensors.
</Content>
<field id="content">
The High-resolution Fourier diffractometer (HRFD) is operated at the pulsed reactor IBR-2 of FLNP JINR allowing to carry out precision research on the crystal structure and microstructure of inorganic materials. The use of the fast Fourier chopper both for intensity modulation of the primary neutron beam and the correlation method of diffraction data accumulation is the principal feature of the HRFD design. This allows to obtain extremely high resolution (Δd/d ≈ 0.001) at HRFD in a wide range of interplanar distances at a relatively short flight distance from the chopper to the sample position (L = 20 m). In 2016 the old Fourier-chopper (the operation period ~20 years) was replaced with a new one manufactured by the Mirrotron Ltd company (Hungary). The basic mechanical characteristics of the previous version of the Fourier chopper, particularly, the rotor diameter, the number of slits, the slit length, the slit width at the middle, the absorbing material Gd2O3 and the width of the layer Gd2O3, have been maintained in the new Fourier chopper for HRFD. The rotor is produced from the high-strength Al based alloy and allows maximum rotation speed of 6000 rpm. As compared to the previous version, the rotor and the stator are installed in a hermetic casing, the mechanical design of the stator allows to provide the exact configuration and fixation of the pick-up signal phase, a new type of incremental magnetic pickup sensor of the chopper disk rotation speed is applied with interpolation factor of 2 instead of 8, the rotor vacuum and vibration monitoring sensors are installed, a new control system for stator position is used. The new pick-up signal sensor and control system allowed to decrease the differential nonlinearity of the rotor instant speed up to ~2.5%. The chopper control and monitoring system based on the software logic controller Omron provides the predefined law of change in the Fourier-chopper rotation speed and monitors the readings of vacuum, vibration and temperature control sensors.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Zernin</FamilyName>
<Email>nikolay-zernin@yandex.ru</Email>
<Affiliation>
JINR, FLNP, Department of Spectrometers Complex (DSC)
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Anatoly</FirstName>
<FamilyName>Balagurov</FamilyName>
<Email>bala@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Balagurov</FamilyName>
<Email>dbala@ya.ru</Email>
<Affiliation>JINR, FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Bobrikov</FamilyName>
<Email>bobrikov@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Svetlana</FirstName>
<FamilyName>Murashkevich</FamilyName>
<Email>svetlana@nf.jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Sirotin</FamilyName>
<Email>sirotin@nf.jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Tatiana</FirstName>
<FamilyName>Petuhova</FamilyName>
<Email>petukhova@jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valery</FirstName>
<FamilyName>Zhuravlev</FamilyName>
<Email>zhur@nf.jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Gyula</FirstName>
<FamilyName>Vasvari</FamilyName>
<Email>vasvarigy@mirrotron.hu</Email>
<Affiliation>Mirrotron Ltd, Hungary, Budapest</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Gabor</FirstName>
<FamilyName>Szasz</FamilyName>
<Email>szaszg@mirrotron.hu</Email>
<Affiliation>Mirrotron Ltd, Hungary, Budapest</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nikolay</FirstName>
<FamilyName>Zernin</FamilyName>
<Email>nikolay-zernin@yandex.ru</Email>
<Affiliation>
JINR, FLNP, Department of Spectrometers Complex (DSC)
</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>111</Id>
<Title>
The automation of neutron activation analysis at IBR-2 reactor
</Title>
<Content>
The automation of neutron activation analysis at IBR-2 reactor M.V. FRONTASYEVA, S.S. PAVLOV*, B.D.RUMYANTSEV, A.YU. DMITRIEV Department of Neutron Activation Analysis and Applied Research Division of Nuclear Physics, Frank Laboratory of Neutron Physics Joint Institute for Nuclear Research, str. Joliot-Curie, 6, Dubna, Russian Federation *Correspondence: pavlov@nf.jinr.ru The automation of neutron activation analysis (NAA) in the Joint Institute for Nuclear Research using the installation REGATA at the reactor IBR-2 of the Frank Laboratory of Neutron Physics, JINR, Dubna, RF is described. The database for acquisition of the information about all steps of NAA and statistical analysis of obtained results are presented. The construction of sample changer and software for automation of spectra measurement were developed and three sample changers were assembled. The program for quantitative determination of elemental content in samples, some additional programs and structure chart of software are described. Automation of Quality Control (QC) procedures is integrated in the software developed. Details of the design are shown. The application of new software make it possible to use electronic circulation of documents. It is very comfortably taking into account a large distance between laboratories for sample preparation, irradiation and analysis of spectra. Key-Words: neutron activation analysis, sample changer, automation, software, concentration
</Content>
<field id="content">
The automation of neutron activation analysis at IBR-2 reactor M.V. FRONTASYEVA, S.S. PAVLOV*, B.D.RUMYANTSEV, A.YU. DMITRIEV Department of Neutron Activation Analysis and Applied Research Division of Nuclear Physics, Frank Laboratory of Neutron Physics Joint Institute for Nuclear Research, str. Joliot-Curie, 6, Dubna, Russian Federation *Correspondence: pavlov@nf.jinr.ru The automation of neutron activation analysis (NAA) in the Joint Institute for Nuclear Research using the installation REGATA at the reactor IBR-2 of the Frank Laboratory of Neutron Physics, JINR, Dubna, RF is described. The database for acquisition of the information about all steps of NAA and statistical analysis of obtained results are presented. The construction of sample changer and software for automation of spectra measurement were developed and three sample changers were assembled. The program for quantitative determination of elemental content in samples, some additional programs and structure chart of software are described. Automation of Quality Control (QC) procedures is integrated in the software developed. Details of the design are shown. The application of new software make it possible to use electronic circulation of documents. It is very comfortably taking into account a large distance between laboratories for sample preparation, irradiation and analysis of spectra. Key-Words: neutron activation analysis, sample changer, automation, software, concentration
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Pavlov</FamilyName>
<Email>pavlov@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Marina</FirstName>
<FamilyName>Frontasyeva</FamilyName>
<Email>marin@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Boris</FirstName>
<FamilyName>Rumyantsev</FamilyName>
<Email>bdrum@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Dmitriev</FamilyName>
<Email>dmitriev@sunse.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Pavlov</FamilyName>
<Email>pavlov@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>112</Id>
<Title>
Modernization of the Management and Control System for the Cold Neutron Moderator at the Fast Pulsed Reactor
</Title>
<Content>
The management and control system of the cold neutron moderator allows the engineering staff to monitor, in the process of its operation, the main parameters of the moderator, including the gas blower rotation speed, the consumption and temperature of helium, the vacuum in the jacket, and movement of pellets in the transport pipe. Today, complex upgrading of the cold neutron moderator at the fast pulsed reactor «IBR-2M» is under way. The paper presents the current version of the structure of the management and control system for the cold moderator. Interface convertors are used to connect the management and control equipment of the cold moderator to the computer. The main interface of the data acquisition and control system of the executive devices is RS-485. Specialized software has been created to operate the system of management and control of the cold neutron moderator.
</Content>
<field id="content">
The management and control system of the cold neutron moderator allows the engineering staff to monitor, in the process of its operation, the main parameters of the moderator, including the gas blower rotation speed, the consumption and temperature of helium, the vacuum in the jacket, and movement of pellets in the transport pipe. Today, complex upgrading of the cold neutron moderator at the fast pulsed reactor «IBR-2M» is under way. The paper presents the current version of the structure of the management and control system for the cold moderator. Interface convertors are used to connect the management and control equipment of the cold moderator to the computer. The main interface of the data acquisition and control system of the executive devices is RS-485. Specialized software has been created to operate the system of management and control of the cold neutron moderator.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexey</FirstName>
<FamilyName>Altynov</FamilyName>
<Email>altbady@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Maxim</FirstName>
<FamilyName>Bulavin</FamilyName>
<Email>bulavin85@inbox.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Sitotin</FamilyName>
<Email>sirotin@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexey</FirstName>
<FamilyName>Altynov</FamilyName>
<Email>altbady@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>113</Id>
<Title>
Calculation of efficiency of cylindrical thermal neutron counter assemblies
</Title>
<Content>
Cylindrical proportional counter assemblies are the main tool for observing neutron fluxes on many spectrometers. Optimization of geometric parameters of assemblies is of interest from the point of view of increasing homogeneity of efficiency and simplifying the design of the detector system. The calculation of efficiency of different variants of assembly designs consisting of 4 and 5 Helium-4-1 type counters has been carried out in the paper. GEANT-4 package has been used to simulate the operation of the modules designed to replace the old counters of the spectrometer NERA. The calculations have been compared with the experimental results.
</Content>
<field id="content">
Cylindrical proportional counter assemblies are the main tool for observing neutron fluxes on many spectrometers. Optimization of geometric parameters of assemblies is of interest from the point of view of increasing homogeneity of efficiency and simplifying the design of the detector system. The calculation of efficiency of different variants of assembly designs consisting of 4 and 5 Helium-4-1 type counters has been carried out in the paper. GEANT-4 package has been used to simulate the operation of the modules designed to replace the old counters of the spectrometer NERA. The calculations have been compared with the experimental results.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Aleksey</FirstName>
<FamilyName>Kurilkin</FamilyName>
<Email>akurilkin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Churakov</FamilyName>
<Email>churakov@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anton</FirstName>
<FamilyName>Glazkov</FamilyName>
<Email>xa08942870@student.karazin.ua</Email>
<Affiliation>V. N. Karazin Kharkiv National University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Aleksey</FirstName>
<FamilyName>Kurilkin</FamilyName>
<Email>akurilkin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>114</Id>
<Title>
The New Data Acquisition System MPD-32 for the High-Resolution Fourier Diffractometer at the IBR-2 Pulsed Reactor
</Title>
<Content>
In the Laboratory of Neutron Physics a new high-performance data acquisition system (DAQ) is being developed in the framework of the project on creation of a high-aperture backscattering detector (BSD) for the high-resolution Fourier diffractometer HRFD. The designed increase in the BSD aperture of 12.5 times together with an increase in the neutron flux on the sample by a factor of 2-3 due to employment of the new neutron guide demand for raising the neutron registration rate to ~ 3*107 n/s [1]. In addition to signals from the multielement scintillation detector BSD, time encoders also digitize pick-up signals from the chopper as well as of reactor startups that are transmitted to the computer in the list mode to be recorded on the disk for further processing. This has required development of new electronics and programs as the MPD-240-based DAQ system used today has the neutron registration limit on the level of ~ 106 n / s. Earlier, in order to increase the transmission capacity of the data acquisition systems with a USB-2 interface for the IBR-2 spectrometers, the FLINK USB 3.0 was developed [2] to provide links between the modules having an optical interface with a computer according to the USB 3.0 protocol. This has solved the problem of increasing the performance of the DAQ systems for all the spectrometers except those for the HRFD that has undergone modernization. This work presents the results of development of a high-performance data acquisition system on the basis of MPD-32 blocks integrated into a common system of a high-speed interblock interface and an USB 3.0 computer interface with an optical fiber extender.
</Content>
<field id="content">
In the Laboratory of Neutron Physics a new high-performance data acquisition system (DAQ) is being developed in the framework of the project on creation of a high-aperture backscattering detector (BSD) for the high-resolution Fourier diffractometer HRFD. The designed increase in the BSD aperture of 12.5 times together with an increase in the neutron flux on the sample by a factor of 2-3 due to employment of the new neutron guide demand for raising the neutron registration rate to ~ 3*107 n/s [1]. In addition to signals from the multielement scintillation detector BSD, time encoders also digitize pick-up signals from the chopper as well as of reactor startups that are transmitted to the computer in the list mode to be recorded on the disk for further processing. This has required development of new electronics and programs as the MPD-240-based DAQ system used today has the neutron registration limit on the level of ~ 106 n / s. Earlier, in order to increase the transmission capacity of the data acquisition systems with a USB-2 interface for the IBR-2 spectrometers, the FLINK USB 3.0 was developed [2] to provide links between the modules having an optical interface with a computer according to the USB 3.0 protocol. This has solved the problem of increasing the performance of the DAQ systems for all the spectrometers except those for the HRFD that has undergone modernization. This work presents the results of development of a high-performance data acquisition system on the basis of MPD-32 blocks integrated into a common system of a high-speed interblock interface and an USB 3.0 computer interface with an optical fiber extender.
</field>
<field id="summary">
In the Laboratory of Neutron Physics a new high-performance data acquisition system (DAQ) is being developed in the framework of the project on creation of a high-aperture backscattering detector (BSD) for the high-resolution Fourier diffractometer HRFD. The designed increase in the BSD aperture of 12.5 times together with an increase in the neutron flux on the sample by a factor of 2-3 due to employment of the new neutron guide demand for raising the neutron registration rate to ~ 3*107 n/s [1]. In addition to signals from the multielement scintillation detector BSD, time encoders also digitize pick-up signals from the chopper as well as of reactor startups that are transmitted to the computer in the list mode to be recorded on the disk for further processing. This has required development of new electronics and programs as the MPD-240-based DAQ system used today has the neutron registration limit on the level of ~ 106 n / s. Earlier, in order to increase the transmission capacity of the data acquisition systems with a USB-2 interface for the IBR-2 spectrometers, the FLINK USB 3.0 was developed [2] to provide links between the modules having an optical interface with a computer according to the USB 3.0 protocol. This has solved the problem of increasing the performance of the DAQ systems for all the spectrometers except those for the HRFD that has undergone modernization. This work presents the results of development of a high-performance data acquisition system on the basis of MPD-32 blocks integrated into a common system of a high-speed interblock interface and an USB 3.0 computer interface with an optical fiber extender. References [1]	A. Balagurov et al. «High-resolution neutron Fourier diffractometer at the IBR-2 pulsed reactor: A new concept». Nuclear Inst. and Methods in Physics Research B 436 (2018) 263–271. [2]	Shvetsov V.V., V.A. Drozdov. "Increasing Bandwidth of Data Acquisition Systems on IBR-2 Reactor Spectrometers in FLNP". Proceedings of the XXVI International Symposium on Nuclear Electronics & Computing (NEC’2017) Becici, Budva, Montenegro, September 25 - 29, 2017, European repository of the CEUR Workshop Proceedings Vol-2023, pp. 293-298.
</field>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Drozdov</FamilyName>
<Email>drozdov@jinr.ru</Email>
<Affiliation>JINR, FLNP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Svetlana</FirstName>
<FamilyName>Murashkevich</FamilyName>
<Email>svetlana@nf.jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Valentin</FirstName>
<FamilyName>Prikhodko</FamilyName>
<Email>prikh@nf.jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Aleksey</FirstName>
<FamilyName>Bogdzel</FamilyName>
<Email>abogdz@nf.jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vasilii</FirstName>
<FamilyName>Shvetsov</FamilyName>
<Email>shvetc_vas@mail.ru</Email>
<Affiliation>FLNP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vasilii</FirstName>
<FamilyName>Shvetsov</FamilyName>
<Email>shvetc_vas@mail.ru</Email>
<Affiliation>FLNP</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>115</Id>
<Title>
Front-End Electronics for TPC/MPD detector of NICA project
</Title>
<Content>
Time Projection Chamber (TPC) is the main tracker of the Multi-Purpose Detector (MPD). The detector will operate at one of beam interaction points of the collider NICA (Nuclotron-based Ion Collider fAcility) and it is optimized to investigate heavy-ion collisions in the energy range from 4 to 11 GeV/n. The TPC Front-End Electronics (FEE) will operate with event rate up to 7 kHz at average luminosity 1027 cm-2s-1 for gold collisions at 9 GeV/n. The FEE is based on the novel ASIC SAMPA, FPGAs and high-speed serial links. Each of 24 readout chambers will serve by 62 Front-End Cards (FECs) and one Readout and Control Unit (RCU). The whole system will contain 1488 FECs, 24 RCUs which gives us 95232 registration channels. The report presents current status of the FEE and results of the FEC testing.
</Content>
<field id="content">
Time Projection Chamber (TPC) is the main tracker of the Multi-Purpose Detector (MPD). The detector will operate at one of beam interaction points of the collider NICA (Nuclotron-based Ion Collider fAcility) and it is optimized to investigate heavy-ion collisions in the energy range from 4 to 11 GeV/n. The TPC Front-End Electronics (FEE) will operate with event rate up to 7 kHz at average luminosity 1027 cm-2s-1 for gold collisions at 9 GeV/n. The FEE is based on the novel ASIC SAMPA, FPGAs and high-speed serial links. Each of 24 readout chambers will serve by 62 Front-End Cards (FECs) and one Readout and Control Unit (RCU). The whole system will contain 1488 FECs, 24 RCUs which gives us 95232 registration channels. The report presents current status of the FEE and results of the FEC testing.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Stepan</FirstName>
<FamilyName>Vereschagin</FamilyName>
<Email>vereschagin@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Stepan</FirstName>
<FamilyName>Vereschagin</FamilyName>
<Email>vereschagin@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>116</Id>
<Title>
Stand for the investigation radiation hardness of the plastic scintillators and reflectors
</Title>
<Content>
Created the experimental stand for the investigation radiation hardness of the plastic scintillators. Studied two types on polystyrene based samples (UPS-923A and SCSN-81) and two types of polyvinyltoluene based samples (BC-408 and EJ-260). Studied the radiation damage of ESR and Tyvek reflectors, Paint+TiO2 and PMS+TiO2 coatings.
</Content>
<field id="content">
Created the experimental stand for the investigation radiation hardness of the plastic scintillators. Studied two types on polystyrene based samples (UPS-923A and SCSN-81) and two types of polyvinyltoluene based samples (BC-408 and EJ-260). Studied the radiation damage of ESR and Tyvek reflectors, Paint+TiO2 and PMS+TiO2 coatings.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Valentin</FirstName>
<FamilyName>Ustinov</FamilyName>
<Email>ustinov@jinr.ru</Email>
<Affiliation>JINR VBLHEP</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Evgeni</FirstName>
<FamilyName>Sukhov</FamilyName>
<Email>suhov@jinr.ru</Email>
<Affiliation>LHE JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Sergei</FirstName>
<FamilyName>Afanasiev</FamilyName>
<Email>afanasev@lhe.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Valentin</FirstName>
<FamilyName>Ustinov</FamilyName>
<Email>ustinov@jinr.ru</Email>
<Affiliation>JINR VBLHEP</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>117</Id>
<Title>
Distributed control and monitoring tools at LU-20 and HILAC complexes.
</Title>
<Content>
The TANGO control system is chosen as the main platform for developing control software at the Nuclotron. The experimental setup of the TANGO system was successfully tested during the runs of the existing accelerator complex. The report describes hardware, server and client software modules for data acquisition and equipment management at LU-20 and HILAC linear accelerators. Universal web clients were developed for management of equipment groups. The data is transferred is a single stream for each group of equipment. The client layer interacts with TANGO control system via standard http and WebSocket protocols. It allows to significantly expand the choice of programming language for writing the client software. The TANGO device server WebSocketDS was developed for data exchange via WebSocket protocol. Various JavaScript libraries and frameworks were used for the client layer development, such as Angular, ReactJS, ExtJs and few others. They allow to create cross-platform client web applications for the control systems. JavaScript framework Electron was used for creating standard desktop applications.
</Content>
<field id="content">
The TANGO control system is chosen as the main platform for developing control software at the Nuclotron. The experimental setup of the TANGO system was successfully tested during the runs of the existing accelerator complex. The report describes hardware, server and client software modules for data acquisition and equipment management at LU-20 and HILAC linear accelerators. Universal web clients were developed for management of equipment groups. The data is transferred is a single stream for each group of equipment. The client layer interacts with TANGO control system via standard http and WebSocket protocols. It allows to significantly expand the choice of programming language for writing the client software. The TANGO device server WebSocketDS was developed for data exchange via WebSocket protocol. Various JavaScript libraries and frameworks were used for the client layer development, such as Angular, ReactJS, ExtJs and few others. They allow to create cross-platform client web applications for the control systems. JavaScript framework Electron was used for creating standard desktop applications.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Elkin</FamilyName>
<Email>elkin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitriy</FirstName>
<FamilyName>Ponkin</FamilyName>
<Email>ponkin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Georgy</FirstName>
<FamilyName>Sedykh</FamilyName>
<Email>egor@dubna.tk</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Gorbachev</FamilyName>
<Email>egorbe@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Elkin</FamilyName>
<Email>elkin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>118</Id>
<Title>
Present status and main directions of the JINR cloud development
</Title>
<Content>
The JINR cloud grows not only in terms of amount of resources but in the number of activities they are used for: COMPASS production system services, data management system of the UNECE ICP Vegetation, service for diseases detection of agricultural crops through the use of advanced machine learning approaches, service for scientific and engineering computations, service for data visualization based on Grafana, jupyterhub head and execute nodes for it, gitlab and its runners as well as some other. All these topics are covered in details.
</Content>
<field id="content">
The JINR cloud grows not only in terms of amount of resources but in the number of activities they are used for: COMPASS production system services, data management system of the UNECE ICP Vegetation, service for diseases detection of agricultural crops through the use of advanced machine learning approaches, service for scientific and engineering computations, service for data visualization based on Grafana, jupyterhub head and execute nodes for it, gitlab and its runners as well as some other. All these topics are covered in details.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Baranov</FamilyName>
<Email>baranov@jinr.ru</Email>
<Affiliation>(JINR)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yelena</FirstName>
<FamilyName>Mazhitova</FamilyName>
<Email>emazhitova@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Roman</FirstName>
<FamilyName>Semenov</FamilyName>
<Email>roman@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>119</Id>
<Title>
Development and Integration of the Electronic Logbook for the BM@N experiment at NICA
</Title>
<Content>
The acquisition of experimental data is an integral part of all modern high-energy physics experiments. During experiment sessions, not only the data collected from the detectors are important for understanding the produced events, but also the records in logbooks that are written by the shift crew and describe operating modes of various systems and detectors and different types of events. The report shows a new electronic logbook developed to automate the latter process in the BM@N experiment, a fixed target experiment of the first stage of the NICA project at the Joint Institute for Nuclear Research. The online electronic logbook allows collaboration members during experiment runs to record information on current events, states of various systems, operation conditions of detectors and many others which are further used in the processing and physics analysis of the particle collision events. The system provides users with tools for convenient viewing, transparent managing and searching for the required information in the logbook. The specialized Web-interface and application programming interface for storing and accessing these data are considered. The important task of integrating the online electronic logbook with the central experiment database is also shown. The implementation of such information system is a necessary step for the successful future operation of the BM@N experiment.
</Content>
<field id="content">
The acquisition of experimental data is an integral part of all modern high-energy physics experiments. During experiment sessions, not only the data collected from the detectors are important for understanding the produced events, but also the records in logbooks that are written by the shift crew and describe operating modes of various systems and detectors and different types of events. The report shows a new electronic logbook developed to automate the latter process in the BM@N experiment, a fixed target experiment of the first stage of the NICA project at the Joint Institute for Nuclear Research. The online electronic logbook allows collaboration members during experiment runs to record information on current events, states of various systems, operation conditions of detectors and many others which are further used in the processing and physics analysis of the particle collision events. The system provides users with tools for convenient viewing, transparent managing and searching for the required information in the logbook. The specialized Web-interface and application programming interface for storing and accessing these data are considered. The important task of integrating the online electronic logbook with the central experiment database is also shown. The implementation of such information system is a necessary step for the successful future operation of the BM@N experiment.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Gertsenberger</FamilyName>
<Email>gertsen@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Moshkin</FamilyName>
<Email>andrem@thsun1.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>alexandr</FirstName>
<FamilyName>chebotov</FamilyName>
<Email>lokzzzor@gmail.com</Email>
<Affiliation>lit</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Konstantin</FirstName>
<FamilyName>Gertsenberger</FamilyName>
<Email>gertsen@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>Research Data Infrastructures</Track>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
</abstract>
<abstract>
<Id>120</Id>
<Title>
Methodical aspects of training data scientists using the Data GRID in a virtual computer lab environment
</Title>
<Content>
1. Introduction The tasks of distributed data storage and processing, data mining and mathematical modeling based on these data are priorities within the agenda of the digital economy development program in the Russian Federation. Today it is very important to train data scientists that serve as the bridge between cutting-edge technology and digital economy needs. It is important to teach them to improve access to big data, analytics tools, and innovative research methods. They also should be able to design and deploy Data GRID clusters use and advise on such tools as machine learning, natural language processing, web scraping, big data platforms, and data visualization techniques and their application to relevant business needs and public policy issues. The aim of an innovative IT education is the possibility of training specialists who can effectively solve such problems as conducting researches that explores methods to harness technology and innovation to advance equity, mobility and inclusion in cities, building innovative data products and pipelines to produce novel data that can help tackle pressing issues from a new angle, building systems and processes to collect, analyze, and combine multiple sources of data in novel ways. 2. Hardware and software tools for educational process In order to provide students with the opportunity to design the Data GRID cluster for personal researches in the field of data analysis and mathematical modeling, we decided to replace physical computers with virtual machines in the virtual computer lab, which was established at the Institute of System Analysis and Control since 2007 by our associate professor M. Belov. The virtual computer lab (VCL) provides a set of software and hardware-based virtualization and containerizations tools that enable the flexible and on-demand provision and use of computing resources in the form of cloud Internet services with integrated knowledge management system based on the principles of self-organization, functioning as a homogeneous environment with elements of cognitive representation of internal operational resources based on visual models and partial automation of basic technological operations with expert system for carrying out research projects, resource-intensive computational calculations and tasks related to the development of complex corporate and other distributed information systems. The service also provides dedicated virtual servers for innovative projects that are carried out by students and staff at the Institute of System Analysis and Control. The main features of a virtual computer lab are the principles of self-organization, which make the transition from a complex system of granular group security policies with a large number of restrictions to the formation of personal responsibility and respect for colleagues, which should be a solid foundation for strengthening and developing classical cultural values in the educational environment. [1-9] Data GRID is the general term which utilize the multiple sites or clusters for distribute the processing and storage among them, so the Hadoop HDFS is a method or a way for data grid implementation since many other Hadoop frameworks like MapReduce or Spark used for distributed data processing. Traditional GRID computing is a processor architecture that combines computer resources from various domains to reach a main objective. In grid computing, the computers on the network can work on a task together, thus functioning as a supercomputer. Another way to look at is that GRID computing is now the traditional high-performance system with a flavor of MPI. [10] We look at GRID as a distributed system concept – a way to use computers distributed over a network to solve a problem. GRID is a group of physical machines connected to make a GRID Computer and Hadoop is the software running on these machines, therefore Hadoop is a subset of Grid computing. In order to provide the ability to quickly deploy Data GRID clusters, we added new blade servers (to optimize the space they occupy in a server room) with SSD disks of increased wear resistance and increased RAM. In order to minimize costs, we use the VMware vCenter technology platform with an integrated set of proprietary software tools, for the productive implementation of educational tasks. 3. The advantages of using the virtual computer lab in the educational process The organization of an effective educational process for the goal-directed training of IT experts has demanded a speedy solution to the following problems: an often insufficient number of classroom hours for students to cover a necessary and sufficient set of practical exercises that help students learn complex information systems; on a typical personal computer with average capabilities it is impossible to get real practical experience working with multi-component Data GRID cluster because the hardware requirements for such systems often go beyond what is offered on typical home, office and laptop computers; the single-user license cost for some software components or professional technical support is too high, and in most cases, such a license is required only for the duration of the learning process. The training of the «consumers» should be cut off in the process of the IT specialists’ education, and we should spare no effort to training of the «creative doers». For this purpose, it is important to study the ways of creating the information systems from the scratch, paying attention to the configuring and adjustment of the equipment, connection and integration of all the necessary parts of the system without any help, and only after that to accomplish issue-oriented tasks. The data scientist of the future is an expert, which has not only the fundamental scientific knowledge, but he is a promising engineer with an outstanding potential and is able to compose and make the capable data analysis solutions suitable for the project. Only the skilled professionals of this level can create the right conditions for the science development and its practical applications at an increasing rate. All above-mentioned problems can be solved in the virtual computer lab, which has become not only the innovative tool for the training of the high skilled IT specialists, but also a demanded space for the technical cooperation between a final-year student and a potential employer. It gives an opportunity to show qualification in real time, and to present the employer's problem in the virtual format and try to solve it together, attracting the young minds and sometimes people with different ways of thinking, for example, the history of the neural network expansion and the idea of calculation of the back propagation errors, using the gradient descent method and so on. A centralized management portal as well as a knowledge management system were created in order to manage the virtual computer laboratory. The need to create such a system was conditioned by the fact that students are able to learn about Data GRID clusters, so it is important to create a social network between all participants as well as to create an environment that allows pupils the opportunity to independently engage in such processes as the identification, acquisition, presentation, and use (distribution) of knowledge without the direct involvement of the instructor. Methods of use (propagation) are directly related to storage methods and, consequently, the technological tools that may be used for the transmission of formal knowledge include knowledge bases with various search functionality; blogs, wikis, and social networks; "Wiki Textbooks" that allow all participants to collaboratively create and update educational content and exchange practical problems (including from real companies); as well as user blogs, forums, and group chat systems. That is why the priority of the university is to create the most favorable conditions for the forming of the professional competence in IT, which will help the graduates to solve a wide range of the tasks, happening during all the stages of the Data GRID development, including the design itself. It is evident that to form the professional competence the students should do the following in order master a lot of literature, do many practical tasks and make research works on the modern data analysis systems, their deployment, maintenance and effective appliance for solving the problem-oriented tasks. The main way to solve these problems has been to create a virtual computer lab that is able to solve the problem of insufficient computing and software resources and to provide an adequate level of technological and methodological support; to teach how to use cutting-edge technologies to work with distributed information systems on the example of Hadoop Data GRID Cluster; to organize group work with educational materials by involving users in the process of improving these materials and allowing them to communicate freely with each other on the basis of self-organizational principles. 4. The results of the educational process Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, beliefs, and habits. Educational methods include storytelling, discussion, teaching, training, and directed research. Technology can enhance relationships between teachers and students. When teachers effectively integrate technology into subject areas, teachers grow into roles of adviser, content expert, and coach. Technology helps make teaching and learning more meaningful and fun. Using Virtual Computer Lab students learn to design and deploy a Data GRID cluster based on Apache Hadoop software using most common topologies (Basic Horizontal topology, Federation topology, Monadic topology, Hierarchical topology, Hybrid Topology), perform basic cluster administration tasks, such as adding or removing hosts and service instances, changing the replication factor, adjusting the amount of allocated memory for execution containers, etc. Learners upload real-world data from various data sources into distributed HDFS file system, perform data rebalancing. Based on the uploaded data, they study the main components of the cluster and most important analytics tools (MapReduce, Spark and utility tools HUE, HCatalog, Hive, Impala, Pig Latin, Sqoop, Solr, Oozie, CDSW). For example, based on several tens of millions posts from technical forum, evaluate the popularity of programming languages, the effectiveness of moderation, the tonality of a statement on a given product, etc. 5. Conclusion The results that we get specialists who can create Data GRID clusters and productively solve problems in corresponding application domains. Their jobs can focus on data management, analytics modeling, and business analysis. Data scientists can be real change-makers within an organization, offering insight that can illuminate the company’s trajectory toward its ultimate business goals. Data scientists are integral to supporting both leaders and developers in creating better products and paradigms. And as their role in big business becomes more and more important, they are in increasingly short supply. The Institute of System Analysis and Control has achieved in improving the educational process represent strategic foundations for overcoming perhaps one of the most acute problems in modern education: the fact that it tends to respond to changes in the external environment weakly and slowly. It should also be emphasized that the virtual computer lab has helped us provide an optimal and sustainable technological, educational-organizational, scientific-methodological, and regulatory-administrative environment for supporting innovative approaches to computer education. It promotes the integration of the scientific and educational potential of Dubna State University and the formation of industry and academic research partnerships with leading companies that are potential employers of graduates of the Institute of System Analysis and Control. References [1] Belov M.A., Kryukov Y.A., Miheev M.A., Lupanov P.E., Tokareva N.A., Cheremisina E.N., Improving the efficiency of mastering distributed information systems in a virtual computer lab based on the use of containerization and container orchestration technologies, Sovremennye informatsionnye tekhnologii i IT-obrazovanie. 2018, T.14. №4. [2] Belov, M.A., Krukov, Y.A., Mikheev, M.A., Tokareva, N.A., Cheremisina, E.N. Essential aspects of it training technology for processing, storage and data mining using the virtual computer lab, CEUR Workshop Proceedings 2267, pp. 207-212, 2018. [3] Belov M.A., Kryukov Y.A., Lupanov P.E., Miheev M.A., Cheremisina E.N., Koncepciya kognitivnogo vzaimodeystviya s virtual'noy komp'yuternoy laboratoriey na osnove vizual'nyh modeley i ehkspertnoy sistemy, Estestvennye i tekhnicheskie nauki, 2018, №10, S. 27-36. [4] Belov M.A., Lupanov P.E., Tokareva N.A., Cheremisina E.N. Kontseptsiya usovershenstvovannoy arhitektury virtual'noy komp'yuternoy laboratorii dlya effektivnogo obucheniya spetsialistov po raspredelennym informatsionnym sistemam razlichnogo naznacheniya i instrumental'nym sredstvam proektirovaniya, Sovremennye informatsionnye tekhnologii i IT-obrazovanie. 2017. T. 13. № 1. S. 182-189. [5] Cheremisina, E.N., Belov, M.A., Tokareva, N.A., Grishko, S.I., Sorokin, A.V. Embedding of containerization technology in the core of the Virtual Computing Lab, CEUR Workshop Proceedings 2023, pp. 299-302, 2018. [6] Belov M.A., Cheremisina E.N., Potemkina S.V., Distance learning through distributed information systems using a virtual computer lab and knowledge management system, Journal of Emerging research and solutions in ICT, 2016. [7] Lishilin M.V., Belov M.A., Tokareva N.A., Sorokin A.V., Kontseptual'naya model' sistemy upravleniya znaniyami dlya formirovaniya professional'nyh kompetentsiy v oblasti IT v srede virtual'noy komp'yuternoy laboratorii, Fundamental'nye issledovaniya. 2015. № 11-5. S. 886-890. [8] Belov M.A., Lishilin M.V., Tokareva N.A., Antipov O.E., Ot virtual'noy komp'yuternoy laboratorii k upravleniyu znaniyami. Itogi i perspektivy, Kachestvo. Innovatsii. Obrazovanie. 2014. № 9 (112). S. 3-14. [9] Cheremisina E.N., Belov M.A., Lishilin M.V., Integratsiya virtual'noy komp'yuternoy laboratorii i znanievogo prostranstva - novyy vzglyad na podgotovku vysokokvalifitsirovannyh it-spetsialistov, Sistemnyy analiz v nauke i obrazovanii. 2014. № 1 (23). S. 97-104. [10] Foster, Ian., Kesselman, Carl The Grid2: Blueprint for a New Computing Infrastructure. — Morgan Kaufmann Publishers. — ISBN ISBN 1-55860-475-8, 2003.
</Content>
<field id="content">
1. Introduction The tasks of distributed data storage and processing, data mining and mathematical modeling based on these data are priorities within the agenda of the digital economy development program in the Russian Federation. Today it is very important to train data scientists that serve as the bridge between cutting-edge technology and digital economy needs. It is important to teach them to improve access to big data, analytics tools, and innovative research methods. They also should be able to design and deploy Data GRID clusters use and advise on such tools as machine learning, natural language processing, web scraping, big data platforms, and data visualization techniques and their application to relevant business needs and public policy issues. The aim of an innovative IT education is the possibility of training specialists who can effectively solve such problems as conducting researches that explores methods to harness technology and innovation to advance equity, mobility and inclusion in cities, building innovative data products and pipelines to produce novel data that can help tackle pressing issues from a new angle, building systems and processes to collect, analyze, and combine multiple sources of data in novel ways. 2. Hardware and software tools for educational process In order to provide students with the opportunity to design the Data GRID cluster for personal researches in the field of data analysis and mathematical modeling, we decided to replace physical computers with virtual machines in the virtual computer lab, which was established at the Institute of System Analysis and Control since 2007 by our associate professor M. Belov. The virtual computer lab (VCL) provides a set of software and hardware-based virtualization and containerizations tools that enable the flexible and on-demand provision and use of computing resources in the form of cloud Internet services with integrated knowledge management system based on the principles of self-organization, functioning as a homogeneous environment with elements of cognitive representation of internal operational resources based on visual models and partial automation of basic technological operations with expert system for carrying out research projects, resource-intensive computational calculations and tasks related to the development of complex corporate and other distributed information systems. The service also provides dedicated virtual servers for innovative projects that are carried out by students and staff at the Institute of System Analysis and Control. The main features of a virtual computer lab are the principles of self-organization, which make the transition from a complex system of granular group security policies with a large number of restrictions to the formation of personal responsibility and respect for colleagues, which should be a solid foundation for strengthening and developing classical cultural values in the educational environment. [1-9] Data GRID is the general term which utilize the multiple sites or clusters for distribute the processing and storage among them, so the Hadoop HDFS is a method or a way for data grid implementation since many other Hadoop frameworks like MapReduce or Spark used for distributed data processing. Traditional GRID computing is a processor architecture that combines computer resources from various domains to reach a main objective. In grid computing, the computers on the network can work on a task together, thus functioning as a supercomputer. Another way to look at is that GRID computing is now the traditional high-performance system with a flavor of MPI. [10] We look at GRID as a distributed system concept – a way to use computers distributed over a network to solve a problem. GRID is a group of physical machines connected to make a GRID Computer and Hadoop is the software running on these machines, therefore Hadoop is a subset of Grid computing. In order to provide the ability to quickly deploy Data GRID clusters, we added new blade servers (to optimize the space they occupy in a server room) with SSD disks of increased wear resistance and increased RAM. In order to minimize costs, we use the VMware vCenter technology platform with an integrated set of proprietary software tools, for the productive implementation of educational tasks. 3. The advantages of using the virtual computer lab in the educational process The organization of an effective educational process for the goal-directed training of IT experts has demanded a speedy solution to the following problems: an often insufficient number of classroom hours for students to cover a necessary and sufficient set of practical exercises that help students learn complex information systems; on a typical personal computer with average capabilities it is impossible to get real practical experience working with multi-component Data GRID cluster because the hardware requirements for such systems often go beyond what is offered on typical home, office and laptop computers; the single-user license cost for some software components or professional technical support is too high, and in most cases, such a license is required only for the duration of the learning process. The training of the «consumers» should be cut off in the process of the IT specialists’ education, and we should spare no effort to training of the «creative doers». For this purpose, it is important to study the ways of creating the information systems from the scratch, paying attention to the configuring and adjustment of the equipment, connection and integration of all the necessary parts of the system without any help, and only after that to accomplish issue-oriented tasks. The data scientist of the future is an expert, which has not only the fundamental scientific knowledge, but he is a promising engineer with an outstanding potential and is able to compose and make the capable data analysis solutions suitable for the project. Only the skilled professionals of this level can create the right conditions for the science development and its practical applications at an increasing rate. All above-mentioned problems can be solved in the virtual computer lab, which has become not only the innovative tool for the training of the high skilled IT specialists, but also a demanded space for the technical cooperation between a final-year student and a potential employer. It gives an opportunity to show qualification in real time, and to present the employer's problem in the virtual format and try to solve it together, attracting the young minds and sometimes people with different ways of thinking, for example, the history of the neural network expansion and the idea of calculation of the back propagation errors, using the gradient descent method and so on. A centralized management portal as well as a knowledge management system were created in order to manage the virtual computer laboratory. The need to create such a system was conditioned by the fact that students are able to learn about Data GRID clusters, so it is important to create a social network between all participants as well as to create an environment that allows pupils the opportunity to independently engage in such processes as the identification, acquisition, presentation, and use (distribution) of knowledge without the direct involvement of the instructor. Methods of use (propagation) are directly related to storage methods and, consequently, the technological tools that may be used for the transmission of formal knowledge include knowledge bases with various search functionality; blogs, wikis, and social networks; "Wiki Textbooks" that allow all participants to collaboratively create and update educational content and exchange practical problems (including from real companies); as well as user blogs, forums, and group chat systems. That is why the priority of the university is to create the most favorable conditions for the forming of the professional competence in IT, which will help the graduates to solve a wide range of the tasks, happening during all the stages of the Data GRID development, including the design itself. It is evident that to form the professional competence the students should do the following in order master a lot of literature, do many practical tasks and make research works on the modern data analysis systems, their deployment, maintenance and effective appliance for solving the problem-oriented tasks. The main way to solve these problems has been to create a virtual computer lab that is able to solve the problem of insufficient computing and software resources and to provide an adequate level of technological and methodological support; to teach how to use cutting-edge technologies to work with distributed information systems on the example of Hadoop Data GRID Cluster; to organize group work with educational materials by involving users in the process of improving these materials and allowing them to communicate freely with each other on the basis of self-organizational principles. 4. The results of the educational process Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, beliefs, and habits. Educational methods include storytelling, discussion, teaching, training, and directed research. Technology can enhance relationships between teachers and students. When teachers effectively integrate technology into subject areas, teachers grow into roles of adviser, content expert, and coach. Technology helps make teaching and learning more meaningful and fun. Using Virtual Computer Lab students learn to design and deploy a Data GRID cluster based on Apache Hadoop software using most common topologies (Basic Horizontal topology, Federation topology, Monadic topology, Hierarchical topology, Hybrid Topology), perform basic cluster administration tasks, such as adding or removing hosts and service instances, changing the replication factor, adjusting the amount of allocated memory for execution containers, etc. Learners upload real-world data from various data sources into distributed HDFS file system, perform data rebalancing. Based on the uploaded data, they study the main components of the cluster and most important analytics tools (MapReduce, Spark and utility tools HUE, HCatalog, Hive, Impala, Pig Latin, Sqoop, Solr, Oozie, CDSW). For example, based on several tens of millions posts from technical forum, evaluate the popularity of programming languages, the effectiveness of moderation, the tonality of a statement on a given product, etc. 5. Conclusion The results that we get specialists who can create Data GRID clusters and productively solve problems in corresponding application domains. Their jobs can focus on data management, analytics modeling, and business analysis. Data scientists can be real change-makers within an organization, offering insight that can illuminate the company’s trajectory toward its ultimate business goals. Data scientists are integral to supporting both leaders and developers in creating better products and paradigms. And as their role in big business becomes more and more important, they are in increasingly short supply. The Institute of System Analysis and Control has achieved in improving the educational process represent strategic foundations for overcoming perhaps one of the most acute problems in modern education: the fact that it tends to respond to changes in the external environment weakly and slowly. It should also be emphasized that the virtual computer lab has helped us provide an optimal and sustainable technological, educational-organizational, scientific-methodological, and regulatory-administrative environment for supporting innovative approaches to computer education. It promotes the integration of the scientific and educational potential of Dubna State University and the formation of industry and academic research partnerships with leading companies that are potential employers of graduates of the Institute of System Analysis and Control. References [1] Belov M.A., Kryukov Y.A., Miheev M.A., Lupanov P.E., Tokareva N.A., Cheremisina E.N., Improving the efficiency of mastering distributed information systems in a virtual computer lab based on the use of containerization and container orchestration technologies, Sovremennye informatsionnye tekhnologii i IT-obrazovanie. 2018, T.14. №4. [2] Belov, M.A., Krukov, Y.A., Mikheev, M.A., Tokareva, N.A., Cheremisina, E.N. Essential aspects of it training technology for processing, storage and data mining using the virtual computer lab, CEUR Workshop Proceedings 2267, pp. 207-212, 2018. [3] Belov M.A., Kryukov Y.A., Lupanov P.E., Miheev M.A., Cheremisina E.N., Koncepciya kognitivnogo vzaimodeystviya s virtual'noy komp'yuternoy laboratoriey na osnove vizual'nyh modeley i ehkspertnoy sistemy, Estestvennye i tekhnicheskie nauki, 2018, №10, S. 27-36. [4] Belov M.A., Lupanov P.E., Tokareva N.A., Cheremisina E.N. Kontseptsiya usovershenstvovannoy arhitektury virtual'noy komp'yuternoy laboratorii dlya effektivnogo obucheniya spetsialistov po raspredelennym informatsionnym sistemam razlichnogo naznacheniya i instrumental'nym sredstvam proektirovaniya, Sovremennye informatsionnye tekhnologii i IT-obrazovanie. 2017. T. 13. № 1. S. 182-189. [5] Cheremisina, E.N., Belov, M.A., Tokareva, N.A., Grishko, S.I., Sorokin, A.V. Embedding of containerization technology in the core of the Virtual Computing Lab, CEUR Workshop Proceedings 2023, pp. 299-302, 2018. [6] Belov M.A., Cheremisina E.N., Potemkina S.V., Distance learning through distributed information systems using a virtual computer lab and knowledge management system, Journal of Emerging research and solutions in ICT, 2016. [7] Lishilin M.V., Belov M.A., Tokareva N.A., Sorokin A.V., Kontseptual'naya model' sistemy upravleniya znaniyami dlya formirovaniya professional'nyh kompetentsiy v oblasti IT v srede virtual'noy komp'yuternoy laboratorii, Fundamental'nye issledovaniya. 2015. № 11-5. S. 886-890. [8] Belov M.A., Lishilin M.V., Tokareva N.A., Antipov O.E., Ot virtual'noy komp'yuternoy laboratorii k upravleniyu znaniyami. Itogi i perspektivy, Kachestvo. Innovatsii. Obrazovanie. 2014. № 9 (112). S. 3-14. [9] Cheremisina E.N., Belov M.A., Lishilin M.V., Integratsiya virtual'noy komp'yuternoy laboratorii i znanievogo prostranstva - novyy vzglyad na podgotovku vysokokvalifitsirovannyh it-spetsialistov, Sistemnyy analiz v nauke i obrazovanii. 2014. № 1 (23). S. 97-104. [10] Foster, Ian., Kesselman, Carl The Grid2: Blueprint for a New Computing Infrastructure. — Morgan Kaufmann Publishers. — ISBN ISBN 1-55860-475-8, 2003.
</field>
<field id="summary">
This paper discusses methodical aspects of training data scientists using the data grid in a virtual computer lab environment. Data scientists serve as the bridge between cutting-edge technology and digital economy needs. It is important to teach them to improve access to big data, analytics tools, and innovative research methods. They also should be able to design and deploy Data GRID clusters use and advise on such tools as machine learning, natural language processing, web scraping, big data platforms, and data visualization techniques and their application to relevant business needs and public policy issues. Virtual computer lab is a powerful innovative tool for training IT-professionals, created and successfully operated by the experts of the System Analysis and Control Department at the Dubna State University. Keywords: virtual computer lab, virtualization, containerization, Grid, Data Grid, Hadoop, Map Reduce, Spark, HCatalog, Hive, Impala, Solr, Sqoop, Hue, cluster, data mining, distributed systems, mathematical modeling, education, data analytics, IT training, IT education, innovative education.
</field>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Belov</FamilyName>
<Email>belov@uni-dubna.ru</Email>
<Affiliation>Dubna State Univeristy</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nadezhda</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@uni-dubna.ru</Email>
<Affiliation>Dubna Univeristy</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Evgenia</FirstName>
<FamilyName>Cheremisina</FamilyName>
<Email>kirpicheva77@gmail.com</Email>
<Affiliation>
Dubna International University of Nature, Society and Man. Institute of system analysis and management
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Snezhana</FirstName>
<FamilyName>Potemkina</FamilyName>
<Email>snezhik@mail.ru</Email>
<Affiliation>Dubna State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vladimir</FirstName>
<FamilyName>Korenkov</FamilyName>
<Email>korenkov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Innovative IT Education</Track>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>121</Id>
<Title>CompactRIO based Mössbauer spectrometer</Title>
<Content>
This talk reports the development of a new Mössbauer spectrometer based on modular DAQ devices and virtual instrumentation technique. Virtual instrumentation is new technology, which is used for development of measurement and test systems. It allows replacing complex analog circuits with computers and software. Thanks to this technology, it is now possible to easily create complex automatic control systems, that allow you to implement processes with e.g. self-diagnosis or self-setup, and who do not need the immediate presence of an operator because their management may be possible via the Internet or those system can be completely autonomous. Another technology that is becoming increasingly important are FPGA (Field Programmable Gate Arrays). Because of their configurability, determinism and speed have found widespread use in many areas where there is need for great computing power - even in nuclear physics. This talk will deal with combining those two new technologies for creating Mössbauer spectrometer. Mössbauer spectrometery is spectrometric method, which uses Mössbauer effect - recoilless emission and absorbtion of gamma rays by certain nuclei. Developed Mössbauer spectrometer is based on modular industrial computer CompactRIO by National Instruments™. This device has integrated FPGA, which is used for most critical functions, as DAQ, spectra processing and driving of transducer movement. The first part deals with the development of spectrometric application on a CompactRIO, that performs velocity reference signal generation, velocity transducer PID regulation, detector signal acquisition and spectrum registration. The second part deals with PID parameters autotuning using evolution algorithms and additional spectra linearization methods implemented in developed Mössbauer spectrometer.
</Content>
<field id="content">
This talk reports the development of a new Mössbauer spectrometer based on modular DAQ devices and virtual instrumentation technique. Virtual instrumentation is new technology, which is used for development of measurement and test systems. It allows replacing complex analog circuits with computers and software. Thanks to this technology, it is now possible to easily create complex automatic control systems, that allow you to implement processes with e.g. self-diagnosis or self-setup, and who do not need the immediate presence of an operator because their management may be possible via the Internet or those system can be completely autonomous. Another technology that is becoming increasingly important are FPGA (Field Programmable Gate Arrays). Because of their configurability, determinism and speed have found widespread use in many areas where there is need for great computing power - even in nuclear physics. This talk will deal with combining those two new technologies for creating Mössbauer spectrometer. Mössbauer spectrometery is spectrometric method, which uses Mössbauer effect - recoilless emission and absorbtion of gamma rays by certain nuclei. Developed Mössbauer spectrometer is based on modular industrial computer CompactRIO by National Instruments™. This device has integrated FPGA, which is used for most critical functions, as DAQ, spectra processing and driving of transducer movement. The first part deals with the development of spectrometric application on a CompactRIO, that performs velocity reference signal generation, velocity transducer PID regulation, detector signal acquisition and spectrum registration. The second part deals with PID parameters autotuning using evolution algorithms and additional spectra linearization methods implemented in developed Mössbauer spectrometer.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Pavel</FirstName>
<FamilyName>Kohout</FamilyName>
<Email>pvlkohout@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Jiri</FirstName>
<FamilyName>Pechousek</FamilyName>
<Email>jiri.pechousek@upol.cz</Email>
<Affiliation>Palacký University Olomouc</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Lukas</FirstName>
<FamilyName>Kouril</FamilyName>
<Email>lukas.kouril@upol.cz</Email>
<Affiliation>Palacký University Olomouc</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Pavel</FirstName>
<FamilyName>Kohout</FamilyName>
<Email>pvlkohout@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>122</Id>
<Title>
Improvements in the NOvA Detector Simulation based on JINR stand measurements
</Title>
<Content>
NOvA is a long-baseline neutrino experiment aiming to study neutrino oscillation phenomenon in the muon neutrino beam from complex NuMI at Fermilab (USA). Two identical detectors have been built to measure the initial neutrino flux spectra at the near site and the oscillated one at a 810 km distance, which significantly reduces many systematic uncertainties. To improve electron neutrino and neutral current interaction separation, the detector is constructed as a finely segmented structure filled with liquid scintillator. Charged particles lose their energy in the detector materials, producing light signal in a cell which are recorded by readout electronics. The simulation models this using the following chain: a parameterized front-end simulation converts all energy deposits in active material into scintillation light, the scintillation light is transported through an optical fiber to an avalanche photodiode, and the readout electronics simulation models the shaping, digitization, and triggering on the response of the photodiode. Two test stands have been built in JINR (Dubna, Russia) to measure the proton light response of NOvA scintillator and the electronic signal shaping of the NOvA front-end-board. The parameters measured using these test stands have been implemented in the custom NOvA simulation chain.
</Content>
<field id="content">
NOvA is a long-baseline neutrino experiment aiming to study neutrino oscillation phenomenon in the muon neutrino beam from complex NuMI at Fermilab (USA). Two identical detectors have been built to measure the initial neutrino flux spectra at the near site and the oscillated one at a 810 km distance, which significantly reduces many systematic uncertainties. To improve electron neutrino and neutral current interaction separation, the detector is constructed as a finely segmented structure filled with liquid scintillator. Charged particles lose their energy in the detector materials, producing light signal in a cell which are recorded by readout electronics. The simulation models this using the following chain: a parameterized front-end simulation converts all energy deposits in active material into scintillation light, the scintillation light is transported through an optical fiber to an avalanche photodiode, and the readout electronics simulation models the shaping, digitization, and triggering on the response of the photodiode. Two test stands have been built in JINR (Dubna, Russia) to measure the proton light response of NOvA scintillator and the electronic signal shaping of the NOvA front-end-board. The parameters measured using these test stands have been implemented in the custom NOvA simulation chain.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Samoylov</FamilyName>
<Email>samoylov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Samoylov</FamilyName>
<Email>samoylov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>123</Id>
<Title>
Trigger and beam monitoring system of BM@N and SRC experiments
</Title>
<Content>
The report describes a Trigger module control and monitoring system used at experiments BM@N and SRC held in JINR. The system includes both hardware and software and allows to control trigger system including delays setting, discriminator level adjusting and trigger logics selection with few mouse clicks without human access to the electronics. System also provides on-line beam and trigger intensity publishing using web and TSP/IP servers.
</Content>
<field id="content">
The report describes a Trigger module control and monitoring system used at experiments BM@N and SRC held in JINR. The system includes both hardware and software and allows to control trigger system including delays setting, discriminator level adjusting and trigger logics selection with few mouse clicks without human access to the electronics. System also provides on-line beam and trigger intensity publishing using web and TSP/IP servers.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitri</FirstName>
<FamilyName>Bogoslovski</FamilyName>
<Email>bogoslovski.dimson@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Rogov</FamilyName>
<Email>rogovictor@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Yurevich</FamilyName>
<Email>yurevich@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>124</Id>
<Title>
Project of a fast interaction trigger for MPD experiment
</Title>
<Content>
The Fast Forward Detector based Level 0 Trigger system architecture is described. The system must provide fast and effective triggering on nucleus – nucleus collisions at the center of the setup with high efficiency for central and semi-central Au + Au collisions. It should identify z- position of the collision with uncertainty better than 5 cm and an event multiplicity in pseudorapidity interval of 2.7 < |η| < 4.1. The system is modular and consists of two arm signal processors and a vertex processor. FPGAs are widely used.
</Content>
<field id="content">
The Fast Forward Detector based Level 0 Trigger system architecture is described. The system must provide fast and effective triggering on nucleus – nucleus collisions at the center of the setup with high efficiency for central and semi-central Au + Au collisions. It should identify z- position of the collision with uncertainty better than 5 cm and an event multiplicity in pseudorapidity interval of 2.7 < |η| < 4.1. The system is modular and consists of two arm signal processors and a vertex processor. FPGAs are widely used.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Rogov</FamilyName>
<Email>rogovictor@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Yurevich</FamilyName>
<Email>yurevich@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergey</FirstName>
<FamilyName>Sergeev</FamilyName>
<Email>serguei.sergueev@mail.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>125</Id>
<Title>
Electronics of the fission fragments spectrometer "COMETA-F"
</Title>
<Content>
The article describes the electronics for the time-of-flight two-arm fission fragments spectrometer COMETA-F. The installation is constructed of ‘’Start’’ detector on the base of microchannel plates and mosaics of eight PiN diodes. Each PiN diode of 18x18 mm surface area provides both energy and timing signal. The waveform is digitized by V1742 modules with a speed of 5 Gs / s. Start for registration is provided by V945 discriminators and a specially designed trigger block.
</Content>
<field id="content">
The article describes the electronics for the time-of-flight two-arm fission fragments spectrometer COMETA-F. The installation is constructed of ‘’Start’’ detector on the base of microchannel plates and mosaics of eight PiN diodes. Each PiN diode of 18x18 mm surface area provides both energy and timing signal. The waveform is digitized by V1742 modules with a speed of 5 Gs / s. Start for registration is provided by V945 discriminators and a specially designed trigger block.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Strekalovsky</FamilyName>
<Email>stroleg1@yandex.ru</Email>
<Affiliation>JINR FLNR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Kamanin</FamilyName>
<Email>kamanin@jinr.ru</Email>
<Affiliation>Joint Institute for Nuclear Research</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yury</FirstName>
<FamilyName>Pyatkov</FamilyName>
<Email>yvp_nov@mail.ru</Email>
<Affiliation>
National Nuclear Research University “MEPHI” &amp; Joint Institute for Nuclear Research
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexandr</FirstName>
<FamilyName>Strekalovsky</FamilyName>
<Email>alex.strek@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Oleg</FirstName>
<FamilyName>Strekalovsky</FamilyName>
<Email>stroleg1@yandex.ru</Email>
<Affiliation>JINR FLNR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>126</Id>
<Title>
Design of the front-end electronics based on multichannel IDEAS ASICs for silicon and GEM detectors
</Title>
<Content>
IDEAS ASICs are designed for the front-end readout of ionizing radiation detectors and produced by commercial fabless IC supplier – Integrated Detector Electronics AS (Norway). IDEAS ASIC is a multichannel (32/ 64/ 128) chips. Each chip channel has pre-amplifiers, shaper and multiplexed analogue readout. It’s necessary to configure internal chip registers, control analogue readout and transmit data from each measuring channel to DAQ System. These are basic functions of Control Unit based on FPGA. Design of the front-end electronics for silicon and GEM detectors consists of IDEAS IC, ADC and Control Unit. Current FEE BM@N configuration (March 2018) is based on IDEAS ASICs for Forward Silicon Detector, GEM detectors and CSC. According to upgrade plans for BM@N FEE for Si beam tracker, Si beam profiler, Forward Silicon Tracking Detectors also will be based on the same ASICs. This paper presents the design of the front-end electronics of the BM@N Si beam profiler: - Double-Sided Silicon Detectors – a coordinate plane with 2x128 measuring channels; - IDEAS ASICs – the front-end readout of DSSD; - Analog Devices ADC; - FPGA Xilinx – Control Unit.
</Content>
<field id="content">
IDEAS ASICs are designed for the front-end readout of ionizing radiation detectors and produced by commercial fabless IC supplier – Integrated Detector Electronics AS (Norway). IDEAS ASIC is a multichannel (32/ 64/ 128) chips. Each chip channel has pre-amplifiers, shaper and multiplexed analogue readout. It’s necessary to configure internal chip registers, control analogue readout and transmit data from each measuring channel to DAQ System. These are basic functions of Control Unit based on FPGA. Design of the front-end electronics for silicon and GEM detectors consists of IDEAS IC, ADC and Control Unit. Current FEE BM@N configuration (March 2018) is based on IDEAS ASICs for Forward Silicon Detector, GEM detectors and CSC. According to upgrade plans for BM@N FEE for Si beam tracker, Si beam profiler, Forward Silicon Tracking Detectors also will be based on the same ASICs. This paper presents the design of the front-end electronics of the BM@N Si beam profiler: - Double-Sided Silicon Detectors – a coordinate plane with 2x128 measuring channels; - IDEAS ASICs – the front-end readout of DSSD; - Analog Devices ADC; - FPGA Xilinx – Control Unit.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yulia</FirstName>
<FamilyName>Ivanova</FamilyName>
<Email>avinovayu@gmail.com</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergei</FirstName>
<FamilyName>Khabarov</FamilyName>
<Email>sergei.khabarov@mail.ru</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yuri</FirstName>
<FamilyName>Kovalev</FamilyName>
<Email>humbalumumba@gmail.com</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yulia</FirstName>
<FamilyName>Ivanova</FamilyName>
<Email>avinovayu@gmail.com</Email>
<Affiliation>VBLHEP JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>127</Id>
<Title>
DijetGAN: A Generative-Adversarial Network Approach for the Simulation of QCD Dijet Events at the LHC
</Title>
<Content>
We present a Generative-Adversarial Network (GAN) based on convolutional neural networks that are used to simulate the production of pairs of jets at the LHC. The GAN is trained on events generated using MadGraph5 + Pythia8, and Delphes3 fast detector simulation. A number of kinematic distributions both at Monte Carlo truth level and after the detector simulation can be reproduced by the generator network with a very good level of agreement. Our GAN can generate 1 million events in less than a minute and can be used to increase the size of Monte Carlo samples used by LHC experiments that are currently limited by the high CPU time required to generate events.
</Content>
<field id="content">
We present a Generative-Adversarial Network (GAN) based on convolutional neural networks that are used to simulate the production of pairs of jets at the LHC. The GAN is trained on events generated using MadGraph5 + Pythia8, and Delphes3 fast detector simulation. A number of kinematic distributions both at Monte Carlo truth level and after the detector simulation can be reproduced by the generator network with a very good level of agreement. Our GAN can generate 1 million events in less than a minute and can be used to increase the size of Monte Carlo samples used by LHC experiments that are currently limited by the high CPU time required to generate events.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Michele</FirstName>
<FamilyName>Faucci Giannelli</FamilyName>
<Email>michele.faucci.giannelli@ed.ac.uk</Email>
<Affiliation>University of Edinburgh</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Riccardo</FirstName>
<FamilyName>Di Sipio</FamilyName>
<Email>riccardo.di.sipio@cern.ch</Email>
<Affiliation>University of Toronto</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Serena</FirstName>
<FamilyName>Palazzo</FamilyName>
<Email>serena.palazzo@cern.ch</Email>
<Affiliation>University of Edinburgh</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Sana</FirstName>
<FamilyName>Ketabchi Haghighat</FamilyName>
<Email>sana.ketabchihaghighat@mail.utoronto.ca</Email>
<Affiliation>University of Toronto</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Michele</FirstName>
<FamilyName>Faucci Giannelli</FamilyName>
<Email>michele.faucci.giannelli@ed.ac.uk</Email>
<Affiliation>University of Edinburgh</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>128</Id>
<Title>
APPLICATION OF QUANTUM TECHNOLOGIES FOR THE DEVELOPMENT OF AN INTELLECTUAL CONTROL SYSTEM TO SETUP CURRENTS OF THE CORRECTIVE MAGNETS FOR THE BOOSTER SYNCHROTRON OF THE NICA FACILITY
</Title>
<Content>
One of the promising directions in the development of robust control systems for complex physical facilities is the application of quantum computing for building intelligent controllers based on neural networks and genetic algorithms. The main advantage of the application of quantum technologies is the high speed of adaptation of the intelligent control system (ICS) to changing conditions of functioning. The most promising solution is to use IBM's quantum processor for quickly calculating Grover’s algorithm (GA) to find the “extremum” of the function of a set of control variables. For example, in the process of tuning the frequency of the HF stations of the NICA complex, unexpected “parasitic” oscillations may appear whose frequency spectrum cannot be predicted. In such conditions, the task of developing self-organizing ICS, capable of functioning and ensuring the achievement of the goal of control in emergency situations and information risk conditions, is relevant for the NICA complex.
</Content>
<field id="content">
One of the promising directions in the development of robust control systems for complex physical facilities is the application of quantum computing for building intelligent controllers based on neural networks and genetic algorithms. The main advantage of the application of quantum technologies is the high speed of adaptation of the intelligent control system (ICS) to changing conditions of functioning. The most promising solution is to use IBM's quantum processor for quickly calculating Grover’s algorithm (GA) to find the “extremum” of the function of a set of control variables. For example, in the process of tuning the frequency of the HF stations of the NICA complex, unexpected “parasitic” oscillations may appear whose frequency spectrum cannot be predicted. In such conditions, the task of developing self-organizing ICS, capable of functioning and ensuring the achievement of the goal of control in emergency situations and information risk conditions, is relevant for the NICA complex.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitrii</FirstName>
<FamilyName>Monakhov</FamilyName>
<Email>cornflyer@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Reshetnikov</FamilyName>
<Email>agreshetnikov@gmail.com</Email>
<Affiliation>Ph.D.</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Dmitrii</FirstName>
<FamilyName>Monakhov</FamilyName>
<Email>cornflyer@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>129</Id>
<Title>
Realistic simulation of the MPD Time Projection Chamber with Garfield.
</Title>
<Content>
The detailed simulation of electron drifting in the MPD TPC was made with CERN Garfield toolkit for the simulation of gas particle detectors. For electron transporting were used the Ar+CH4 gas mixture with impact of magnetic and electric fields. Ionization processes were investigated in the wire planes area near readout chambers of the TPC.
</Content>
<field id="content">
The detailed simulation of electron drifting in the MPD TPC was made with CERN Garfield toolkit for the simulation of gas particle detectors. For electron transporting were used the Ar+CH4 gas mixture with impact of magnetic and electric fields. Ionization processes were investigated in the wire planes area near readout chambers of the TPC.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bychkov</FamilyName>
<Email>abychkov@jinr.ru</Email>
<Affiliation>LHEP</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Oleg</FirstName>
<FamilyName>Rogachevskiy</FamilyName>
<Email>rogachevsky@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Bychkov</FamilyName>
<Email>abychkov@jinr.ru</Email>
<Affiliation>LHEP</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>130</Id>
<Title>
Global Neutrino Analysis framework and GPU based computations
</Title>
<Content>
GNA is a high-performance fitting framework developed for the data analysis of the neutrino experiments. The framework is based on data flow principles: an experiment model is represented by the computational graph of simple functions as separate nodes that are computed lazily. In this work, we describe the GPU support library for GNA named cuGNA which uses CUDA toolkit. This library is implemented to enable both the performance of GPU and the versatility of data flow approach. We have added GPU-based node implementation to the existing library as well as implemented GNA core features that make GPU support hidden from the end user. Current status of CUDA computations in GNA, tests on real-life computational graphs, and performance comparison to CPU-based models are presented in this work.
</Content>
<field id="content">
GNA is a high-performance fitting framework developed for the data analysis of the neutrino experiments. The framework is based on data flow principles: an experiment model is represented by the computational graph of simple functions as separate nodes that are computed lazily. In this work, we describe the GPU support library for GNA named cuGNA which uses CUDA toolkit. This library is implemented to enable both the performance of GPU and the versatility of data flow approach. We have added GPU-based node implementation to the existing library as well as implemented GNA core features that make GPU support hidden from the end user. Current status of CUDA computations in GNA, tests on real-life computational graphs, and performance comparison to CPU-based models are presented in this work.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Maxim</FirstName>
<FamilyName>Gonchar</FamilyName>
<Email>gonchar@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Anna</FirstName>
<FamilyName>Fatkina</FamilyName>
<Email>fatkina.a.i@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Treskov</FamilyName>
<Email>treskov@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Naumov</FamilyName>
<Email>dnaumov@jinr.ru</Email>
<Affiliation>DLNP JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Liudmila</FirstName>
<FamilyName>Kolupaeva</FamilyName>
<Email>ldkolupaeva@yandex.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Anastasia</FirstName>
<FamilyName>Kalitkina</FamilyName>
<Email>mir8796@gmail.com</Email>
<Affiliation>JINR, DLNP, University Centre, MSU</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Anna</FirstName>
<FamilyName>Fatkina</FamilyName>
<Email>fatkina.a.i@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>131</Id>
<Title>
PROGRAM MANAGER FOR DC-280 CYCLOTRON CONTROL SYSTEM
</Title>
<Content>
March 25, 2019 the experimental hall of the Super-heavy Elements Factory (SHE) was opened at the FLNR JINR and its basic facility – the DC-280 cyclotron was launched. The cyclotron control system software contains modules that must be serviced to ensure reliability and fault tolerance. The Program Manager was developed to accomplish this. It performs start, stop and status monitoring of project modules. As a feature it is able to update programs and has voice informer to alarm the failures. This paper describes the algorithm and user interface of the Program Manager.
</Content>
<field id="content">
March 25, 2019 the experimental hall of the Super-heavy Elements Factory (SHE) was opened at the FLNR JINR and its basic facility – the DC-280 cyclotron was launched. The cyclotron control system software contains modules that must be serviced to ensure reliability and fault tolerance. The Program Manager was developed to accomplish this. It performs start, stop and status monitoring of project modules. As a feature it is able to update programs and has voice informer to alarm the failures. This paper describes the algorithm and user interface of the Program Manager.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Veronika</FirstName>
<FamilyName>Zabanova</FamilyName>
<Email>badver@jinr.ru</Email>
<Affiliation>FLNR, JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vitali</FirstName>
<FamilyName>Aleinikov</FamilyName>
<Email>vitalirus@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Kirill</FirstName>
<FamilyName>Sychev</FamilyName>
<Email>sychev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergei</FirstName>
<FamilyName>Pashchenko</FamilyName>
<Email>svpashch@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Veronika</FirstName>
<FamilyName>Zabanova</FamilyName>
<Email>badver@jinr.ru</Email>
<Affiliation>FLNR, JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>132</Id>
<Title>Development of the timing system of the NICA</Title>
<Content>
The report is devoted to the issues of creating a unified timing system for the NICA complex. The created system will be based on the “White Rabbit” technology, the implementation of this technology will be considered. The problem of adjusting the phases of the RF during the transfer of the beam from the accelerator to the accelerator will also be discussed. Particular attention will be paid to the development of hardware for the synchronization system.
</Content>
<field id="content">
The report is devoted to the issues of creating a unified timing system for the NICA complex. The created system will be based on the “White Rabbit” technology, the implementation of this technology will be considered. The problem of adjusting the phases of the RF during the transfer of the beam from the accelerator to the accelerator will also be discussed. Particular attention will be paid to the development of hardware for the synchronization system.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ilya</FirstName>
<FamilyName>Shirikov</FamilyName>
<Email>shirikov@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ilya</FirstName>
<FamilyName>Shirikov</FamilyName>
<Email>shirikov@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>133</Id>
<Title>EI3 – The ATLAS EventIndex for LHC Run 3</Title>
<Content>
The ATLAS Event Index provides since 2015 a good and reliable service for the initial use cases (mainly event picking) and several additional ones, such as production consistency checks, duplicate event detection and measurements of the overlaps of trigger chains and derivation datasets. LHC Run 3 will see increased data-taking and simulation production rates, with which the current infrastructure would still cope but may be stretched to its limits by the end of Run 3. This talk describes a new implementation of the front and back-end services that will be able to provide at least the same functionality as the current one for increased data ingestion and search rates and with increasing volumes of stored data. It is based on a set of HBase tables, with schemas derived from the current Oracle implementation, coupled to Apache Phoenix for data access; in this way we will add to the advantages of a BigData based storage system the possibility of SQL as well as NoSQL data access, allowing us to re-use most of the existing code for metadata integration.
</Content>
<field id="content">
The ATLAS Event Index provides since 2015 a good and reliable service for the initial use cases (mainly event picking) and several additional ones, such as production consistency checks, duplicate event detection and measurements of the overlaps of trigger chains and derivation datasets. LHC Run 3 will see increased data-taking and simulation production rates, with which the current infrastructure would still cope but may be stretched to its limits by the end of Run 3. This talk describes a new implementation of the front and back-end services that will be able to provide at least the same functionality as the current one for increased data ingestion and search rates and with increasing volumes of stored data. It is based on a set of HBase tables, with schemas derived from the current Oracle implementation, coupled to Apache Phoenix for data access; in this way we will add to the advantages of a BigData based storage system the possibility of SQL as well as NoSQL data access, allowing us to re-use most of the existing code for metadata integration.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Fedor</FirstName>
<FamilyName>Prokoshin</FamilyName>
<Email>prof@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>aleksand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Julius</FirstName>
<FamilyName>Hrivnac</FamilyName>
<Email>julius.hrivnac@cern.ch</Email>
<Affiliation>LAL Orsay</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrei</FirstName>
<FamilyName>Kazymov</FamilyName>
<Email>kazymai@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Mineev</FamilyName>
<Email>mineev@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Grigori</FirstName>
<FamilyName>Rybkin</FamilyName>
<Email>grigori.rybkine@cern.ch</Email>
<Affiliation>LAL, Univ. Paris-Sud</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Javier</FirstName>
<FamilyName>Sánchez</FamilyName>
<Email>javier.sanchez@cern.ch</Email>
<Affiliation>Instituto de Fisica Corpuscular (IFIC)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>José</FirstName>
<FamilyName>Salt Cairols</FamilyName>
<Email>jose.salt@cern.ch</Email>
<Affiliation>Instituto de Fisica Corpuscular (IFIC)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Miguel</FirstName>
<FamilyName>Villaplana</FamilyName>
<Email>miguel.villaplana.perez@cern.ch</Email>
<Affiliation>Università degli Studi e INFN Milano</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Alexandrov</FamilyName>
<Email>alexand@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Zbigniew</FirstName>
<FamilyName>Baranowski</FamilyName>
<Email>zbigniew.baranowski@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dario</FirstName>
<FamilyName>Barberis</FamilyName>
<Email>dario.barberis@ge.infn.it</Email>
<Affiliation>University and INFN Genova (Italy)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Gancho</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>gancho.dimitrov@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Álvaro</FirstName>
<FamilyName>Fernández Casaní</FamilyName>
<Email>alvaro.fernandez.casani@cern.ch</Email>
<Affiliation>Instituto de Fisica Corpuscular (IFIC)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Elizabeth</FirstName>
<FamilyName>Gallas</FamilyName>
<Email>elizabeth.gallas@physics.ox.ac.uk</Email>
<Affiliation>University of Oxford</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Carlos</FirstName>
<FamilyName>García Montoro</FamilyName>
<Email>carlos.garcia.montoro@cern.ch</Email>
<Affiliation>Instituto de Fisica Corpuscular (IFIC)</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Santiago</FirstName>
<FamilyName>González de la Hoz</FamilyName>
<Email>santiago.gonzalezdelahoz@cern.ch</Email>
<Affiliation>Instituto de Fisica Corpuscular (IFIC)</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Fedor</FirstName>
<FamilyName>Prokoshin</FamilyName>
<Email>prof@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
</abstract>
<abstract>
<Id>134</Id>
<Title>
Simulating Lattice QCD on the "Govorun" Supercomputer
</Title>
<Content>
Lattice Quantum Chromodynamics (QCD) is a well-established non-perturbative approach to the theory of strong interactions, QCD. It provides a framework for numerical studies of various complex problems of QCD. Such computations are numerically very demanding and require the most powerful modern supercomputers and algorithms. Within this talk, the lattice QCD simulations which are carried out on "Govorun" supercomputer are discussed. The basic algorithms and their implementation on "Govorun" architecture are reviewed. Important physical results and projects which are studied on "Govorun", including QCD at finite temperature, isospin and baryon density, are presented.
</Content>
<field id="content">
Lattice Quantum Chromodynamics (QCD) is a well-established non-perturbative approach to the theory of strong interactions, QCD. It provides a framework for numerical studies of various complex problems of QCD. Such computations are numerically very demanding and require the most powerful modern supercomputers and algorithms. Within this talk, the lattice QCD simulations which are carried out on "Govorun" supercomputer are discussed. The basic algorithms and their implementation on "Govorun" architecture are reviewed. Important physical results and projects which are studied on "Govorun", including QCD at finite temperature, isospin and baryon density, are presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Kotov</FamilyName>
<Email>kotov@itep.ru</Email>
<Affiliation>Institute for Theoretical and Experimental Physics</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Victor</FirstName>
<FamilyName>Braguta</FamilyName>
<Email>braguta@itep.ru</Email>
<Affiliation>ITEP</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Aleksandr</FirstName>
<FamilyName>Nikolaev</FamilyName>
<Email>aleksandr.nikolaev@swansea.ac.uk</Email>
<Affiliation>Swansea University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Astrakhantsev</FamilyName>
<Email>nikita.astronaut@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kotov</FamilyName>
<Email>kotov@itep.ru</Email>
<Affiliation>Institute for Theoretical and Experimental Physics</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>135</Id>
<Title>
Simulation of spectra of cylindrical neutron counters using the GEANT-4 package
</Title>
<Content>
It’s commonly supposed that the amplitude spectrum of the helium proportional counter at irradiation by thermal and cold neutrons has a peak of full absorption with energy of 768 keV and two small “shelves”, caused by boundary effects from falling of charged particles (of proton or tritium nucleus) in the detector wall. Simulation of amplitude spectra of cylindrical counters with different gas filling is presented in the paper. The possibility of the third peak, not coinciding with that of full absorption, is shown, while the peak position depends on the ratio of the path length towards the counter diameter. The results obtained may be of interest in the development of low efficiency neutron detectors and neutron monitors.
</Content>
<field id="content">
It’s commonly supposed that the amplitude spectrum of the helium proportional counter at irradiation by thermal and cold neutrons has a peak of full absorption with energy of 768 keV and two small “shelves”, caused by boundary effects from falling of charged particles (of proton or tritium nucleus) in the detector wall. Simulation of amplitude spectra of cylindrical counters with different gas filling is presented in the paper. The possibility of the third peak, not coinciding with that of full absorption, is shown, while the peak position depends on the ratio of the path length towards the counter diameter. The results obtained may be of interest in the development of low efficiency neutron detectors and neutron monitors.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Churakov</FamilyName>
<Email>churakov@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Aleksey</FirstName>
<FamilyName>Kurilkin</FamilyName>
<Email>akurilkin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Saiz Lomas</FirstName>
<FamilyName>Juan</FamilyName>
<Email>js2604@york.ac.uk</Email>
<Affiliation>University of York</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Churakov</FamilyName>
<Email>churakov@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>136</Id>
<Title>
Data Knowledge Base: metadata integration system for HENP experiments
</Title>
<Content>
HENP experiments, especially the long-living ones like the ATLAS experiment at the LHC, have a diverse and evolving ecosystem of information systems that help scientists to organize research processes -- such as data handling (including data taking, simulation, processing, storage, and access), preparing and discussion of publications, etc. With time all the components of the ecosystem grow, develop into complex structures, accumulate metadata and become more independent and less flexible. Automated information integration becomes a pressing need for effective operating within the ecosystem. This contribution is dedicated to the meta-system, known as Data Knowledge Base (DKB), designed to integrate information from multiple independent sources and provide fast and flexible access to the integrated knowledge. Over the last two years, the system is being successfully integrated with the production system of the ATLAS experiment, including the extension of the production system web-interface with functionality built upon the unified metadata provided by DKB.
</Content>
<field id="content">
HENP experiments, especially the long-living ones like the ATLAS experiment at the LHC, have a diverse and evolving ecosystem of information systems that help scientists to organize research processes -- such as data handling (including data taking, simulation, processing, storage, and access), preparing and discussion of publications, etc. With time all the components of the ecosystem grow, develop into complex structures, accumulate metadata and become more independent and less flexible. Automated information integration becomes a pressing need for effective operating within the ecosystem. This contribution is dedicated to the meta-system, known as Data Knowledge Base (DKB), designed to integrate information from multiple independent sources and provide fast and flexible access to the integrated knowledge. Over the last two years, the system is being successfully integrated with the production system of the ATLAS experiment, including the extension of the production system web-interface with functionality built upon the unified metadata provided by DKB.
</field>
<field id="summary">
The contribution is dedicated to the meta-system, known as Data Knowledge Base (DKB), designed to integrate information from multiple independent sources and provide fast and flexible access to the integrated knowledge.
</field>
<PrimaryAuthor>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Grigorieva</FirstName>
<FamilyName>Maria</FamilyName>
<Email>maria.grigorieva@cern.ch</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anastasiia</FirstName>
<FamilyName>Kaida</FamilyName>
<Email>anastasiakaida@gmail.com</Email>
<Affiliation>
National Research Tomsk Polytechnic University, Institute of Cybernetics
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vasiliy</FirstName>
<FamilyName>Aulov</FamilyName>
<Email>vasilii.aulov@cern.ch</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Mikhail</FirstName>
<FamilyName>Borodin</FamilyName>
<Email>mborodin@cern.ch</Email>
<Affiliation>The University of Iowa (US)</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
</abstract>
<abstract>
<Id>137</Id>
<Title>
Data streams processing in metadata integration system for HENP experiments
</Title>
<Content>
Nowadays, heterogeneous metadata integration has become a widespread objective. Whenever it is addressed, there are numerous tasks to be solved, such as data sources analysis and storage schema development. No less important one is the development of automated, configurable and highly manageable ETL (data Extraction, Transformation, and Load) processes, as well as the creation of tools for their automatization, scheduling, management, monitoring. This work describes the Metadata Integration and Topology Management System, initially designed as a subsystem of the Data Knowledge Base (DKB) developed for the ATLAS experiment. The core idea of the subsystem is to separate the common features of the majority of ETL-processes from the implementation of particular tasks. It is implemented as standalone modules: supervisor and workers; a supervisor is responsible for data streams building through workers that implement a set of specific operations for a particular process. The system is intended to considerably facilitate the organizing of ongoing data integration operations with automated data stream processing.
</Content>
<field id="content">
Nowadays, heterogeneous metadata integration has become a widespread objective. Whenever it is addressed, there are numerous tasks to be solved, such as data sources analysis and storage schema development. No less important one is the development of automated, configurable and highly manageable ETL (data Extraction, Transformation, and Load) processes, as well as the creation of tools for their automatization, scheduling, management, monitoring. This work describes the Metadata Integration and Topology Management System, initially designed as a subsystem of the Data Knowledge Base (DKB) developed for the ATLAS experiment. The core idea of the subsystem is to separate the common features of the majority of ETL-processes from the implementation of particular tasks. It is implemented as standalone modules: supervisor and workers; a supervisor is responsible for data streams building through workers that implement a set of specific operations for a particular process. The system is intended to considerably facilitate the organizing of ongoing data integration operations with automated data stream processing.
</field>
<field id="summary">
This work describes the Metadata Integration and Topology Management System, initially designed as a subsystem of the Data Knowledge Base (DKB) developed for the ATLAS experiment.
</field>
<PrimaryAuthor>
<FirstName>Anastasiia</FirstName>
<FamilyName>Kaida</FamilyName>
<Email>anastasiakaida@gmail.com</Email>
<Affiliation>
National Research Tomsk Polytechnic University, School of Computer Science & Robotics
</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Marina</FirstName>
<FamilyName>Golosova</FamilyName>
<Email>golosova.marina@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Maria</FirstName>
<FamilyName>Grigorieva</FamilyName>
<Email>magsend@gmail.com</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vasiliy</FirstName>
<FamilyName>Aulov</FamilyName>
<Email>vasiliyaulov@gmail.com</Email>
<Affiliation>NRC Kurchatov Institute</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Anastasiia</FirstName>
<FamilyName>Kaida</FamilyName>
<Email>anastasiakaida@gmail.com</Email>
<Affiliation>
National Research Tomsk Polytechnic University, School of Computer Science & Robotics
</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>138</Id>
<Title>
The ATLAS Electron and Photon Trigger Performance in Run 2
</Title>
<Content>
ATLAS electron and photon triggers covering transverse energies from 5 GeV to several TeV are essential to record signals for a wide variety of physics: from Standard Model processes to searches for new phenomena in both proton-proton and heavy ion collisions. Main triggers used during Run 2 (2015-2018) for those physics studies were a single-electron trigger with ET threshold around 25 GeV and a diphoton trigger with thresholds at 25 and 35 GeV. Relying on those simple, general-purpose triggers is seen as a more robust trigger strategy, at a cost of slightly higher trigger output rates, than to use a large number of analysis-specific triggers. To cope with ever-increasing luminosity and more challenging pile-up conditions at the LHC, the trigger selections needed to be optimized to control the rates and keep efficiencies high. The ATLAS electron and photon trigger performance during Run-2 data-taking is presented as well as work ongoing to prepare to even higher luminosity of Run 3 (2021-2023).
</Content>
<field id="content">
ATLAS electron and photon triggers covering transverse energies from 5 GeV to several TeV are essential to record signals for a wide variety of physics: from Standard Model processes to searches for new phenomena in both proton-proton and heavy ion collisions. Main triggers used during Run 2 (2015-2018) for those physics studies were a single-electron trigger with ET threshold around 25 GeV and a diphoton trigger with thresholds at 25 and 35 GeV. Relying on those simple, general-purpose triggers is seen as a more robust trigger strategy, at a cost of slightly higher trigger output rates, than to use a large number of analysis-specific triggers. To cope with ever-increasing luminosity and more challenging pile-up conditions at the LHC, the trigger selections needed to be optimized to control the rates and keep efficiencies high. The ATLAS electron and photon trigger performance during Run-2 data-taking is presented as well as work ongoing to prepare to even higher luminosity of Run 3 (2021-2023).
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>139</Id>
<Title>ATLAS Muon Trigger performance</Title>
<Content>
Events containing muons in the final state are an important signature for many analyses being carried out at the Large Hadron Collider (LHC), including both standard model measurements and searches for new physics. To be able to study such events, it is required to have an efficient and well-understood muon trigger. The ATLAS muon trigger consists of a hardware based system (Level 1), as well as a software based reconstruction (High Level Trigger). Due to the high luminosity in Run 2, several improvements have been implemented to keep the trigger rate low, while still maintaining a high efficiency. Some examples of recent improvements include requiring coincidence of hits in the muon spectrometer and the calorimeter and optimised muon isolation. We will present an overview of how we trigger on muons, recent improvements, the performance of the muon trigger in Run-2 data and an outlook for the improvements planned for run-3.
</Content>
<field id="content">
Events containing muons in the final state are an important signature for many analyses being carried out at the Large Hadron Collider (LHC), including both standard model measurements and searches for new physics. To be able to study such events, it is required to have an efficient and well-understood muon trigger. The ATLAS muon trigger consists of a hardware based system (Level 1), as well as a software based reconstruction (High Level Trigger). Due to the high luminosity in Run 2, several improvements have been implemented to keep the trigger rate low, while still maintaining a high efficiency. Some examples of recent improvements include requiring coincidence of hits in the muon spectrometer and the calorimeter and optimised muon isolation. We will present an overview of how we trigger on muons, recent improvements, the performance of the muon trigger in Run-2 data and an outlook for the improvements planned for run-3.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>140</Id>
<Title>The ATLAS Run-2 Trigger Menu</Title>
<Content>
The ATLAS experiment aims at recording about 1 kHz of physics collisions, starting with an LHC design bunch crossing rate of 40 MHz. To reduce the significant background rate while maintaining a high selection efficiency for rare physics events (such as beyond the Standard Model physics), a two-level trigger system is used. Events are selected based on physics signatures such as the presence of energetic leptons, photons, jets or large missing energy. The trigger system exploits topological information, as well as multivariate methods to carry out the necessary physics filtering for the many analyses that are pursued by the ATLAS community. In total, the ATLAS online selection consists of around 1500 individual triggers. A Trigger Menu is the compilation of these triggers, it specifies the physics selection algorithms to be used during data taking, and the rate and bandwidth a given trigger is allocated. Trigger menus must reflect the physics goals for a given run, and also must take into consideration the instantaneous luminosity of the LHC and limitations from the ATLAS detector readout and offline processing farm. For the 2017-2018 run, the ATLAS trigger has been enhanced to be able to handle higher instantaneous luminosities and to ensure the selection robustness against higher average multiple interactions per bunch crossing. We will describe the design criteria for the trigger menu for Run 2. We discuss several aspects of the process of planning the trigger menu, starting from how ATLAS physics goals and the need for detector performance measurements enter the menu design, and how rate, bandwidth, and CPU constraints are folded in during the compilation of the menu. We present the tools that allow us to predict and optimize the trigger rates and CPU consumption for the anticipated LHC luminosities. We outline the online system that we implemented to monitor deviations from the individual trigger target rates, and to quickly react to the changing LHC conditions and data taking scenarios. Finally, we give an overview of the 2015-2018 Trigger Menu and performance, allowing the audience to get a taste of the broad physics program that the trigger is supporting.
</Content>
<field id="content">
The ATLAS experiment aims at recording about 1 kHz of physics collisions, starting with an LHC design bunch crossing rate of 40 MHz. To reduce the significant background rate while maintaining a high selection efficiency for rare physics events (such as beyond the Standard Model physics), a two-level trigger system is used. Events are selected based on physics signatures such as the presence of energetic leptons, photons, jets or large missing energy. The trigger system exploits topological information, as well as multivariate methods to carry out the necessary physics filtering for the many analyses that are pursued by the ATLAS community. In total, the ATLAS online selection consists of around 1500 individual triggers. A Trigger Menu is the compilation of these triggers, it specifies the physics selection algorithms to be used during data taking, and the rate and bandwidth a given trigger is allocated. Trigger menus must reflect the physics goals for a given run, and also must take into consideration the instantaneous luminosity of the LHC and limitations from the ATLAS detector readout and offline processing farm. For the 2017-2018 run, the ATLAS trigger has been enhanced to be able to handle higher instantaneous luminosities and to ensure the selection robustness against higher average multiple interactions per bunch crossing. We will describe the design criteria for the trigger menu for Run 2. We discuss several aspects of the process of planning the trigger menu, starting from how ATLAS physics goals and the need for detector performance measurements enter the menu design, and how rate, bandwidth, and CPU constraints are folded in during the compilation of the menu. We present the tools that allow us to predict and optimize the trigger rates and CPU consumption for the anticipated LHC luminosities. We outline the online system that we implemented to monitor deviations from the individual trigger target rates, and to quickly react to the changing LHC conditions and data taking scenarios. Finally, we give an overview of the 2015-2018 Trigger Menu and performance, allowing the audience to get a taste of the broad physics program that the trigger is supporting.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>141</Id>
<Title>
Implementation of the ATLAS trigger within the multi-threaded AthenaMT framework
</Title>
<Content>
Athena is the software framework used in the ATLAS experiment throughout the data processing path, from the software trigger system through offline event reconstruction to physics analysis. The shift from high-power single-core CPUs to multi-core systems in the computing market means that the throughput capabilities of the framework have become limited by the available memory per process. For Run 2 of the Large Hadron Collider (LHC), ATLAS has exploited a multi-process forking approach with the copy-on-write mechanism to reduce the memory use. To better match the increasing CPU core count and the, therefore, decreasing available memory per core, a multi-threaded framework, AthenaMT, has been designed and is now being implemented. The ATLAS High Level Trigger (HLT) system has been remodelled to fit the new framework and to rely on common solutions between online and offline software to a greater extent than in Run 2. We present the implementation of the new HLT system within the AthenaMT framework, which will be used in ATLAS data-taking during Run 3 (2021-2023) of the LHC.
</Content>
<field id="content">
Athena is the software framework used in the ATLAS experiment throughout the data processing path, from the software trigger system through offline event reconstruction to physics analysis. The shift from high-power single-core CPUs to multi-core systems in the computing market means that the throughput capabilities of the framework have become limited by the available memory per process. For Run 2 of the Large Hadron Collider (LHC), ATLAS has exploited a multi-process forking approach with the copy-on-write mechanism to reduce the memory use. To better match the increasing CPU core count and the, therefore, decreasing available memory per core, a multi-threaded framework, AthenaMT, has been designed and is now being implemented. The ATLAS High Level Trigger (HLT) system has been remodelled to fit the new framework and to rely on common solutions between online and offline software to a greater extent than in Run 2. We present the implementation of the new HLT system within the AthenaMT framework, which will be used in ATLAS data-taking during Run 3 (2021-2023) of the LHC.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>142</Id>
<Title>
FELIX: commissioning the new detector interface for the ATLAS trigger and readout system
</Title>
<Content>
After the current LHC shutdown (2019-2021), the ATLAS experiment will be required to operate in an increasingly harsh collision environment. To maintain physics performance, the ATLAS experiment will undergo a series of upgrades during the shutdown. A key goal of this upgrade is to improve the capacity and flexibility of the detector readout system. To this end, the Front-End Link eXchange (FELIX) system has been developed. FELIX acts as the interface between the data acquisition; detector control and TTC (Timing, Trigger and Control) systems; and new or updated trigger and detector front-end electronics. The system functions as a router between custom serial links from front end ASICs and FPGAs to data collection and processing components via a commodity switched network. FELIX also forwards the LHC bunch-crossing clock, fixed latency trigger accepts and resets received from the TTC system to front-end electronics. FELIX uses commodity server technology in combination with FPGA-based PCIe I/O cards. FELIX servers run a software routing platform serving data to network clients. This presentation will cover the design of FELIX and the results of the installation and commissioning activities for the full system in summer 2019.
</Content>
<field id="content">
After the current LHC shutdown (2019-2021), the ATLAS experiment will be required to operate in an increasingly harsh collision environment. To maintain physics performance, the ATLAS experiment will undergo a series of upgrades during the shutdown. A key goal of this upgrade is to improve the capacity and flexibility of the detector readout system. To this end, the Front-End Link eXchange (FELIX) system has been developed. FELIX acts as the interface between the data acquisition; detector control and TTC (Timing, Trigger and Control) systems; and new or updated trigger and detector front-end electronics. The system functions as a router between custom serial links from front end ASICs and FPGAs to data collection and processing components via a commodity switched network. FELIX also forwards the LHC bunch-crossing clock, fixed latency trigger accepts and resets received from the TTC system to front-end electronics. FELIX uses commodity server technology in combination with FPGA-based PCIe I/O cards. FELIX servers run a software routing platform serving data to network clients. This presentation will cover the design of FELIX and the results of the installation and commissioning activities for the full system in summer 2019.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>143</Id>
<Title>
ATLAS Trigger and Data Acquisition Upgrades for the High Luminosity LHC
</Title>
<Content>
The ATLAS experiment at CERN has started the construction of upgrades for the "High Luminosity LHC", with collisions due to start in 2026. In order to deliver an order of magnitude more data than previous LHC runs, 14 TeV protons will collide with an instantaneous luminosity of up to 7.5x10^34 cm^-2s^-1, resulting in much higher pileup and data rates than the current experiment was designed to handle. While this is essential to realise the physics programme, it presents a huge challenge for the detector, trigger, data acquisition and computing. The detector upgrades themselves also present new requirements and opportunities for the trigger and data acquisition system. The approved baseline design of the TDAQ upgrade comprises: a hardware-based low-latency real-time Trigger operating at 40 MHz, Data Acquisition which combines custom readout with commodity hardware and networking to deal with 5.2 TB/s input, and an Event Filter running at 1 MHz which combines offline-like algorithms on a large commodity compute service augmented by hardware tracking. Commodity servers and networks are used as far as possible, with custom ATCA boards, high speed links and powerful FPGAs deployed in the low-latency parts of the system. Offline-style clustering and jet-finding in FPGAs, and track reconstruction with Associative Memory ASICs and FPGAs are designed to combat pileup in the Trigger and Event Filter respectively. This paper will report recent progress on the design, technology and construction of the system. The physics motivation and expected performance will be shown for key physics processes.
</Content>
<field id="content">
The ATLAS experiment at CERN has started the construction of upgrades for the "High Luminosity LHC", with collisions due to start in 2026. In order to deliver an order of magnitude more data than previous LHC runs, 14 TeV protons will collide with an instantaneous luminosity of up to 7.5x10^34 cm^-2s^-1, resulting in much higher pileup and data rates than the current experiment was designed to handle. While this is essential to realise the physics programme, it presents a huge challenge for the detector, trigger, data acquisition and computing. The detector upgrades themselves also present new requirements and opportunities for the trigger and data acquisition system. The approved baseline design of the TDAQ upgrade comprises: a hardware-based low-latency real-time Trigger operating at 40 MHz, Data Acquisition which combines custom readout with commodity hardware and networking to deal with 5.2 TB/s input, and an Event Filter running at 1 MHz which combines offline-like algorithms on a large commodity compute service augmented by hardware tracking. Commodity servers and networks are used as far as possible, with custom ATCA boards, high speed links and powerful FPGAs deployed in the low-latency parts of the system. Offline-style clustering and jet-finding in FPGAs, and track reconstruction with Associative Memory ASICs and FPGAs are designed to combat pileup in the Trigger and Event Filter respectively. This paper will report recent progress on the design, technology and construction of the system. The physics motivation and expected performance will be shown for key physics processes.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Savanna</FirstName>
<FamilyName>Shaw</FamilyName>
<Email>savanna.marie.shaw@cern.ch</Email>
<Affiliation>University of Manchester</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>144</Id>
<Title>
The Visualization Method Pipeline for the Application to Dynamic Data Analysis
</Title>
<Content>
The new era of scientific research brings an enormous amount of data for scientists. These complex and multidimensional data structures are used for the verification of scientific hypothesis. Exploring such data by researchers requires the development of new technologies for its efficient processing, investigation and interpretation. Intellectual data analysis and statistical methods are rapidly developing, and this is where visualization methods are getting their place. This work describes mathematical basis of the developed visualization tool for the analysis of multidimensional dynamic data. This tool provides the pipeline of methods, which combined, allow to cope with a set of practical tasks (anomalies detection, cluster, trends and variation analysis) using visualization method. Authors provided mathematical models of geometrical operations under the data domain, algorithms for solving the mentioned classes of tasks and several use-cases with technological and economical data based on visualization method.
</Content>
<field id="content">
The new era of scientific research brings an enormous amount of data for scientists. These complex and multidimensional data structures are used for the verification of scientific hypothesis. Exploring such data by researchers requires the development of new technologies for its efficient processing, investigation and interpretation. Intellectual data analysis and statistical methods are rapidly developing, and this is where visualization methods are getting their place. This work describes mathematical basis of the developed visualization tool for the analysis of multidimensional dynamic data. This tool provides the pipeline of methods, which combined, allow to cope with a set of practical tasks (anomalies detection, cluster, trends and variation analysis) using visualization method. Authors provided mathematical models of geometrical operations under the data domain, algorithms for solving the mentioned classes of tasks and several use-cases with technological and economical data based on visualization method.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Timofei</FirstName>
<FamilyName>Galkin</FamilyName>
<Email>tpgalkin@mephi.ru</Email>
<Affiliation>NRNU MEPhI</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Popov</FamilyName>
<Email>ddpopov@mephi.ru</Email>
<Affiliation>Student NRNU MEPhI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Victor</FirstName>
<FamilyName>Pilyugin</FamilyName>
<Email>vvpilyugin@mephi.ru</Email>
<Affiliation>
National Research Nuclear University "MEPhI", Moscow, Russian Federation
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Timofei</FirstName>
<FamilyName>Galkin</FamilyName>
<Email>tpgalkin@mephi.ru</Email>
<Affiliation>NRNU MEPhI</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
</abstract>
<abstract>
<Id>145</Id>
<Title>
Measurement of basic static characteristics (I-V, C-V) of silicon detectors
</Title>
<Content>
The use of microstrip detectors in creating coordinate track systems for HEP experiments with high geometric efficiency (~100%), a large number of strips (measuring channels) over 10^6 and accuracy a/√12 (a-pitch)requires careful preliminary selection of detectors by main parameters. The main static parameters of silicon microstrip detectors include the following:I-V characteristic determines the amount of dark leakage current of a silicon detector.C-V characteristic allows you to define the full depletion voltage and the value of the capacitance of both the strip and detector. Modern systems for testing and selection of microstrip detectors make it possible in the possible in the automated mode to identify strips with high dark currents, possible short circuits and breaks in interstrip metallization.
</Content>
<field id="content">
The use of microstrip detectors in creating coordinate track systems for HEP experiments with high geometric efficiency (~100%), a large number of strips (measuring channels) over 10^6 and accuracy a/√12 (a-pitch)requires careful preliminary selection of detectors by main parameters. The main static parameters of silicon microstrip detectors include the following:I-V characteristic determines the amount of dark leakage current of a silicon detector.C-V characteristic allows you to define the full depletion voltage and the value of the capacitance of both the strip and detector. Modern systems for testing and selection of microstrip detectors make it possible in the possible in the automated mode to identify strips with high dark currents, possible short circuits and breaks in interstrip metallization.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Catherine</FirstName>
<FamilyName>Streletskaya</FamilyName>
<Email>estreletskaya@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Catherine</FirstName>
<FamilyName>Streletskaya</FamilyName>
<Email>estreletskaya@bk.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>146</Id>
<Title>
The software and solutions for express processing of the raw list mode data measured on the neutron spectrometers of the IBR-2 reactor using a delay line position-sensitive detector as designed to be integrated into the experiment control system
</Title>
<Content>
Recently we have performed a comparative study of the characteristics of the data acquisition systems for the position-sensitive detectors with a delay line operating on the neutron instruments of the IBR-2 reactor. As a result, to have an optimal version of electronics we have chosen two directions of further development: the DeLiDAQ-2 system for high-flux measurements and the CAEN N6730 digitizer-based system for high-precision experiments. The study has also revealed an urgent need to integrate list mode measurements into the experiment control system on some of the neutron spectrometers. So far, the experiment control system SONIX operating on most of the IBR-2 spectrometers has received and displayed the data measured in the histogram mode. The report, besides the results of the comparative study also describes the software that is developed to solve the task of formation of events from raw data, their sorting, selecting by appropriate criteria, histogramming and to be appropriate for integration into the SONIX. The proposed solutions are not limited to any specific types of electronics for PSD.
</Content>
<field id="content">
Recently we have performed a comparative study of the characteristics of the data acquisition systems for the position-sensitive detectors with a delay line operating on the neutron instruments of the IBR-2 reactor. As a result, to have an optimal version of electronics we have chosen two directions of further development: the DeLiDAQ-2 system for high-flux measurements and the CAEN N6730 digitizer-based system for high-precision experiments. The study has also revealed an urgent need to integrate list mode measurements into the experiment control system on some of the neutron spectrometers. So far, the experiment control system SONIX operating on most of the IBR-2 spectrometers has received and displayed the data measured in the histogram mode. The report, besides the results of the comparative study also describes the software that is developed to solve the task of formation of events from raw data, their sorting, selecting by appropriate criteria, histogramming and to be appropriate for integration into the SONIX. The proposed solutions are not limited to any specific types of electronics for PSD.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Elena</FirstName>
<FamilyName>Litvinenko</FamilyName>
<Email>litvin@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Bogdzel</FamilyName>
<Email>abogdz@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Viktor</FirstName>
<FamilyName>Bodnarchuk</FamilyName>
<Email>bodnarch@nf.jinr.ru</Email>
<Affiliation>
Frank Laboratory of Neutron Physics Joint Institute for Nuclear Research
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Gapon</FamilyName>
<Email>gapon@jinr.ru</Email>
<Affiliation>
Physics Department, National Taras Shevchenko University of Kyiv, Kyiv, Ukraine
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Drozdov</FamilyName>
<Email>drozdov@jinr.ru</Email>
<Affiliation>JINR, FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Kulikov</FamilyName>
<Email>ksa@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Svetlana</FirstName>
<FamilyName>Murashkevich</FamilyName>
<Email>svetlana@nf.jinr.ru</Email>
<Affiliation>JINR FLNP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anatolii</FirstName>
<FamilyName>Nagornyi</FamilyName>
<Email>avnagorny@jinr.ru</Email>
<Affiliation>
National Taras Shevchenko University of Kyiv / Joint Institute for Nuclear Research
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Churakov</FamilyName>
<Email>churakov@nf.jinr.ru</Email>
<Affiliation>FLNP JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Elena</FirstName>
<FamilyName>Litvinenko</FamilyName>
<Email>litvin@nf.jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>147</Id>
<Title>
Implementation of semantic approach for cross-agency information sharing: building the information exchange package on the basis of domain ontology
</Title>
<Content>
Heterogeneous environment inherent for the information sharing at all levels requires the application of integration methods, which will guarantee the achievement of unambiguous and meaningful interpretation of data. Semantic interoperability is one of key enablers of digital transformation in public sector as well as in the other economic fields. For effective cross-agency information sharing it is important to provide the participants of interaction with the methods, technologies and tools, simplifying the implementation of semantic approach. In this paper we represent the method of building the information exchange package on the basis of a unified data model, the domain ontology. This method combines the NIEM approach for IEPD development with the arrangement of collaboration for domain experts and information system developers. The method makes the domain model more comprehensive for external users, giving them constant expert support, as well as it serves for domain model improvement based on the suggested extensions.
</Content>
<field id="content">
Heterogeneous environment inherent for the information sharing at all levels requires the application of integration methods, which will guarantee the achievement of unambiguous and meaningful interpretation of data. Semantic interoperability is one of key enablers of digital transformation in public sector as well as in the other economic fields. For effective cross-agency information sharing it is important to provide the participants of interaction with the methods, technologies and tools, simplifying the implementation of semantic approach. In this paper we represent the method of building the information exchange package on the basis of a unified data model, the domain ontology. This method combines the NIEM approach for IEPD development with the arrangement of collaboration for domain experts and information system developers. The method makes the domain model more comprehensive for external users, giving them constant expert support, as well as it serves for domain model improvement based on the suggested extensions.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Akatkin</FamilyName>
<Email>u.akatkin@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Elena</FirstName>
<FamilyName>Yasinovskaya</FamilyName>
<Email>elena@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Michail</FirstName>
<FamilyName>Bich</FamilyName>
<Email>misha@e-projecting.ru</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Elena</FirstName>
<FamilyName>Yasinovskaya</FamilyName>
<Email>elena@semanticpro.org</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Non-relational Databases and Heterogeneous Repositories
</Track>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>148</Id>
<Title>
Control and measurement system for the MASHA experimental setup at the DC280 cyclotron
</Title>
<Content>
The control system modernization of the MASHA experiment (Mass Analyzer of Super Heavy Atoms) will be discussed. The control system, based on CompactRIO and PXI/PXIe standards, will be developed, tested, and integrated to the new experimental setup at the DC280 cyclotron. The MASHA experiment is designed for the study of super heavy element properties, synthesized in reactions of 48Са beam with neutron-rich actinides target. The setup of MASHA is a combination of ISOL (Isotope Separator On-Line) methods and traditional mass spectrometry. The detection system is based on silicon multistripe detectors combined with a Timepix detector (hybrid pixel imaging). The detection system can properly identify products of synthesis and its decay chains (alpha, beta, fragments). The control and measurement system requires high reliability and stability. Therefore, a distributed control network have being gradually built, consisting of up-to-date devices. Controllers based on the RIO architecture were applied for control (several actuators) and connected to the entire experiment for the cross sections measurement of reactions 40Ar + 144Sm and 166Er. There are plans to use the RIO standard (consisting of a microprocessor working on real-time operating system and Field Programmable Gate Array) in the new setup of MASHA with the gas catcher at the beam line from the new DC280 cyclotron in the near future.
</Content>
<field id="content">
The control system modernization of the MASHA experiment (Mass Analyzer of Super Heavy Atoms) will be discussed. The control system, based on CompactRIO and PXI/PXIe standards, will be developed, tested, and integrated to the new experimental setup at the DC280 cyclotron. The MASHA experiment is designed for the study of super heavy element properties, synthesized in reactions of 48Са beam with neutron-rich actinides target. The setup of MASHA is a combination of ISOL (Isotope Separator On-Line) methods and traditional mass spectrometry. The detection system is based on silicon multistripe detectors combined with a Timepix detector (hybrid pixel imaging). The detection system can properly identify products of synthesis and its decay chains (alpha, beta, fragments). The control and measurement system requires high reliability and stability. Therefore, a distributed control network have being gradually built, consisting of up-to-date devices. Controllers based on the RIO architecture were applied for control (several actuators) and connected to the entire experiment for the cross sections measurement of reactions 40Ar + 144Sm and 166Er. There are plans to use the RIO standard (consisting of a microprocessor working on real-time operating system and Field Programmable Gate Array) in the new setup of MASHA with the gas catcher at the beam line from the new DC280 cyclotron in the near future.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Antonin</FirstName>
<FamilyName>Opichal</FamilyName>
<Email>opichaltonda@gmail.com</Email>
<Affiliation>Palacky University Olomouc</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Antonin</FirstName>
<FamilyName>Opichal</FamilyName>
<Email>opichaltonda@gmail.com</Email>
<Affiliation>Palacky University Olomouc</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>149</Id>
<Title>
Monitoring and Accounting for the Distributed Computing System of the ATLAS Experiment
</Title>
<Content>
ATLAS developed over the years a large number of monitoring and accounting tools for distributed computing applications. In advance of the increased experiment data rates and monitoring data volumes foreseen for LHC Run 3 starting in 2012, a new infrastructure has been provided by the CERN-IT Monit group, based on InfluxDB as the data store and Grafana as the display environment. ATLAS is adapting and further developing its monitoring tools to use this infrastructure for data and workflow management monitoring and accounting dashboards, expanding the range of previous possibilities with the aim of achieving a single, simpler, environment for all monitoring applications. This presentation will describe the tools used, the data flows for monitoring and accounting, the problems encountered and the solutions found.
</Content>
<field id="content">
ATLAS developed over the years a large number of monitoring and accounting tools for distributed computing applications. In advance of the increased experiment data rates and monitoring data volumes foreseen for LHC Run 3 starting in 2012, a new infrastructure has been provided by the CERN-IT Monit group, based on InfluxDB as the data store and Grafana as the display environment. ATLAS is adapting and further developing its monitoring tools to use this infrastructure for data and workflow management monitoring and accounting dashboards, expanding the range of previous possibilities with the aim of achieving a single, simpler, environment for all monitoring applications. This presentation will describe the tools used, the data flows for monitoring and accounting, the problems encountered and the solutions found.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dario</FirstName>
<FamilyName>Barberis</FamilyName>
<Email>dario.barberis@ge.infn.it</Email>
<Affiliation>University and INFN Genova (Italy)</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dario</FirstName>
<FamilyName>Barberis</FamilyName>
<Email>dario.barberis@ge.infn.it</Email>
<Affiliation>University and INFN Genova (Italy)</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>150</Id>
<Title>
The equipment part of the automatic control system of 8th Phasotron tract
</Title>
<Content>
The JINR Phasotron is the basic research facility of the Laboratory of Nuclear Problems of JINR. In 1985, a clinical complex of proton therapy of cancer patients was created on the basis of the facility. For the tasks of the complex, the 8th Phasotron tract is used. The tract consists of 15 elements: 2 rotary electromagnets and 13 electromagnetic lenses controlled by an automatic control system. The system in automatic mode ensures the achievement and maintenance of the necessary modes of operation of the elements, and allows personnel to control the operation of the beam from two control stations. The report describes the equipment of the current version of the automatic control system of the 8th Phasotron tract - the composition, operating principles, issues that arose during the implementation and their solution.
</Content>
<field id="content">
The JINR Phasotron is the basic research facility of the Laboratory of Nuclear Problems of JINR. In 1985, a clinical complex of proton therapy of cancer patients was created on the basis of the facility. For the tasks of the complex, the 8th Phasotron tract is used. The tract consists of 15 elements: 2 rotary electromagnets and 13 electromagnetic lenses controlled by an automatic control system. The system in automatic mode ensures the achievement and maintenance of the necessary modes of operation of the elements, and allows personnel to control the operation of the beam from two control stations. The report describes the equipment of the current version of the automatic control system of the 8th Phasotron tract - the composition, operating principles, issues that arose during the implementation and their solution.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Владимир</FirstName>
<FamilyName>Халин</FamilyName>
<Email>aurafoto@list.ru</Email>
<Affiliation>Михайлович</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Yudin</FamilyName>
<Email>andrew@nusun.jinr.ru</Email>
<Affiliation>Vladimirovich</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Владимир</FirstName>
<FamilyName>Халин</FamilyName>
<Email>aurafoto@list.ru</Email>
<Affiliation>Михайлович</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>151</Id>
<Title>
The software part of the automatic control system of 8th Phasotron tract
</Title>
<Content>
The JINR Phasotron is the basic research facility of the Laboratory of Nuclear Problems of JINR. In 1985, a clinical complex of proton therapy of cancer patients was created on the basis of the facility. For the tasks of the complex, the 8th Phasotron tract is used. The tract consists of 15 elements: 2 rotary electromagnets and 13 electromagnetic lenses controlled by an automatic control system. The system in automatic mode ensures the achievement and maintenance of the necessary modes of operation of the elements, and allows personnel to control the operation of the beam from two control stations. The report focuses on the software part of the current version of the automatic control system of the 8th Phazotron tract - the composition of the components, the control principles, issues that arose during the implementation and their solution.
</Content>
<field id="content">
The JINR Phasotron is the basic research facility of the Laboratory of Nuclear Problems of JINR. In 1985, a clinical complex of proton therapy of cancer patients was created on the basis of the facility. For the tasks of the complex, the 8th Phasotron tract is used. The tract consists of 15 elements: 2 rotary electromagnets and 13 electromagnetic lenses controlled by an automatic control system. The system in automatic mode ensures the achievement and maintenance of the necessary modes of operation of the elements, and allows personnel to control the operation of the beam from two control stations. The report focuses on the software part of the current version of the automatic control system of the 8th Phazotron tract - the composition of the components, the control principles, issues that arose during the implementation and their solution.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Yudin</FamilyName>
<Email>andrew@nusun.jinr.ru</Email>
<Affiliation>Vladimirovich</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Владимир</FirstName>
<FamilyName>Халин</FamilyName>
<Email>aurafoto@list.ru</Email>
<Affiliation>Михайлович</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Yudin</FamilyName>
<Email>andrew@nusun.jinr.ru</Email>
<Affiliation>Vladimirovich</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>152</Id>
<Title>
A study on performance assessment of essential clustering algorithms for the interactive visual analysis toolkit InVEx
</Title>
<Content>
Interactive visual analysis tools bring the ability of the real-time discovery of knowledge in large and complex datasets using visual analytics. It involves multiple iterations of data processing using various data handling approaches and the efficiency of the whole chain of the analysis process depends on the performance of chosen techniques and related implementations, as well as the quality of applied methods. Stages, where data processing includes intellectual handling (i.e., data mining and machine learning), which are the most resource-intensive, require a distinct attention for evaluation of different approaches. Clustering is one of such machine learning techniques that is commonly used to discover groups of data objects for further analysis. This work is focused on evaluation of clustering algorithms within the interactive visual analysis toolkit InVEx (Interactive Visual Explorer). InVEx represents a visual analytics approach aimed at cluster analysis and in-depth study of implicit correlations between multidimensional data objects. It is originally designed to enhance the analysis of computing metadata of the ATLAS experiment at the LHC for operational needs, but it also provides the same capabilities for other domains to analyze large amounts of multidimensional data. The experiments and evaluation processes are carried out using operational data from the supercomputer at the Lomonosov Moscow State University. These processes includes benchmark tests to assess the relative performance between chosen clustering algorithms and corresponding metrics to assess the quality of produced clusters. Obtained results will be used as guidelines in assisting users in a process of visual analysis using InVEx.
</Content>
<field id="content">
Interactive visual analysis tools bring the ability of the real-time discovery of knowledge in large and complex datasets using visual analytics. It involves multiple iterations of data processing using various data handling approaches and the efficiency of the whole chain of the analysis process depends on the performance of chosen techniques and related implementations, as well as the quality of applied methods. Stages, where data processing includes intellectual handling (i.e., data mining and machine learning), which are the most resource-intensive, require a distinct attention for evaluation of different approaches. Clustering is one of such machine learning techniques that is commonly used to discover groups of data objects for further analysis. This work is focused on evaluation of clustering algorithms within the interactive visual analysis toolkit InVEx (Interactive Visual Explorer). InVEx represents a visual analytics approach aimed at cluster analysis and in-depth study of implicit correlations between multidimensional data objects. It is originally designed to enhance the analysis of computing metadata of the ATLAS experiment at the LHC for operational needs, but it also provides the same capabilities for other domains to analyze large amounts of multidimensional data. The experiments and evaluation processes are carried out using operational data from the supercomputer at the Lomonosov Moscow State University. These processes includes benchmark tests to assess the relative performance between chosen clustering algorithms and corresponding metrics to assess the quality of produced clusters. Obtained results will be used as guidelines in assisting users in a process of visual analysis using InVEx.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Titov</FamilyName>
<Email>mikhail.titov@cern.ch</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Maria</FirstName>
<FamilyName>Grigorieva</FamilyName>
<Email>maria.grigorieva@cern.ch</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Aleksandr</FirstName>
<FamilyName>Alekseev</FamilyName>
<Email>frt@tpu.ru</Email>
<Affiliation>National Research Tomsk Polytechnic University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nikita</FirstName>
<FamilyName>Belov</FamilyName>
<Email>zodiac.nv@gmail.com</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Timofei</FirstName>
<FamilyName>Galkin</FamilyName>
<Email>tpgalkin@mephi.ru</Email>
<Affiliation>NRNU MEPhI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igal</FirstName>
<FamilyName>Milman</FamilyName>
<Email>igal.milman@gmail.com</Email>
<Affiliation>NRNU MEPhI</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Zhumatiy</FamilyName>
<Email>serg@parallel.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Titov</FamilyName>
<Email>mikhail.titov@cern.ch</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>153</Id>
<Title>CMS High Level Trigger performance in Run 2</Title>
<Content>
The CMS experiment selects events with a two-level trigger system, the Level-1 (L1) trigger and the High Level trigger (HLT). The HLT is a farm of approximately 30K CPU cores that reduces the rate from 100 kHz to about 1 kHz. The HLT has access to the full detector readout and runs a streamlined version of the offline event reconstruction. In Run 2 the peak instantaneous luminosity reached values above $2 \times 10^{34}$ cm$^{-2}$ sec$^{-1}$, posing a challenge to the online event selection. An overview of the object reconstruction and trigger selections used in the 2016-2018 data-taking period will be presented. The performance of the main trigger paths and the lessons learned will be summarized, also in view of the coming Run 3.
</Content>
<field id="content">
The CMS experiment selects events with a two-level trigger system, the Level-1 (L1) trigger and the High Level trigger (HLT). The HLT is a farm of approximately 30K CPU cores that reduces the rate from 100 kHz to about 1 kHz. The HLT has access to the full detector readout and runs a streamlined version of the offline event reconstruction. In Run 2 the peak instantaneous luminosity reached values above $2 \times 10^{34}$ cm$^{-2}$ sec$^{-1}$, posing a challenge to the online event selection. An overview of the object reconstruction and trigger selections used in the 2016-2018 data-taking period will be presented. The performance of the main trigger paths and the lessons learned will be summarized, also in view of the coming Run 3.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Hale</FirstName>
<FamilyName>Sert</FamilyName>
<Email>hale.sert@cern.ch</Email>
<Affiliation>RWTH Aachen University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Hale</FirstName>
<FamilyName>Sert</FamilyName>
<Email>hale.sert@cern.ch</Email>
<Affiliation>RWTH Aachen University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>154</Id>
<Title>
Application of modern commercial digitizers for new approaches to neutron's detection
</Title>
<Content>
Modern commercially available digitizers provide for a moderate price new detection approaches (pulse shape discrimination (PSD), pulse height analysis, etc.) in nuclear and particle physics. In particular, such new electronics became highly demanded for neutron's detection. One of a new detection methods is to use PSD technique for new lithium containing scintillators for effective discrimination between neutron- and gamma- events. As we found, high level of intrinsic alpha- background of these scintillators still does not allows to use such detectors in low background experiments. The actual work presented fundamentally new neutron detection method which is combination of modern digitizers with well-known NaI detectors. Method based on delayed coincidences in deexcitation of iodine-128 which is result of neutron capture on iodine-127. Sensitivity of the method has been investigated with several different digitizers.
</Content>
<field id="content">
Modern commercially available digitizers provide for a moderate price new detection approaches (pulse shape discrimination (PSD), pulse height analysis, etc.) in nuclear and particle physics. In particular, such new electronics became highly demanded for neutron's detection. One of a new detection methods is to use PSD technique for new lithium containing scintillators for effective discrimination between neutron- and gamma- events. As we found, high level of intrinsic alpha- background of these scintillators still does not allows to use such detectors in low background experiments. The actual work presented fundamentally new neutron detection method which is combination of modern digitizers with well-known NaI detectors. Method based on delayed coincidences in deexcitation of iodine-128 which is result of neutron capture on iodine-127. Sensitivity of the method has been investigated with several different digitizers.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitrii</FirstName>
<FamilyName>Ponomarev</FamilyName>
<Email>ponom@jinr.ru</Email>
<Affiliation>DLNP</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dmitrii</FirstName>
<FamilyName>Ponomarev</FamilyName>
<Email>ponom@jinr.ru</Email>
<Affiliation>DLNP</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>155</Id>
<Title>
Particle detection system for SHE synthesis at DGFRS-II
</Title>
<Content>
This talk will provide information about particle detection chain for the new Dubna gas-filled recoil separator-II. Detector chamber itself consists of time-of-flight system and implantation double-sided silicon strip detector surrounded by six single side strip detectors (overall 224 channels). Main part of the talk will be focused on PXI based Alpha & Gamma spectrometer which will handle each channel from DSSSD and side detectors independently. This leads to PC controlled PXI multi crate system with timestamp sync and more. Last part will be about DAQ software written in C++ and relevant online monitoring client.
</Content>
<field id="content">
This talk will provide information about particle detection chain for the new Dubna gas-filled recoil separator-II. Detector chamber itself consists of time-of-flight system and implantation double-sided silicon strip detector surrounded by six single side strip detectors (overall 224 channels). Main part of the talk will be focused on PXI based Alpha & Gamma spectrometer which will handle each channel from DSSSD and side detectors independently. This leads to PC controlled PXI multi crate system with timestamp sync and more. Last part will be about DAQ software written in C++ and relevant online monitoring client.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Leo</FirstName>
<FamilyName>Schlattauer</FamilyName>
<Email>leospv@gmail.com</Email>
<Affiliation>Palacky University Olomouc, Czech Republic</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Maxim</FirstName>
<FamilyName>Shumeyko</FamilyName>
<Email>eastwoodknight@rambler.ru</Email>
<Affiliation>JINR FLNR, Russia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexey</FirstName>
<FamilyName>Voinov</FamilyName>
<Email>voinov_2000@mail.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Yury</FirstName>
<FamilyName>Tsyganov</FamilyName>
<Email>tyra@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Polyakov</FamilyName>
<Email>polyakov@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>V.G.</FirstName>
<FamilyName>Subbotin</FamilyName>
<Email>voinov@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>А.М.</FirstName>
<FamilyName>Zubareva</FamilyName>
<Email>zubareva@jinr.ru</Email>
<Affiliation>FLNR JINR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>A.V.</FirstName>
<FamilyName>Podshibiakin</FamilyName>
<Email>shura.pod@gmail.com</Email>
<Affiliation>FLNR JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Leo</FirstName>
<FamilyName>Schlattauer</FamilyName>
<Email>leospv@gmail.com</Email>
<Affiliation>Palacky University Olomouc, Czech Republic</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>156</Id>
<Title>Zero-Knowledger Proof in Self-Sovereign Identity</Title>
<Content>
This article provides an overview of the currently existing technologies in the field of Self-Sovereign Identity. Special attention is paid to the zero-knowledge proof and how it can be used in distributed ledgers technologies. The work shows how to make a new user anonymous, but at the same time provide him with all the features without decreasing the level of trust to him. It will be the same as if he was fully known to the system. Particular attention is paid to the ability of users to provide access to each other's resources without losing security. The algorithms of how it is done is presented.
</Content>
<field id="content">
This article provides an overview of the currently existing technologies in the field of Self-Sovereign Identity. Special attention is paid to the zero-knowledge proof and how it can be used in distributed ledgers technologies. The work shows how to make a new user anonymous, but at the same time provide him with all the features without decreasing the level of trust to him. It will be the same as if he was fully known to the system. Particular attention is paid to the ability of users to provide access to each other's resources without losing security. The algorithms of how it is done is presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nataliia</FirstName>
<FamilyName>Kulabukhova</FamilyName>
<Email>kulabukhova.nv@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Nataliia</FirstName>
<FamilyName>Kulabukhova</FamilyName>
<Email>kulabukhova.nv@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>157</Id>
<Title>
Detector performance and stability of the CMS RPC system during Run-2
</Title>
<Content>
The CMS (Compact Muon Solenoid) experiment, at the Large Hadron Collider (LHC) in CERN explores three different gaseous detector technologies in order to measure and trigger muons: Cathode Strip Chambers (in the forward regions), Drift Tubes (in the central region), and Resistive Plate Chambers (both its central and forward regions). The CMS RPC system provides information to all muon track finders and thus ensure the robustness and redundancy to the first level of muon triggering. Different approaches have been used to monitor the detector stability during the Run-2 data taking. The summary of the CMS RPC detector performance will be presented in terms of the main detector parameters – efficiency and cluster size, including the background measurements as well.
</Content>
<field id="content">
The CMS (Compact Muon Solenoid) experiment, at the Large Hadron Collider (LHC) in CERN explores three different gaseous detector technologies in order to measure and trigger muons: Cathode Strip Chambers (in the forward regions), Drift Tubes (in the central region), and Resistive Plate Chambers (both its central and forward regions). The CMS RPC system provides information to all muon track finders and thus ensure the robustness and redundancy to the first level of muon triggering. Different approaches have been used to monitor the detector stability during the Run-2 data taking. The summary of the CMS RPC detector performance will be presented in terms of the main detector parameters – efficiency and cluster size, including the background measurements as well.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Roumyana</FirstName>
<FamilyName>Hadjiiska</FamilyName>
<Email>roumyana.mileva.hadjiiska@cern.ch</Email>
<Affiliation>Bulgarian Academy of Sciences - INRNE</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Roumyana</FirstName>
<FamilyName>Hadjiiska</FamilyName>
<Email>roumyana.mileva.hadjiiska@cern.ch</Email>
<Affiliation>Bulgarian Academy of Sciences - INRNE</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>158</Id>
<Title>
Front-End Electronics for BM@N STS, characterization and quality assurance
</Title>
<Content>
Data acquisition system (DAQ) for the Silicon Tracking System (STS) of BM@N (Dubna,Russia) experiment is described. The system will be based on double-sided microstrip silicon sensors of CBM type and will be commissioned in 2022. DAQ system of BM@N STS will operate in a data-driven mode with a high throughput bandwidth (up to 300 Gb/s) in radiation hard environment and will transmit data from more than 600 000 channels. Results of the characterisation of the Front-end electronics are presented. The key component of the Front-end board (FEB) is STS/MUCH-XYTER ASIC. Test results of the analog and digital part of the ASIC are presented. Also results of the in-beam tests of the front-end electronics are presented. Assembly of the first STS modules was already started at Joint Institute for Nuclear Research (JINR). STS modules consist of double-sided microstrip sensor, set of aluminum signal micro-cables and FEBs. Quality assurance system for the bonding quality control during the assembly was developed.
</Content>
<field id="content">
Data acquisition system (DAQ) for the Silicon Tracking System (STS) of BM@N (Dubna,Russia) experiment is described. The system will be based on double-sided microstrip silicon sensors of CBM type and will be commissioned in 2022. DAQ system of BM@N STS will operate in a data-driven mode with a high throughput bandwidth (up to 300 Gb/s) in radiation hard environment and will transmit data from more than 600 000 channels. Results of the characterisation of the Front-end electronics are presented. The key component of the Front-end board (FEB) is STS/MUCH-XYTER ASIC. Test results of the analog and digital part of the ASIC are presented. Also results of the in-beam tests of the front-end electronics are presented. Assembly of the first STS modules was already started at Joint Institute for Nuclear Research (JINR). STS modules consist of double-sided microstrip sensor, set of aluminum signal micro-cables and FEBs. Quality assurance system for the bonding quality control during the assembly was developed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Mikhail</FirstName>
<FamilyName>Shitenkov</FamilyName>
<Email>shitenkow@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>DEMENTYEV</FamilyName>
<Email>d.dementev@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Yurii</FirstName>
<FamilyName>MURIN</FamilyName>
<Email>murin@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Mikhail</FirstName>
<FamilyName>Shitenkov</FamilyName>
<Email>shitenkow@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>159</Id>
<Title>Improving Resource Usage in HPC Clouds</Title>
<Content>
HPC-as-a-service is a new cloud paradigm that represents easy to access and use cloud environments for High Performance Computing (HPC). This paradigm has been receiving a lot of attention from the research community lately since it represents a good tradeoff between computational power and usability. One of the key drawbacks associated with HPC clouds is low CPU usage due to the network communication overhead [2, 3]. Instances of HPC applications may reside on different physical machines separated by significant network latencies. Network communications between such instances may consume significant time and thus result in CPU stalls. In this paper we propose the scheduling algorithm that overcomes such drawbacks to increase the HPC task capacity in the Ethernet-based HPC cloud by sharing CPU cores between different VMs. The algorithm observes parallel tasks’ behavior and packs tasks with low CPU usage on same CPU cores. We fully implemented and evaluated our algorithm on 15 popular MPI benchmarks/libraries. The experiments have shown that we can significantly improve the CPU usage with negligible performance degradation.
</Content>
<field id="content">
HPC-as-a-service is a new cloud paradigm that represents easy to access and use cloud environments for High Performance Computing (HPC). This paradigm has been receiving a lot of attention from the research community lately since it represents a good tradeoff between computational power and usability. One of the key drawbacks associated with HPC clouds is low CPU usage due to the network communication overhead [2, 3]. Instances of HPC applications may reside on different physical machines separated by significant network latencies. Network communications between such instances may consume significant time and thus result in CPU stalls. In this paper we propose the scheduling algorithm that overcomes such drawbacks to increase the HPC task capacity in the Ethernet-based HPC cloud by sharing CPU cores between different VMs. The algorithm observes parallel tasks’ behavior and packs tasks with low CPU usage on same CPU cores. We fully implemented and evaluated our algorithm on 15 popular MPI benchmarks/libraries. The experiments have shown that we can significantly improve the CPU usage with negligible performance degradation.
</field>
<field id="summary">
During the past decade public clouds have attracted tremendous amount of interest from academic and industrial audiences as the effective and relatively cheap way to get powerful computational infrastructure without the burden of building and maintaining physical infrastructure. Although clouds are less powerful than server clusters or supercomputers [1], they are becoming more popular as a platform for High Performance Computing (HPC) due to the low cost and easy to access. Cloud providers are starting to support this interest and come up with a new cloud paradigm - HPC-as-a-service. This paradigm represents a service that gives cloud resources for computationally heavy applications. Several papers [2, 3] have shown that one of the main performance bottlenecks in HPC clouds issues from communication delays within the DС network. Such bottleneck is due to the insufficient network performance in HPC clouds. While supercomputers use fast interconnections like InfiniBand or GE, HPC clouds mostly use Ethernet. However, this bottleneck brings important impact on the behavior of the applications in HPC clouds – communication heavy HPC applications tend to underutilize the CPU. This happens because most of computationally heavy applications use network to exchange messages between physical machines. And since cloud network is not fast enough for HPC, such applications spend a lot of time idling for messages to pass through the network [2, 3]. Such behavior of HPC applications also leads to a highly regular execution and network usage pattern, i.e. HPC applications show tendency to alternate computations with frequent network communications [5]. This communication pattern contribute to the idle CPU usage since the slowest message delivery dictates the overall performance degradation of an application. These specifics of the behavior of HPC applications can be used in HPC clouds to improve the resource utilization by sharing the same CPU core between different applications, i.e. providing more virtual CPUs than there are physical ones. In this research we are proposing a scheduling algorithm that increases the resource utilization and the HPC task capacity of an Ethernet-based HPC cloud. The developed algorithm observes network behavior of HPC tasks and uses a greedy heuristic to share CPU cores between such tasks, thus improving the overall CPU usage and increasing the number of tasks performed via HPC-as-a-service. We have performed experiments with 15 popular MPI benchmarks/libraries and show that we can significantly improve CPU usage with negligible performance degradation. [1] Netto, M. A., Calheiros, R. N., Rodrigues, E. R., Cunha, R. L., & Buyya, R. (2018). HPC cloud for scientific and business applications: Taxonomy, vision, and research challenges. ACM Computing Surveys (CSUR), 51(1), 8. [2] Gupta, A., Faraboschi, P., Gioachin, F., Kale, L. V., Kaufmann, R., Lee, B. S., ... & Suen, C. H. (2016). Evaluating and improving the performance and scheduling of HPC applications in cloud. IEEE Transactions on Cloud Computing, 4(3), 307-321. [3] Gupta, A., & Milojicic, D. (2011, October). Evaluation of hpc applications on cloud. In 2011 Sixth Open Cirrus Summit (pp. 22-26). IEEE.
</field>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Petrov</FamilyName>
<Email>ipetrov@cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Vitaly</FirstName>
<FamilyName>Antonenko</FamilyName>
<Email>anvial@lvk.cs.msu.su</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ruslan</FirstName>
<FamilyName>Smeliansky</FamilyName>
<Email>smel@cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Petrov</FamilyName>
<Email>ipetrov@cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>160</Id>
<Title>
New read-out electronics for the Drift-Tube Chambers of CMS
</Title>
<Content>
The Drift Tubes (DT) system is the key detector in the region of the CMS barrel dedicated to the measurement of muon tracks. The signals from about 172000 DT cells must be fast and synchronously acquired to deliver the information about the hits. In the context of increasing the luminosity of the LHC in preparation for the Phase-II the DT system is being upgraded. The main focus of this upgrade is the development of a new generation of read-out electronics based on the FPGA technology. The new on-chamber electronics will provide higher acquisition rates, radiation resistance and flexibility of the trigger settings for the DT system. The DT-chambers will be equipped, depending on chamber type, with 3 to 5 single type boards called OBDTs (On Board electronics for Drift Tubes). Along with better read-out characteristics, the OBDTs ensure less intermediate elements in the read-out chain. The talk presents an overview of the OBDT architecture. Special attention will be given to the explanation of the FPGA firmware structure and functionality.
</Content>
<field id="content">
The Drift Tubes (DT) system is the key detector in the region of the CMS barrel dedicated to the measurement of muon tracks. The signals from about 172000 DT cells must be fast and synchronously acquired to deliver the information about the hits. In the context of increasing the luminosity of the LHC in preparation for the Phase-II the DT system is being upgraded. The main focus of this upgrade is the development of a new generation of read-out electronics based on the FPGA technology. The new on-chamber electronics will provide higher acquisition rates, radiation resistance and flexibility of the trigger settings for the DT system. The DT-chambers will be equipped, depending on chamber type, with 3 to 5 single type boards called OBDTs (On Board electronics for Drift Tubes). Along with better read-out characteristics, the OBDTs ensure less intermediate elements in the read-out chain. The talk presents an overview of the OBDT architecture. Special attention will be given to the explanation of the FPGA firmware structure and functionality.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Eliseev</FamilyName>
<Email>eliseev@physik.rwth-aachen.de</Email>
<Affiliation>RWTH Aachen University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Marco</FirstName>
<FamilyName>Bellato</FamilyName>
<Email>marco.bellato@pf.infn.it</Email>
<Affiliation>Istituto Nazionale di Fisica Nucleare</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Cristina F.</FirstName>
<FamilyName>Bedoya</FamilyName>
<Email>cristina.fernandez@ciemat.es</Email>
<Affiliation>
Centro de Investigaciones Energéticas, Medioambientales y Tecnológicas
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Thomas</FirstName>
<FamilyName>Hebbeker</FamilyName>
<Email>hebbeker@physik.rwth-aachen.de</Email>
<Affiliation>RWTH Aachen University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Alvaro</FirstName>
<FamilyName>Navarro</FamilyName>
<Email>alvaro.navarro@ciemat.es</Email>
<Affiliation>
Centro de Investigaciones Energéticas, Medioambientales y Tecnológicas
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ignacio</FirstName>
<FamilyName>Redondo</FamilyName>
<Email>ignacio.redondo@ciemat.es</Email>
<Affiliation>
Centro de Investigaciones Energéticas, Medioambientales y Tecnológicas
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Javier</FirstName>
<FamilyName>Sastre</FamilyName>
<Email>javier.sastre@ciemat.es</Email>
<Affiliation>
Centro de Investigaciones Energéticas, Medioambientales y Tecnológicas
</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrea</FirstName>
<FamilyName>Triossi</FamilyName>
<Email>triossi@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Sandro</FirstName>
<FamilyName>Ventura</FamilyName>
<Email>sandro.ventura@pd.infn.it</Email>
<Affiliation>Istituto Nazionale de Fisica Nucleare</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Dmitry</FirstName>
<FamilyName>Eliseev</FamilyName>
<Email>eliseev@physik.rwth-aachen.de</Email>
<Affiliation>RWTH Aachen University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>161</Id>
<Title>MC2E: Meta-Cloud Computing Environment for HPC</Title>
<Content>
Modern practical research in physics, chemistry and biology has shifted in the area of simulations, experimental results’ processing and data mining, thus imposing immense demands on computational resources. The problem is that due to the heterogeneous nature such resources may have a high variance in their load. So users may wait for weeks until their job is done, even though there is plenty of resources available on other platforms. Such problem arises because various platforms may have significantly different APIs and when researchers used to work with one interface it’s often expensive to fit their software to work with some other interface. In this research we present MC2E - an environment for academic multidisciplinary research. MC2E aggregates heterogeneous resources such as private/public clouds, HPC clusters and supercomputers under a unified easy-to-use interface.
</Content>
<field id="content">
Modern practical research in physics, chemistry and biology has shifted in the area of simulations, experimental results’ processing and data mining, thus imposing immense demands on computational resources. The problem is that due to the heterogeneous nature such resources may have a high variance in their load. So users may wait for weeks until their job is done, even though there is plenty of resources available on other platforms. Such problem arises because various platforms may have significantly different APIs and when researchers used to work with one interface it’s often expensive to fit their software to work with some other interface. In this research we present MC2E - an environment for academic multidisciplinary research. MC2E aggregates heterogeneous resources such as private/public clouds, HPC clusters and supercomputers under a unified easy-to-use interface.
</field>
<field id="summary">
Today's research in various fields such as physics, chemistry and biology have shows large demands in computational resources due to the complexity of tasks performed. Such resources are often provided as supercomputers and clusters for High Performance Computing (HPC). The problem is that such platforms may have a high variance in their load. So more popular platforms may have large queues and significant waiting times, while other platforms are vacant. On-demand clouds could help solving this problem by offering virtualized resources customized for specific purposes. Such clouds offer more flexible and convenient platform for researchers to execute their computational tasks, but their heterogeneity makes it hard to switch between platforms if some platform becomes highly loaded or inaccessible. This happens because various resource providers have different interfaces (APIs) for task submission. So in order to change the target platform researchers need to spend time and resources to adjust their software for the new API. In this research we propose MC2E - an environment for multidisciplinary academic research that aggregates heterogeneous resources such as private/public clouds, HPC clusters and supercomputers under a unified easy-to-use interface. Comparing with “traditional” resource orchestration in data centers, that use free tools like OpenStack or commercial provided by VMware, MC2E offers a number of new features/opportunities and advantages: - MC2E provides higher level of resource control (a set of platforms instead of a single local data center or HPC cluster); - It provides users with more flexible capabilities to define virtual environments, more types of resources and services; - It supplies higher quality of resource scheduling and utilization; - It relieves a user from tedious system administration tasks; - It specifies a unified way to describe and support the data center (or HPC cluster) service livecycle. Allowing an MC2E user to use his software for providing experiments on MC2E infrastructure. MC2E enlarges the concepts of PaaS and IaaS to the scientific applications area. We believe that it could be of great help to research groups that work jointly and need a shared virtual collaboration environment with resources from different geographically distributed platforms.
</field>
<PrimaryAuthor>
<FirstName>Ivan</FirstName>
<FamilyName>Petrov</FamilyName>
<Email>ipetrov@cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ruslan</FirstName>
<FamilyName>Smeliansky</FamilyName>
<Email>smel@cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Vitaly</FirstName>
<FamilyName>Antonenko</FamilyName>
<Email>anvial@lvk.cs.msu.su</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Zhenchun</FirstName>
<FamilyName>Huang</FamilyName>
<Email>huangzc@tsinghua.edu.cn</Email>
<Affiliation>Tsinghua University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Min</FirstName>
<FamilyName>Chen</FamilyName>
<Email>minchen2012@hust.edu.cn</Email>
<Affiliation>Huazhong University of Science & Technology</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Donggang</FirstName>
<FamilyName>Cao</FamilyName>
<Email>caodg@pku.edu.cn</Email>
<Affiliation>Peking University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Xiangqun</FirstName>
<FamilyName>Chen</FamilyName>
<Email>cherry@pku.edu.cn</Email>
<Affiliation>Peking University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anatoly</FirstName>
<FamilyName>Bahmurov</FamilyName>
<Email>bahmurov@lvk.cs.msu.su</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ruslan</FirstName>
<FamilyName>Smeliansky</FamilyName>
<Email>smel@cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>162</Id>
<Title>On Bandwidth on Demand Problem</Title>
<Content>
In recent years there is a trend in a backbone traffic growth between Data Centers (DC). According to TeleGeography on the most demanded route across the Atlantic ocean, by the end of 2017 the share of such traffic had reached 75%, and in 2023 it will exceed 93%. It can be explained by the development of the global cloud service market, which is currently concentrated in North America and Europe. Therefore the traffic growth between DC is provided mainly by the DCs of cloud providers, as well as enterprise DC that use hybrid clouds. But cloud DC impose special requirements on channel bandwidth allocation and charging policy. The most promising approach to satisfy these requirements is to provide the channel bandwidth on the model “pay and go” - only when there is a need in it, i.e. the bandwidth on demand. Having a high penetration of SDN and NFV technologies within DC, cloud providers and their enterprise customers impose requirements on the SDN and NFV technologies implementation for backbone networks allowing bandwidth on demand (BoD service) to balance the computational load and to carry out the data migration between DCs. In this paper we consider the possible protocols and technologies on different OSI Reference model levels that can help with the implementation of bandwidth on demand. On the transport and network level we can engage multipath protocols that allow the data transmission through several routes simultaneously (we will call data flows on different routes as subflows). There are two schemes of such route generation: static and dynamic. In static route generation scheme there is always the same number of used routes. We will note the drawbacks of such approach that show favour to dynamic scheme when the routes are allocated dynamically in dependence of current network load and bandwidth requirements. Also we discuss the balancing techniques (Equal Cost Multi-Path (ECMP), Ethernet VPN (EVPN), Link Aggregation Group (LAG)), that allow to route different transport flows through different disjoint channels. It can give an advantage when used together with multipath protocols. We also present the mathematical problem statement for the bandwidth on demand problem and considering this problem in relation to the optical transport network (OTN). The main issue here is that optical network is a channel switching network whereas OSI Reference model was developed for packet switching network. We assume that optical network contains Reconfigurable Optical Add-Drop Multiplexors (ROADM) to offer the flexibility to add wavelengths or easily change their destination. In addition, they can be managed remotely, providing full control and monitoring over the entire high capacity infrastructure. To implement bandwidth on demand, the network providers should maintain the certain level of bandwidth reservation. In this paper we wouldn't consider the problem of reservation size selection and would assume that required bandwidth is always available for clients. With such assumption and Menger's theorem we can deduce the number of wavelength that is needed to the data transfer with a given bandwidth requirement. This work is supported by Russian Ministry of Science and Higher Education, grant #05.613.21.0088, unique ID RFMEFI61318X0088.
</Content>
<field id="content">
In recent years there is a trend in a backbone traffic growth between Data Centers (DC). According to TeleGeography on the most demanded route across the Atlantic ocean, by the end of 2017 the share of such traffic had reached 75%, and in 2023 it will exceed 93%. It can be explained by the development of the global cloud service market, which is currently concentrated in North America and Europe. Therefore the traffic growth between DC is provided mainly by the DCs of cloud providers, as well as enterprise DC that use hybrid clouds. But cloud DC impose special requirements on channel bandwidth allocation and charging policy. The most promising approach to satisfy these requirements is to provide the channel bandwidth on the model “pay and go” - only when there is a need in it, i.e. the bandwidth on demand. Having a high penetration of SDN and NFV technologies within DC, cloud providers and their enterprise customers impose requirements on the SDN and NFV technologies implementation for backbone networks allowing bandwidth on demand (BoD service) to balance the computational load and to carry out the data migration between DCs. In this paper we consider the possible protocols and technologies on different OSI Reference model levels that can help with the implementation of bandwidth on demand. On the transport and network level we can engage multipath protocols that allow the data transmission through several routes simultaneously (we will call data flows on different routes as subflows). There are two schemes of such route generation: static and dynamic. In static route generation scheme there is always the same number of used routes. We will note the drawbacks of such approach that show favour to dynamic scheme when the routes are allocated dynamically in dependence of current network load and bandwidth requirements. Also we discuss the balancing techniques (Equal Cost Multi-Path (ECMP), Ethernet VPN (EVPN), Link Aggregation Group (LAG)), that allow to route different transport flows through different disjoint channels. It can give an advantage when used together with multipath protocols. We also present the mathematical problem statement for the bandwidth on demand problem and considering this problem in relation to the optical transport network (OTN). The main issue here is that optical network is a channel switching network whereas OSI Reference model was developed for packet switching network. We assume that optical network contains Reconfigurable Optical Add-Drop Multiplexors (ROADM) to offer the flexibility to add wavelengths or easily change their destination. In addition, they can be managed remotely, providing full control and monitoring over the entire high capacity infrastructure. To implement bandwidth on demand, the network providers should maintain the certain level of bandwidth reservation. In this paper we wouldn't consider the problem of reservation size selection and would assume that required bandwidth is always available for clients. With such assumption and Menger's theorem we can deduce the number of wavelength that is needed to the data transfer with a given bandwidth requirement. This work is supported by Russian Ministry of Science and Higher Education, grant #05.613.21.0088, unique ID RFMEFI61318X0088.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Evgeniy</FirstName>
<FamilyName>Stepanov</FamilyName>
<Email>estepanov@lvk.cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Ruslan</FirstName>
<FamilyName>Smeliansky</FamilyName>
<Email>smel@cs.msu.su</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Evgeniy</FirstName>
<FamilyName>Stepanov</FamilyName>
<Email>estepanov@lvk.cs.msu.ru</Email>
<Affiliation>Lomonosov Moscow State University</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
</abstract>
<abstract>
<Id>163</Id>
<Title>
The system of automatically processing and analyzing neurobiological experimental data of the Kurchatov Institute Resource Center.
</Title>
<Content>
The report will present a project of developing the system, based on the Kurchatov Institute Computing Center, provides the automatically processing and analysis of experimental data, obtained on the equipment of the Kurchatov Institute Resource Center (Tomograph Siemens Verio Magnetom 3T). In addition to the processing and analysis module, the system also includes a database, that provides centralized orderly storage of experimental data, and the results of their processing and analisys. User's works with system is provided by a web interface, that allow work with data at any time and any place. In particular, the user is provided with access to virtual machines with preinstalled specialized software, enabling remote viewing of neuroimaging results. The implementation of the system allows: speed up processing and analysis of data due to the parallelization of computational processes; provide the wide of choice of processing methods and analysis data by deploying a large number of specialized software packages; provide the intelligent search engine for data mining and future data analysis; organize a single centralized database of experimental data and the result of their processing and analysis; provide the ability to work with data at any time and from any place.
</Content>
<field id="content">
The report will present a project of developing the system, based on the Kurchatov Institute Computing Center, provides the automatically processing and analysis of experimental data, obtained on the equipment of the Kurchatov Institute Resource Center (Tomograph Siemens Verio Magnetom 3T). In addition to the processing and analysis module, the system also includes a database, that provides centralized orderly storage of experimental data, and the results of their processing and analisys. User's works with system is provided by a web interface, that allow work with data at any time and any place. In particular, the user is provided with access to virtual machines with preinstalled specialized software, enabling remote viewing of neuroimaging results. The implementation of the system allows: speed up processing and analysis of data due to the parallelization of computational processes; provide the wide of choice of processing methods and analysis data by deploying a large number of specialized software packages; provide the intelligent search engine for data mining and future data analysis; organize a single centralized database of experimental data and the result of their processing and analysis; provide the ability to work with data at any time and from any place.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Irina</FirstName>
<FamilyName>Enyagina</FamilyName>
<Email>irina_enyagina@mail.ru</Email>
<Affiliation>Kurchatov Institute</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Polyakov</FamilyName>
<Email>andrew@kiae.ru</Email>
<Affiliation>Kurchatov Institute</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Irina</FirstName>
<FamilyName>Enyagina</FamilyName>
<Email>irina_enyagina@mail.ru</Email>
<Affiliation>Kurchatov Institute</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>164</Id>
<Title>
Benchmarking HTC and HPC resources using HEP experiment workflows
</Title>
<Content>
The benchmarking and accounting of compute resources in WLCG needs to be revised in view of the adoption by the LHC experiments of heterogeneous computing resources based on x86 CPUs, GPUs, FPGAs. After evaluating several alternatives for the replacement of HS06, the HEPIX benchmarking WG has chosen to focus on the development of a HEP-specific suite based on actual software workloads of the LHC experiments, rather than on a standard industrial benchmark like the new SPEC CPU 2017 suite. This presentation will describe the motivation and implementation of this new benchmark suite, which is based on container technologies to ensure portability and reproducibility. This approach is designed to provide a better correlation between the new benchmark and the actual production workloads of the experiments. It also offers the possibility to separately explore and describe the independent architectural features of different computing resource types, which is expected to be increasingly important with the growing heterogeneity of the HEP computing landscape. In particular, an overview of the initial developments to address the benchmarking of non-traditional computing resources such as HPCs and GPUs will also be provided. On behalf of the HEPiX CPU Benchmarking Working Group [1,2] [1] https://w3.hepix.org/benchmarking.html [2] https://twiki.cern.ch/twiki/bin/view/HEPIX/CpuBenchmark
</Content>
<field id="content">
The benchmarking and accounting of compute resources in WLCG needs to be revised in view of the adoption by the LHC experiments of heterogeneous computing resources based on x86 CPUs, GPUs, FPGAs. After evaluating several alternatives for the replacement of HS06, the HEPIX benchmarking WG has chosen to focus on the development of a HEP-specific suite based on actual software workloads of the LHC experiments, rather than on a standard industrial benchmark like the new SPEC CPU 2017 suite. This presentation will describe the motivation and implementation of this new benchmark suite, which is based on container technologies to ensure portability and reproducibility. This approach is designed to provide a better correlation between the new benchmark and the actual production workloads of the experiments. It also offers the possibility to separately explore and describe the independent architectural features of different computing resource types, which is expected to be increasingly important with the growing heterogeneity of the HEP computing landscape. In particular, an overview of the initial developments to address the benchmarking of non-traditional computing resources such as HPCs and GPUs will also be provided. On behalf of the HEPiX CPU Benchmarking Working Group [1,2] [1] https://w3.hepix.org/benchmarking.html [2] https://twiki.cern.ch/twiki/bin/view/HEPIX/CpuBenchmark
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Domenico</FirstName>
<FamilyName>Giordano</FamilyName>
<Email>domenico.giordano@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Domenico</FirstName>
<FamilyName>Giordano</FamilyName>
<Email>domenico.giordano@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>165</Id>
<Title>Structural approach to the deep learning method</Title>
<Content>
When considering any method customary to distinguish several structural levels: syntax, semantics, pragmatics. Syntax gives the ability to apply the method in question, semantics helps set tasks, and pragmatics answers the questions: what is the essence method, what is the place of the method among other methods. In this paper, the authors apply this approach to the consideration of thedeep learning. Considers the syntax: how can practically use this method. Semantics: what are the approaches exist under this method, how do these approaches relate to solvable problems. Pragmatics: the genesis of this method is examined, reasons for its popularity, possible applications of this method, its restrictions. As a result, the authors conclude about the prospects for the use of the deep learning method for a number of practical problems.
</Content>
<field id="content">
When considering any method customary to distinguish several structural levels: syntax, semantics, pragmatics. Syntax gives the ability to apply the method in question, semantics helps set tasks, and pragmatics answers the questions: what is the essence method, what is the place of the method among other methods. In this paper, the authors apply this approach to the consideration of thedeep learning. Considers the syntax: how can practically use this method. Semantics: what are the approaches exist under this method, how do these approaches relate to solvable problems. Pragmatics: the genesis of this method is examined, reasons for its popularity, possible applications of this method, its restrictions. As a result, the authors conclude about the prospects for the use of the deep learning method for a number of practical problems.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Dmitry</FirstName>
<FamilyName>Kulyabov</FamilyName>
<Email>yamadharma@gmail.com</Email>
<Affiliation>PFUR & JINR</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Leonid</FirstName>
<FamilyName>Sevastyanov</FamilyName>
<Email>leonid.sevast@gmail.com</Email>
<Affiliation>PFUR</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Anna</FirstName>
<FamilyName>Korolkova</FamilyName>
<Email>avkorolkova@gmail.com</Email>
<Affiliation>Peoples' Friendship University of Russia</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Dmitry</FirstName>
<FamilyName>Kulyabov</FamilyName>
<Email>yamadharma@gmail.com</Email>
<Affiliation>PFUR & JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>166</Id>
<Title>
Wide-area networking for RDIG consortium: status update and plans
</Title>
<Content>
We present the current status of the wide-area networking for Russian RDIG/WLCG-sites in context of LHCOPN, LHCONE, research & educational networks: M/Light traffic exchange in Moscow, local and international links. Detailed traffic statistics and trends overview for the IPv4 and IPv6 traffic will be presented; they will be supplemented with the current policies and traffic handling methods. We will also talk about concrete plans for 2020 and network growth strategies for 2021-2022.
</Content>
<field id="content">
We present the current status of the wide-area networking for Russian RDIG/WLCG-sites in context of LHCOPN, LHCONE, research & educational networks: M/Light traffic exchange in Moscow, local and international links. Detailed traffic statistics and trends overview for the IPv4 and IPv6 traffic will be presented; they will be supplemented with the current policies and traffic handling methods. We will also talk about concrete plans for 2020 and network growth strategies for 2021-2022.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Eygene</FirstName>
<FamilyName>Ryabinkin</FamilyName>
<Email>rea@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Yury</FirstName>
<FamilyName>Gugel</FamilyName>
<Email>gugel@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Shitov</FamilyName>
<Email>andreyss@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Eygene</FirstName>
<FamilyName>Ryabinkin</FamilyName>
<Email>rea@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>167</Id>
<Title>Experiments with GABRIELA detector system.</Title>
<Content>
For several years, on SHELS (Separator for Heavy ELements Spectroscopy) was carried out more dozen experiments, aimed to investigation of characteristics of heavy elements and discover new isotopes. Projectiles from Ne-22 to Cr-54, and targets of Pb-204,206,208, Bi-209, U-236,238 were used. Perfect data acquisition system GABRIELA lets fix 70% alpha particles and 90% gamma-quanta by spontaneous fission, and also accurately to separate events by time (1μs). The mixing of α- decay with γ- and β-decay spectroscopy allows to investigate single particle states behavior, as well as the structure of little known elements in the Z = 100-104 and N = 152-162 region.
</Content>
<field id="content">
For several years, on SHELS (Separator for Heavy ELements Spectroscopy) was carried out more dozen experiments, aimed to investigation of characteristics of heavy elements and discover new isotopes. Projectiles from Ne-22 to Cr-54, and targets of Pb-204,206,208, Bi-209, U-236,238 were used. Perfect data acquisition system GABRIELA lets fix 70% alpha particles and 90% gamma-quanta by spontaneous fission, and also accurately to separate events by time (1μs). The mixing of α- decay with γ- and β-decay spectroscopy allows to investigate single particle states behavior, as well as the structure of little known elements in the Z = 100-104 and N = 152-162 region.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alena</FirstName>
<FamilyName>Kuznetsova</FamilyName>
<Email>tanstudent@gmail.com</Email>
<Affiliation>Alekseevna</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alena</FirstName>
<FamilyName>Kuznetsova</FamilyName>
<Email>tanstudent@gmail.com</Email>
<Affiliation>Alekseevna</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>168</Id>
<Title>Технологии НR для обслуживания работников</Title>
<Content>
В Объединенном институте ядерных исследований (ОИЯИ) в г.Дубна для получения справочной и кадровой информации установлены информационные киоски сенсорного типа, которые имеет в конструкции своего корпуса сенсорный экран, а также компьютер и принтер. Сотрудник получает расчетный листок по почте, но при желании сотрудник может распечатать его с помощью информационного киска. Это требование статьи 136 Трудового кодекса РФ, о том, что доложена быть предоставлена организацией возможность вывода документа на бумажный носитель. Так же с помощью киоска сотрудник ОИЯИ может узнать и вывести на принтер размер начисленной зарплаты и премии, уточнить количество неиспользованных дней отпуска, подать заявление на внеочередной отпуск, оформить командировку, заказать справку, узнать свой табельный номер и СНИЛС. Все это работники ОИЯИ могут сделать сами и быстро. Преимущество информационных киосков в том, что они позволяют исключить утечку персональных данных. Анализ показал, что у части рабочего персонала ОИЯИ не всегда имеется возможность прийти за консультацией в администрацию своего подразделения так как его рабочее место находится на значительном удалении или, работник работает по сменному графику. В результате панируется создать сеть информационных киосков в подразделениях ОИЯИ.
</Content>
<field id="content">
В Объединенном институте ядерных исследований (ОИЯИ) в г.Дубна для получения справочной и кадровой информации установлены информационные киоски сенсорного типа, которые имеет в конструкции своего корпуса сенсорный экран, а также компьютер и принтер. Сотрудник получает расчетный листок по почте, но при желании сотрудник может распечатать его с помощью информационного киска. Это требование статьи 136 Трудового кодекса РФ, о том, что доложена быть предоставлена организацией возможность вывода документа на бумажный носитель. Так же с помощью киоска сотрудник ОИЯИ может узнать и вывести на принтер размер начисленной зарплаты и премии, уточнить количество неиспользованных дней отпуска, подать заявление на внеочередной отпуск, оформить командировку, заказать справку, узнать свой табельный номер и СНИЛС. Все это работники ОИЯИ могут сделать сами и быстро. Преимущество информационных киосков в том, что они позволяют исключить утечку персональных данных. Анализ показал, что у части рабочего персонала ОИЯИ не всегда имеется возможность прийти за консультацией в администрацию своего подразделения так как его рабочее место находится на значительном удалении или, работник работает по сменному графику. В результате панируется создать сеть информационных киосков в подразделениях ОИЯИ.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Татьяна</FirstName>
<FamilyName>Тюпикова</FamilyName>
<Email>tanya@jinr.ru</Email>
<Affiliation>Викторовна</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Валерий</FirstName>
<FamilyName>Борисовский</FamilyName>
<Email>vborisov@jinr.ru</Email>
<Affiliation>Федорович</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Александр</FirstName>
<FamilyName>Зорин</FamilyName>
<Email>fergys321@mail.ru</Email>
<Affiliation>Геннадиевич</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Татьяна</FirstName>
<FamilyName>Тюпикова</FamilyName>
<Email>tanya@jinr.ru</Email>
<Affiliation>Викторовна</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>169</Id>
<Title>
Разработка электронного фотоархива Объединённого института ядерных исследований
</Title>
<Content>
В современном мире невозможно представить свою жизнь без фотографий, ведь фотография является историей человека или организации. Работа посвящена разработке и реализации фотоархива ОИЯИ. В работе представлен разработанный электронный фотоархив, который обеспечивает: •	хранения оцифрованных негативов и их описание; •	разделения прав доступа к информационным объектам базы данных; •	регистрации действий персонала; •	поиска фотографий по разным характеристикам; •	защиты от несанкционированного доступа к данным. Неотъемлемой частью электронного фотоархива является возможность быстрого поиска фотографий, что позволяет находить папки с фотографиями не только по теме, но и по каким-либо другим запросам – жанру, времени, месту и так далее. Главный плюс фотоархива – это сэкономленное время при поиске, а время, как известно, бесценно!
</Content>
<field id="content">
В современном мире невозможно представить свою жизнь без фотографий, ведь фотография является историей человека или организации. Работа посвящена разработке и реализации фотоархива ОИЯИ. В работе представлен разработанный электронный фотоархив, который обеспечивает: •	хранения оцифрованных негативов и их описание; •	разделения прав доступа к информационным объектам базы данных; •	регистрации действий персонала; •	поиска фотографий по разным характеристикам; •	защиты от несанкционированного доступа к данным. Неотъемлемой частью электронного фотоархива является возможность быстрого поиска фотографий, что позволяет находить папки с фотографиями не только по теме, но и по каким-либо другим запросам – жанру, времени, месту и так далее. Главный плюс фотоархива – это сэкономленное время при поиске, а время, как известно, бесценно!
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Татьяна</FirstName>
<FamilyName>Тюпикова</FamilyName>
<Email>tanya@jinr.ru</Email>
<Affiliation>ОИЯИ, НТО АСУ</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Valeriy</FirstName>
<FamilyName>Borisovskiy</FamilyName>
<Email>vborisov@jinr.ru</Email>
<Affiliation>Fedorovich</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Борис</FirstName>
<FamilyName>Старченко</FamilyName>
<Email>bstar@jinr.ru</Email>
<Affiliation>Михайлович</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Евгений</FirstName>
<FamilyName>Горячкин</FamilyName>
<Email>goryachkin-jinr@yandex.ru</Email>
<Affiliation>Игоревич</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Татьяна</FirstName>
<FamilyName>Тюпикова</FamilyName>
<Email>tanya@jinr.ru</Email>
<Affiliation>ОИЯИ, НТО АСУ</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Advanced Technologies for the High-Intensity Domains of Science and Business Applications
</Track>
</abstract>
<abstract>
<Id>170</Id>
<Title>
Detector performance of the CMS Precision Proton Spectrometer during LHC Run2 and its upgrades for run3.
</Title>
<Content>
The CMS Precision Proton Spectrometer (PPS) consists of silicon tracking stations as well as timing detectors to measure both the position and direction of protons and their time-of-flight with high precision. Special devices called Roman Pots are used to insert the detectors inside the LHC beam pipe to allow the detection of scattered protons close to the beam itself. They are located at around 200 m from the interaction point in the very forward region on both sides of the CMS experiment. The tracking system consists of 3D pixel silicon detectors while the timing system is made of Diamond pixel detectors and Ultra Fast Silicon Detectors. PPS has taken data at high luminosity while fully integrated to the CMS experiment. The total data collected correspond to around 100 /fb during the LHC Run2. In this presentation, the PPS detector operation, commissioning and performance are discussed, as well as the upgrades foreseen for Run3.
</Content>
<field id="content">
The CMS Precision Proton Spectrometer (PPS) consists of silicon tracking stations as well as timing detectors to measure both the position and direction of protons and their time-of-flight with high precision. Special devices called Roman Pots are used to insert the detectors inside the LHC beam pipe to allow the detection of scattered protons close to the beam itself. They are located at around 200 m from the interaction point in the very forward region on both sides of the CMS experiment. The tracking system consists of 3D pixel silicon detectors while the timing system is made of Diamond pixel detectors and Ultra Fast Silicon Detectors. PPS has taken data at high luminosity while fully integrated to the CMS experiment. The total data collected correspond to around 100 /fb during the LHC Run2. In this presentation, the PPS detector operation, commissioning and performance are discussed, as well as the upgrades foreseen for Run3.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Fabrizio</FirstName>
<FamilyName>Ferro</FamilyName>
<Email>ferro@ge.infn.it</Email>
<Affiliation>INFN Genova</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Fabrizio</FirstName>
<FamilyName>Ferro</FamilyName>
<Email>ferro@ge.infn.it</Email>
<Affiliation>INFN Genova</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>171</Id>
<Title>
Identification of tau lepton using Deep Learning techniques at CMS
</Title>
<Content>
The reconstruction and identification of tau lepton in semi-leptonic (hereinafter referred to as hadronic decays) are crucial for all analyses with tau leptons in the final state. To discriminate the hadronic decays of tau from all 3 main backgrounds (quark or gluon jets, electrons, and muons), maintaining a low rate of misidentification (below 1%) and at the same time with high efficiency on the signal, the information of multiple CMS sub-detectors must be combined. Application of deep machine learning techniques allows exploiting the available information in a very efficient way. Introduction of a new multi-class DNN-based discriminator provides considerable improvement of the tau identification performance at CMS.
</Content>
<field id="content">
The reconstruction and identification of tau lepton in semi-leptonic (hereinafter referred to as hadronic decays) are crucial for all analyses with tau leptons in the final state. To discriminate the hadronic decays of tau from all 3 main backgrounds (quark or gluon jets, electrons, and muons), maintaining a low rate of misidentification (below 1%) and at the same time with high efficiency on the signal, the information of multiple CMS sub-detectors must be combined. Application of deep machine learning techniques allows exploiting the available information in a very efficient way. Introduction of a new multi-class DNN-based discriminator provides considerable improvement of the tau identification performance at CMS.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Konstantin</FirstName>
<FamilyName>Androsov</FamilyName>
<Email>konstantin.androsov@cern.ch</Email>
<Affiliation>INFN Pisa (Italy)</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Konstantin</FirstName>
<FamilyName>Androsov</FamilyName>
<Email>konstantin.androsov@cern.ch</Email>
<Affiliation>INFN Pisa (Italy)</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>172</Id>
<Title>
Design challenges of the CMS High Granularity Calorimeter Level 1 trigger
</Title>
<Content>
The high luminosity (HL) LHC will pose significant detector challenges for radiation tolerance and event pileup, especially for forward calorimetry, and this will provide a benchmark for future hadron colliders. The CMS experiment has chosen a novel high granularity calorimeter (HGCAL) for the forward region as part of its planned Phase 2 upgrade for the HL-LHC. Based largely on silicon sensors, the HGCAL features unprecedented transverse and longitudinal readout segmentation which will be exploited in the upgraded Level 1 (L1) trigger system. The high channel granularity results in around one million trigger channels in total, to be compared with the 2000 trigger channels in the endcaps of the current detector. This presents a significant challenge in terms of data manipulation and processing for the trigger. In addition, the high luminosity will result in an average of 140 interactions per bunch crossing that give a huge background rate in the forward region and these will need to be efficiently rejected by the trigger algorithms. Furthermore, three-dimensional reconstruction of the HGCAL clusters in events with high hit rates is also a more complex computational problem for the trigger than the two-dimensional reconstruction in the current CMS calorimeter trigger. The status of the trigger architecture and design, as well as the concepts for the algorithms needed in order to tackle these major issues and their impact on trigger object performance, will be presented.
</Content>
<field id="content">
The high luminosity (HL) LHC will pose significant detector challenges for radiation tolerance and event pileup, especially for forward calorimetry, and this will provide a benchmark for future hadron colliders. The CMS experiment has chosen a novel high granularity calorimeter (HGCAL) for the forward region as part of its planned Phase 2 upgrade for the HL-LHC. Based largely on silicon sensors, the HGCAL features unprecedented transverse and longitudinal readout segmentation which will be exploited in the upgraded Level 1 (L1) trigger system. The high channel granularity results in around one million trigger channels in total, to be compared with the 2000 trigger channels in the endcaps of the current detector. This presents a significant challenge in terms of data manipulation and processing for the trigger. In addition, the high luminosity will result in an average of 140 interactions per bunch crossing that give a huge background rate in the forward region and these will need to be efficiently rejected by the trigger algorithms. Furthermore, three-dimensional reconstruction of the HGCAL clusters in events with high hit rates is also a more complex computational problem for the trigger than the two-dimensional reconstruction in the current CMS calorimeter trigger. The status of the trigger architecture and design, as well as the concepts for the algorithms needed in order to tackle these major issues and their impact on trigger object performance, will be presented.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Vito</FirstName>
<FamilyName>Palladino</FamilyName>
<Email>vitopalladino@gmail.com</Email>
<Affiliation>Imperial College London</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Vito</FirstName>
<FamilyName>Palladino</FamilyName>
<Email>vitopalladino@gmail.com</Email>
<Affiliation>Imperial College London</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>173</Id>
<Title>Database ecosystem is the way to Data Lakes</Title>
<Content>
The paper examines the existing solutions design of various data warehouses. The main trends in the development of technologies are identified. An analysis of existing big data classifications allowed us to offer our own measure to determine the category of data. On its basis, a new classification of big data has been proposed (taking into account the CAP theorem). A description of the characteristics of the data for each class is given. The developed big data classification is aimed at solving the problems of selecting tools for the development of an ecosystem. The practical significance of the results obtained is shown by the example of determining the type of big data of actual information systems.
</Content>
<field id="content">
The paper examines the existing solutions design of various data warehouses. The main trends in the development of technologies are identified. An analysis of existing big data classifications allowed us to offer our own measure to determine the category of data. On its basis, a new classification of big data has been proposed (taking into account the CAP theorem). A description of the characteristics of the data for each class is given. The developed big data classification is aimed at solving the problems of selecting tools for the development of an ecosystem. The practical significance of the results obtained is shown by the example of determining the type of big data of actual information systems.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Bogdanov</FamilyName>
<Email>bogdanov@csa.ru</Email>
<Affiliation>St.Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nadezhda</FirstName>
<FamilyName>Shchegoleva</FamilyName>
<Email>stil_hope@mail.ru</Email>
<Affiliation>
Saint Petersburg Electrotechnical University "LETI"
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Rukovchuk</FamilyName>
<Email>vrukovchuk@gmail.com</Email>
<Affiliation>Plekhanov Russian University of Economics</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Irina</FirstName>
<FamilyName>Ulitina</FamilyName>
<Email>hg.ulitina@yandex.ru</Email>
<Affiliation>St-Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nadezhda</FirstName>
<FamilyName>Shchegoleva</FamilyName>
<Email>stil_hope@mail.ru</Email>
<Affiliation>
Saint Petersburg Electrotechnical University "LETI"
</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
</abstract>
<abstract>
<Id>174</Id>
<Title>
Electronics upgrade for the CMS CSC muon system at the High Luminosity LHC
</Title>
<Content>
Cathode strip chambers (CSCs) are used to detect muons in the endcap region of the CMS detector. The High Luminosity LHC will present particular challenges to the electronics that read out the CSCs: the demands of increased particle flux, longer trigger latency, and increased trigger rate require upgraded electronics boards in the forward region. In particular, both the anode and cathode readout electronics will include full digitization at the beam crossing rate and data pipelining in deep digital FIFOs that provide nearly deadtimeless operation and the capability to accommodate long latency requirements without loss of data. High speed optical links will be used to increase the bandwidth for data between the on-chamber electronics and the back end. Motivated by experience with the present electronics and the expectations for high radiation conditions during operation of the HL-LHC, some novel features are incorporated into this second-generation electronics design to provide increased robustness. Radiation tolerant optical transceivers are used in all on-chamber applications. Due to low reliability of EPROMs after radiation doses in the range of 10 kRad, an alternate capability for programming the FPGAs is included via an asynchronous optical link that can complete the programming with comparable or better speed than the EPROM. We present the novel features of these electronics along with results from test stands and initial commissioning in CMS with cosmic rays.
</Content>
<field id="content">
Cathode strip chambers (CSCs) are used to detect muons in the endcap region of the CMS detector. The High Luminosity LHC will present particular challenges to the electronics that read out the CSCs: the demands of increased particle flux, longer trigger latency, and increased trigger rate require upgraded electronics boards in the forward region. In particular, both the anode and cathode readout electronics will include full digitization at the beam crossing rate and data pipelining in deep digital FIFOs that provide nearly deadtimeless operation and the capability to accommodate long latency requirements without loss of data. High speed optical links will be used to increase the bandwidth for data between the on-chamber electronics and the back end. Motivated by experience with the present electronics and the expectations for high radiation conditions during operation of the HL-LHC, some novel features are incorporated into this second-generation electronics design to provide increased robustness. Radiation tolerant optical transceivers are used in all on-chamber applications. Due to low reliability of EPROMs after radiation doses in the range of 10 kRad, an alternate capability for programming the FPGAs is included via an asynchronous optical link that can complete the programming with comparable or better speed than the EPROM. We present the novel features of these electronics along with results from test stands and initial commissioning in CMS with cosmic rays.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Isabelle</FirstName>
<FamilyName>De Bruyn</FamilyName>
<Email>isabelle.helena.j.de.bruyn@cern.ch</Email>
<Affiliation>University of Wisconsin - Madison</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Isabelle</FirstName>
<FamilyName>De Bruyn</FamilyName>
<Email>isabelle.helena.j.de.bruyn@cern.ch</Email>
<Affiliation>University of Wisconsin - Madison</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>175</Id>
<Title>
Results of the Radiation Dose Study around the GEM Muon Detectors at CMS
</Title>
<Content>
GEM (Gas Electron Multiplier) detectors are developed to measure the muon flux at the future HL (High Luminosity) LHC. A radiation monitoring system to control the dose absorbed by these detectors during the tests was designed. The system uses a basic detector unit, called RADMON. There are in each unit two types of sensors: RadFETs, measuring the total absorbed dose of all radiations and p-i-n diodes – for particle (proton and neutron) radiations. The system has a modular structure, permitting to increase easily the number of controlled RADMONs – one module controls up to 12 RADMONs. For the first test, a group of 3 GEM chambers called supermodule was installed at the inner CMS endcap at March 2017. One RADMON was placed inside of this supermodule for dose monitoring and through local system controller, it transfers the measured data to the test experiment data acquisition system. The real dose data, registered for this long period are now processing and the more important results will be presented to the NEC 2019.
</Content>
<field id="content">
GEM (Gas Electron Multiplier) detectors are developed to measure the muon flux at the future HL (High Luminosity) LHC. A radiation monitoring system to control the dose absorbed by these detectors during the tests was designed. The system uses a basic detector unit, called RADMON. There are in each unit two types of sensors: RadFETs, measuring the total absorbed dose of all radiations and p-i-n diodes – for particle (proton and neutron) radiations. The system has a modular structure, permitting to increase easily the number of controlled RADMONs – one module controls up to 12 RADMONs. For the first test, a group of 3 GEM chambers called supermodule was installed at the inner CMS endcap at March 2017. One RADMON was placed inside of this supermodule for dose monitoring and through local system controller, it transfers the measured data to the test experiment data acquisition system. The real dose data, registered for this long period are now processing and the more important results will be presented to the NEC 2019.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Lubomir</FirstName>
<FamilyName>Dimitrov</FamilyName>
<Email>ludim@inrne.bas.bg</Email>
<Affiliation>INRNE, BAS, Sofia</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Plamen</FirstName>
<FamilyName>Iaydjiev</FamilyName>
<Email>plamen.iaydjiev@cern.ch</Email>
<Affiliation>INRNE, BAS, Sofia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Georgi</FirstName>
<FamilyName>Mitev</FamilyName>
<Email>georgi.mitev@cern.ch</Email>
<Affiliation>INRNE, BAS, Sofia</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Ivan</FirstName>
<FamilyName>Vankov</FamilyName>
<Email>ivan.vankov@cern.ch</Email>
<Affiliation>INRNE, BAS, Sofia</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Ivan</FirstName>
<FamilyName>Vankov</FamilyName>
<Email>ivan.vankov@cern.ch</Email>
<Affiliation>INRNE, BAS, Sofia</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>176</Id>
<Title>
Outlier search in multidimensional data on engineering characteristics of Symmetra at IHEP IT center
</Title>
<Content>
An anomaly or an outlier in related multidimensional data can seriously affect on their analysis. Discarding anomalies or their ignoring may lead to incorrect results. A study was carried out by using the flexible PyOD tool which are designed for anomalies searching in multidimensional data. This study using various techniques or python software library modules provided by PyOD. The search goal was to discover anomalies in pair of indicators Ibat-TupsC on Symmetra UPS installed at IHEP IT center. This paper presents research results that will be used to further development of the system for searching anomalies in engineering equipment of the IHEP IT center.
</Content>
<field id="content">
An anomaly or an outlier in related multidimensional data can seriously affect on their analysis. Discarding anomalies or their ignoring may lead to incorrect results. A study was carried out by using the flexible PyOD tool which are designed for anomalies searching in multidimensional data. This study using various techniques or python software library modules provided by PyOD. The search goal was to discover anomalies in pair of indicators Ibat-TupsC on Symmetra UPS installed at IHEP IT center. This paper presents research results that will be used to further development of the system for searching anomalies in engineering equipment of the IHEP IT center.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Ekaterina</FirstName>
<FamilyName>Popova</FamilyName>
<Email>ekaterina.popova@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>177</Id>
<Title>
IPv6 dual-stack deployment for the distributed computing center.
</Title>
<Content>
Computing Center of the Institute for High Energy Physics in Protvino provides computing and storage resources for various HEP experiments (Atlas, CMS, Alice, LHCb) and currently operates more than 150 working nodes with around 2700 cores and provides near 2PB of disk space. All resources are connected through two 10Gb/s links to LHCONE and other research networks. IHEP computing center has IPv4 address space limited to one C-sized and all working nodes are installed behind the NAT which has some drawbacks for production use. To optimize routing, switching and to get higher network throughput for data transfer the IPv6 dual-stack deployment was made for the computing farm. In this work the full cycle of the real IPv6 dual-stack deployment from zero to production will be shown. This work can be used by other WLCG centers and all other data centers for distributed computing as an example of necessary steps and configurations which have to be made.
</Content>
<field id="content">
Computing Center of the Institute for High Energy Physics in Protvino provides computing and storage resources for various HEP experiments (Atlas, CMS, Alice, LHCb) and currently operates more than 150 working nodes with around 2700 cores and provides near 2PB of disk space. All resources are connected through two 10Gb/s links to LHCONE and other research networks. IHEP computing center has IPv4 address space limited to one C-sized and all working nodes are installed behind the NAT which has some drawbacks for production use. To optimize routing, switching and to get higher network throughput for data transfer the IPv6 dual-stack deployment was made for the computing farm. In this work the full cycle of the real IPv6 dual-stack deployment from zero to production will be shown. This work can be used by other WLCG centers and all other data centers for distributed computing as an example of necessary steps and configurations which have to be made.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>MIPT</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Anna</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>anna.kotliar@ihep.ru</Email>
<Affiliation>IHEP</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Viktor</FirstName>
<FamilyName>Kotliar</FamilyName>
<Email>victor.kotlyar@ihep.ru</Email>
<Affiliation>MIPT</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>178</Id>
<Title>Upgrade of the CMS Fast Beam Condition Monitor</Title>
<Content>
The CMS Beam Radiation Instrumentation and Luminosity (BRIL) project is responsible for the instrumentation in the CMS experiment to precisely measure luminosity, monitor beam-induced background, and give feedback on LHC beam conditions. During the currently ongoing Long Shutdown 2, one of these detectors — the fast beam condition monitor (BCM1F) — is undergoing an upgrade to improve performance and increase radiation tolerance. This includes an all new flex-rigid PCB carrying the sensors, read-out ASICs and onto-electronic transceivers. In addition, the presently installed polycrystalline diamond sensors will be upgraded to AC-coupled silicon-pad detectors with active cooling. This contribution will give an introduction to the BCM1F detector concept and upgrade, summarising key results that illustrate the performance during Run 2 of the LHC, where silicon sensors showed much improved linearity with respect to diamond in luminosity measurement. Furthermore, new results from the qualification of the new AC-coupled sensor prototypes in a beam test will allow estimation of the improvement in performance during the upcoming Run 3.
</Content>
<field id="content">
The CMS Beam Radiation Instrumentation and Luminosity (BRIL) project is responsible for the instrumentation in the CMS experiment to precisely measure luminosity, monitor beam-induced background, and give feedback on LHC beam conditions. During the currently ongoing Long Shutdown 2, one of these detectors — the fast beam condition monitor (BCM1F) — is undergoing an upgrade to improve performance and increase radiation tolerance. This includes an all new flex-rigid PCB carrying the sensors, read-out ASICs and onto-electronic transceivers. In addition, the presently installed polycrystalline diamond sensors will be upgraded to AC-coupled silicon-pad detectors with active cooling. This contribution will give an introduction to the BCM1F detector concept and upgrade, summarising key results that illustrate the performance during Run 2 of the LHC, where silicon sensors showed much improved linearity with respect to diamond in luminosity measurement. Furthermore, new results from the qualification of the new AC-coupled sensor prototypes in a beam test will allow estimation of the improvement in performance during the upcoming Run 3.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Ziheng</FirstName>
<FamilyName>Chen</FamilyName>
<Email>zihengchen2015@u.northwestern.edu</Email>
<Affiliation>Northwestern University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Georg</FirstName>
<FamilyName>Auzinger</FamilyName>
<Email>georg.auzinger@cern.ch</Email>
<Affiliation>CERN</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ziheng</FirstName>
<FamilyName>Chen</FamilyName>
<Email>zihengchen2015@u.northwestern.edu</Email>
<Affiliation>Northwestern University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>179</Id>
<Title>
Hit finder and track reconstruction algorithms in the Multi-Wire Proportional Chambers of BM@N experiment
</Title>
<Content>
BM@N (Baryonic Matter at Nuclotron) is an experiment being developed at Joint Institute for Nuclear Research (Dubna, Russia). It is considered the first step towards implementing the fixed target program at NICA accelerating complex (Nuclotron-based Ion Collider fAcillty). One of the important event reconstruction procedure components is the monitoring of the beam trajectory and the vertex position in transverse plane. A system consisting of two Multi-Wire Proportional Chambers (MWPC) is used for this purpose in BM@N. In this work we describe the hit finder and track reconstruction algorithms in the MWPC. Results of Monte-Carlo tests and efficiency calculations for different input parameters are presented. MWPC track analysis procedure of the BM@N experimental data from RUN-2018 is started and the first results are shown.
</Content>
<field id="content">
BM@N (Baryonic Matter at Nuclotron) is an experiment being developed at Joint Institute for Nuclear Research (Dubna, Russia). It is considered the first step towards implementing the fixed target program at NICA accelerating complex (Nuclotron-based Ion Collider fAcillty). One of the important event reconstruction procedure components is the monitoring of the beam trajectory and the vertex position in transverse plane. A system consisting of two Multi-Wire Proportional Chambers (MWPC) is used for this purpose in BM@N. In this work we describe the hit finder and track reconstruction algorithms in the MWPC. Results of Monte-Carlo tests and efficiency calculations for different input parameters are presented. MWPC track analysis procedure of the BM@N experimental data from RUN-2018 is started and the first results are shown.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Sergei</FirstName>
<FamilyName>Nemnyugin</FamilyName>
<Email>snemnyugin@mail.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergei</FirstName>
<FamilyName>Merts</FamilyName>
<Email>sergey.merts@gmail.com</Email>
<Affiliation>JINR, LHEP</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Vladimir</FirstName>
<FamilyName>Roudnev</FamilyName>
<Email>v.rudnev@spbu.ru</Email>
<Affiliation>St-Petersburg State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Margarita</FirstName>
<FamilyName>Stepanova</FamilyName>
<Email>mstep@mms.nw.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Sergei</FirstName>
<FamilyName>Nemnyugin</FamilyName>
<Email>snemnyugin@mail.ru</Email>
<Affiliation>Saint-Petersburg State University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>180</Id>
<Title>New fast simulation in ATLAS</Title>
<Content>
The ATLAS physics programme relies on very large samples of simulated events. Most of these samples are produced with GEANT4 which provides a detailed simulation of the ATLAS detector. However, this simulation is time and CPU consuming and the available resources will not allow to keep up the MC production with the luminosity increase foreseen by the LHC. To solve this problem, fast simulation tools are needed to replace the Geant4 base simulation. Unfortunately, the current fast simulation tools used in ATLAS are not accurate enough to be used by all analyses. Hence, the ATLAS collaboration is developing new fast calorimeter simulation tools (FastCaloSim) which use machine learning techniques, such as principal component analysis and deep neural networks. Prototypes for both approaches are being tested and validated; the new FastCaloSim showing a significant improvement in the description of cluster level variables in electromagnetic and hadronic showers over existing tools while the Deep Learning approaches are a promising R&D. To complement the new FastCaloSim, ATLAS is developing Fast Chain. This provides fast tools for the simulation of the rest of the ATLAS detector and the digitisation and reconstruction of the events. By combining these tools, ATLAS aims to have the capabilities to simulate the required numbers of events to achieve its physics goals. In this talk, we will describe the new FastCaloSim tool, new deep learning prototypes as well as the status of the ATLAS Fast Chain.
</Content>
<field id="content">
The ATLAS physics programme relies on very large samples of simulated events. Most of these samples are produced with GEANT4 which provides a detailed simulation of the ATLAS detector. However, this simulation is time and CPU consuming and the available resources will not allow to keep up the MC production with the luminosity increase foreseen by the LHC. To solve this problem, fast simulation tools are needed to replace the Geant4 base simulation. Unfortunately, the current fast simulation tools used in ATLAS are not accurate enough to be used by all analyses. Hence, the ATLAS collaboration is developing new fast calorimeter simulation tools (FastCaloSim) which use machine learning techniques, such as principal component analysis and deep neural networks. Prototypes for both approaches are being tested and validated; the new FastCaloSim showing a significant improvement in the description of cluster level variables in electromagnetic and hadronic showers over existing tools while the Deep Learning approaches are a promising R&D. To complement the new FastCaloSim, ATLAS is developing Fast Chain. This provides fast tools for the simulation of the rest of the ATLAS detector and the digitisation and reconstruction of the events. By combining these tools, ATLAS aims to have the capabilities to simulate the required numbers of events to achieve its physics goals. In this talk, we will describe the new FastCaloSim tool, new deep learning prototypes as well as the status of the ATLAS Fast Chain.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Michele</FirstName>
<FamilyName>Faucci Giannelli</FamilyName>
<Email>michele.faucci.giannelli@ed.ac.uk</Email>
<Affiliation>University of Edinburgh</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Michele</FirstName>
<FamilyName>Faucci Giannelli</FamilyName>
<Email>michele.faucci.giannelli@ed.ac.uk</Email>
<Affiliation>University of Edinburgh</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>181</Id>
<Title>
AstroDS — distributed storage for large astroparticle physics facilities
</Title>
<Content>
Currently, a number of experimental facilities in the field of particle astrophysics of the mega-siences class are being built and are already operating in the world. These include installations such as LSST (https://www.lsst.org/), MAGIC (https://doi.org/10.15161/oar.it/1446204371.89), CTA (https: //www.cta-observatory .org), VERITAS (https://veritas.sao.arizona.edu), HESS (https://www.mpi-hd.mpg.de/hfm/HESS/) and others. An important feature of this class of projects is the huge flow of data produced, the participation of many organizations and, as a result, the distributed nature of data processing and analysis. To meet similar requests in high energy physics, a WLCG grid was deployed as part of the LHC project (http://wlcg.web.cern.ch/). This solution, on the one hand, showed high efficiency, and, on the other hand, it turned out to be a rather heavy solution that requires high administrative costs, highly qualified staff and a very homogeneous environment on which applications operate. The success of the WLCG is based primarily on the fact that thousands of physicist users actually solve one global problem using a highly centralized system management. At present, in the field of astrophysics of particles, it has become necessary to provide access to distributed data storage resources of experiments without transferring local storages to a unified software storage environment, while preserving historically established data storage methods, their structure, as well as local access policies. The paper considers the architecture of a distributed storage for astrophysical experiments — AstroDS using the example of the KASCADE(https://doi.org/10.1016/j.nima.2010.03.147) and TAIGA (https://doi.org/10.1016/j.nima.2016.06.041) experiments. The main ideas of the proposed approach are as follows: • unification of access to local storage data without changing their structure based on corresponding adapter modules; • use of local data access policies; • data transfer only at the moment of actual access to them; • search and aggregation of the necessary data on user requests to metadata. A feature of the system is its orientation of storing source data as well as primary processed data, for example, data after calibration, using the Write-One-Read-Many method. It is assumed that data analysis will be performed on the user's local resources or specialized application servers. Adding data to local repositories should be done through special services that provide, among other things, semi-automatic collection of meta-information in the process of downloading new data. Collected metadata is used to search for user-requested data. At present, a prototype of the system has been deployed on the basis of the SINP MSU, which develop the technology of building distributed storages for particle astrophysics. This work was supported by the RSF Grant No. 18-41-06003.
</Content>
<field id="content">
Currently, a number of experimental facilities in the field of particle astrophysics of the mega-siences class are being built and are already operating in the world. These include installations such as LSST (https://www.lsst.org/), MAGIC (https://doi.org/10.15161/oar.it/1446204371.89), CTA (https: //www.cta-observatory .org), VERITAS (https://veritas.sao.arizona.edu), HESS (https://www.mpi-hd.mpg.de/hfm/HESS/) and others. An important feature of this class of projects is the huge flow of data produced, the participation of many organizations and, as a result, the distributed nature of data processing and analysis. To meet similar requests in high energy physics, a WLCG grid was deployed as part of the LHC project (http://wlcg.web.cern.ch/). This solution, on the one hand, showed high efficiency, and, on the other hand, it turned out to be a rather heavy solution that requires high administrative costs, highly qualified staff and a very homogeneous environment on which applications operate. The success of the WLCG is based primarily on the fact that thousands of physicist users actually solve one global problem using a highly centralized system management. At present, in the field of astrophysics of particles, it has become necessary to provide access to distributed data storage resources of experiments without transferring local storages to a unified software storage environment, while preserving historically established data storage methods, their structure, as well as local access policies. The paper considers the architecture of a distributed storage for astrophysical experiments — AstroDS using the example of the KASCADE(https://doi.org/10.1016/j.nima.2010.03.147) and TAIGA (https://doi.org/10.1016/j.nima.2016.06.041) experiments. The main ideas of the proposed approach are as follows: • unification of access to local storage data without changing their structure based on corresponding adapter modules; • use of local data access policies; • data transfer only at the moment of actual access to them; • search and aggregation of the necessary data on user requests to metadata. A feature of the system is its orientation of storing source data as well as primary processed data, for example, data after calibration, using the Write-One-Read-Many method. It is assumed that data analysis will be performed on the user's local resources or specialized application servers. Adding data to local repositories should be done through special services that provide, among other things, semi-automatic collection of meta-information in the process of downloading new data. Collected metadata is used to search for user-requested data. At present, a prototype of the system has been deployed on the basis of the SINP MSU, which develop the technology of building distributed storages for particle astrophysics. This work was supported by the RSF Grant No. 18-41-06003.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Minh-Duc</FirstName>
<FamilyName>Nguyen</FamilyName>
<Email>conqueror@dec1.sinp.msu.ru</Email>
<Affiliation>M.V.Lomonosov Moscow State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Igor</FirstName>
<FamilyName>Bychkov</FamilyName>
<Email>bychkov@icc.ru</Email>
<Affiliation>
Matrosov Institute for System Dynamics and Control Theory, SB RAS
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Mikhailov</FamilyName>
<Email>mikhailov@icc.ru</Email>
<Affiliation>
Matrosov Institute for System Dynamics and Control Theory, SB RAS
</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Andrey</FirstName>
<FamilyName>Shigarov</FamilyName>
<Email>shigarov@gmail.com</Email>
<Affiliation>
Matrosov Institute for System Dynamics and Control Theory, SB RAS
</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>182</Id>
<Title>
The use of CNN for image analysis from Cherenkov telescopes in the TAIGA experiment
</Title>
<Content>
The method of artificial neural networks is a modern powerful tool for solving various problems for which it is difficult to propose well-formalized solutions. These tasks are various aspects of image analysis. This paper describes the use of convolutional neural networks (CNN) for the problems of classifying the type of primary particles and estimating their energy using images obtained from the Cherenkov telescope (IACT) in the TAIGA experiment. For the problem of classifying primary particles, it has been shown that the use of CNN made it possible to significantly improve the quality criterion for the correct classification of gammas compared to traditional methods using the Hillas parameters. For the problem of estimating the energy of primary gammas, the use of CNN made it possible to obtain good results for wide air showers, whose centers are located far enough from the telescope. In particular, it is important for the Cherenkov telescope in the TAIGA experiment, which uses a wide-angle camera when traditional methods do not work. Neural networks were implemented using the PyTorch and TensorFlow libraries. Monte Carlo event sets obtained using the CORSIKA program were used to train CNN. CNN training was performed on both ordinary servers and servers equipped with Tesla P100 GPUs. This work was supported by the RSF Grant No. 18-41-06003.
</Content>
<field id="content">
The method of artificial neural networks is a modern powerful tool for solving various problems for which it is difficult to propose well-formalized solutions. These tasks are various aspects of image analysis. This paper describes the use of convolutional neural networks (CNN) for the problems of classifying the type of primary particles and estimating their energy using images obtained from the Cherenkov telescope (IACT) in the TAIGA experiment. For the problem of classifying primary particles, it has been shown that the use of CNN made it possible to significantly improve the quality criterion for the correct classification of gammas compared to traditional methods using the Hillas parameters. For the problem of estimating the energy of primary gammas, the use of CNN made it possible to obtain good results for wide air showers, whose centers are located far enough from the telescope. In particular, it is important for the Cherenkov telescope in the TAIGA experiment, which uses a wide-angle camera when traditional methods do not work. Neural networks were implemented using the PyTorch and TensorFlow libraries. Monte Carlo event sets obtained using the CORSIKA program were used to train CNN. CNN training was performed on both ordinary servers and servers equipped with Tesla P100 GPUs. This work was supported by the RSF Grant No. 18-41-06003.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Evgeny</FirstName>
<FamilyName>Postnikov</FamilyName>
<Email>evgeny.post@gmail.com</Email>
<Affiliation>SINP MSU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Stanislav</FirstName>
<FamilyName>Polyakov</FamilyName>
<Email>s.p.polyakov@gmail.com</Email>
<Affiliation>SINP MSU</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Dmitry</FirstName>
<FamilyName>Zhurov</FamilyName>
<Email>sidney28@ya.ru</Email>
<Affiliation>Irkutsk State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Kryukov</FamilyName>
<Email>kryukov@theory.sinp.msu.ru</Email>
<Affiliation>SINP MSU</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>183</Id>
<Title>Case of Cloud Designing and Development</Title>
<Content>
The designing and development of a computing clouds is complex process where numerous factors have to be taken into account. For example, size of planned cloud and potential growth, hardware/software platforms, flexible architecture, security, ease of maintenance. Computing cloud is quite often consisted of several data centers (DC). The DC is considered to be a group of hardware and/or virtual servers which is dedicated to run the user virtual machines (VM) and/or storage servers. Each pair of DCs may be interconnected by one or more virtual data transfer links. To manage such cloud to form “Infrastructure as a Service” (IaaS) a distributed operating management system (DOMS) is needed. The proposed architecture for DOMS is a set of software agents. Important advantages of such approach are flexibility, horizontal scalability, independent development/maintenance of any agent. The specially developed protocol to send and receive requests betwen agents is also discussed. Due to geographical distribution, the requirements for system stability in terms of hardware and software malfunctions are high. Proposed DOMS architecture is addressed operating stability as well. Observation of prototype consisting of several DCs in ~100Km distance from each other and practical results were presented. Potential application fields where this development might be used is also discussed.
</Content>
<field id="content">
The designing and development of a computing clouds is complex process where numerous factors have to be taken into account. For example, size of planned cloud and potential growth, hardware/software platforms, flexible architecture, security, ease of maintenance. Computing cloud is quite often consisted of several data centers (DC). The DC is considered to be a group of hardware and/or virtual servers which is dedicated to run the user virtual machines (VM) and/or storage servers. Each pair of DCs may be interconnected by one or more virtual data transfer links. To manage such cloud to form “Infrastructure as a Service” (IaaS) a distributed operating management system (DOMS) is needed. The proposed architecture for DOMS is a set of software agents. Important advantages of such approach are flexibility, horizontal scalability, independent development/maintenance of any agent. The specially developed protocol to send and receive requests betwen agents is also discussed. Due to geographical distribution, the requirements for system stability in terms of hardware and software malfunctions are high. Proposed DOMS architecture is addressed operating stability as well. Observation of prototype consisting of several DCs in ~100Km distance from each other and practical results were presented. Potential application fields where this development might be used is also discussed.
</field>
<field id="summary">
The number of geographically distributed data centres (DC) grows each year. Quite often new DCs are located in cold regions of the world to save money and energy for cooling. An important criterion for the location of DC's is the availability of electrical power. In some of the best locations, access to expert staff is limited. It is probably better to maintain DC without permanent staff. To manage such DCs, a distributed operating management system (DOMS) is needed. The DOMS must be able to control many virtual objects like virtual machines, virtual data links, virtual storage to form an IaaS cloud. Due to geographical distribution, the requirements for system stability in terms of hardware and software malfunctions are high. These are defined through specific Service Level Agreements (SLA). In our solution, the DC is considered to be a group of hardware and/or virtual servers which is dedicated to run the user virtual machines (VM) and/or storage servers. Each pair of DCs may be interconnected by one or more virtual data transfer links. The program system OpenVPN is used to provide data tunnels between DCs located far from each other. In designing the DOMS, we employ: •	Free and Open Source Software (FOSS) for DOMS components; •	Software Defined Networking (SDN); •	Software Defined Storage (SDS); •	Infrastructure as Code (IaC). The DOMS requires a specific combination of operating system, cloud system, storage organization and networking. Many aspects of the specific components selection are out of scope of this paper. DOMS uses the following essential software components: OpenStack as a cloud platform, Ceph as a storage subsystem, SaltStack as an IaC engine and many other program systems. The NauLinux (clone RHEL) is used as an operating system.
</field>
<PrimaryAuthor>
<FirstName>andrey</FirstName>
<FamilyName>shevel</FamilyName>
<Email>andrey.shevel@pnpi.spb.ru</Email>
<Affiliation>PNPI, ITMO</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Petr</FirstName>
<FamilyName>Fedchenkov</FamilyName>
<Email>pvfedchenkov@corp.ifmo.ru</Email>
<Affiliation>ITMO</Affiliation>
</Co-Author>
<Speaker>
<FirstName>andrey</FirstName>
<FamilyName>shevel</FamilyName>
<Email>andrey.shevel@pnpi.spb.ru</Email>
<Affiliation>PNPI, ITMO</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>184</Id>
<Title>Data processing and analysis for Baikal-GVD</Title>
<Content>
Baikal-GVD is a deep underwater gigaton-volume neutrino telescope currently under construction in Lake Baikal. The detector is a spatially distributed lattice of photomultipliers, designed to register Cherenkov radiation from the products of neutrino interactions with the water of the lake. When the trigger conditions are met, digitized photomultiplier waveforms are sent the shore, allowing for the reconstruction of energy and direction of the neutrino. We describe a data processing and analysis infrastructure that has been developed for the detector.
</Content>
<field id="content">
Baikal-GVD is a deep underwater gigaton-volume neutrino telescope currently under construction in Lake Baikal. The detector is a spatially distributed lattice of photomultipliers, designed to register Cherenkov radiation from the products of neutrino interactions with the water of the lake. When the trigger conditions are met, digitized photomultiplier waveforms are sent the shore, allowing for the reconstruction of energy and direction of the neutrino. We describe a data processing and analysis infrastructure that has been developed for the detector.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Alexander</FirstName>
<FamilyName>Avrorin</FamilyName>
<Email>a.d.avrorin@gmail.com</Email>
<Affiliation>INR RAS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Alexander</FirstName>
<FamilyName>Avrorin</FamilyName>
<Email>a.d.avrorin@gmail.com</Email>
<Affiliation>INR RAS</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Research Data Infrastructures</Track>
</abstract>
<abstract>
<Id>185</Id>
<Title>
Performance of the Pixel Luminosity Telescope for Luminosity Measurement at CMS during Run2
</Title>
<Content>
The Pixel Luminosity Telescope (PLT) is a dedicated system for luminosity measurement at the CMS experiment using silicon pixel sensors arranged into "telescopes", each consisting of three planes. It was installed in CMS at the beginning of 2015 and has been providing online and offline luminosity measurements throughout Run 2 of the LHC (2015-2018). The online bunch-by-bunch luminosity measurement reads out at the full bunch crossing rate of 40 MHz, using the "fast-or" capability of the pixel readout chip to identify events where a hit is registered in all three sensors in a telescope, corresponding primarily to tracks originating from the interaction point. In addition, the full pixel information is read out at a lower rate, allowing for studies with full track reconstruction. In this talk, we will present the results and techniques used during Run 2, including commissioning, luminosity calibration using Van der Meer scans, and measurement and correction of stability and linearity effects using data from emittance scans.
</Content>
<field id="content">
The Pixel Luminosity Telescope (PLT) is a dedicated system for luminosity measurement at the CMS experiment using silicon pixel sensors arranged into "telescopes", each consisting of three planes. It was installed in CMS at the beginning of 2015 and has been providing online and offline luminosity measurements throughout Run 2 of the LHC (2015-2018). The online bunch-by-bunch luminosity measurement reads out at the full bunch crossing rate of 40 MHz, using the "fast-or" capability of the pixel readout chip to identify events where a hit is registered in all three sensors in a telescope, corresponding primarily to tracks originating from the interaction point. In addition, the full pixel information is read out at a lower rate, allowing for studies with full track reconstruction. In this talk, we will present the results and techniques used during Run 2, including commissioning, luminosity calibration using Van der Meer scans, and measurement and correction of stability and linearity effects using data from emittance scans.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Francesco</FirstName>
<FamilyName>Romeo</FamilyName>
<Email>fromeo@cern.ch</Email>
<Affiliation>Vanderbilt University</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Francesco</FirstName>
<FamilyName>Romeo</FamilyName>
<Email>fromeo@cern.ch</Email>
<Affiliation>Vanderbilt University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>186</Id>
<Title>
Information Security Issues in a Distributed Computing Educational Environment
</Title>
<Content>
The intensive development of computing technology and distributed computing systems technologies has led to the emergence of new active and interactive forms of education in which students have the opportunity of wide access to electronic educational resources. At the same time, new threats and vulnerabilities have emerged, which can be classified into 3 groups: integrity and confidentiality of information in a distributed computing educational environment, protection of intellectual property of the electronic educational resources, and security of the learning management system. The report discusses the mechanisms for ensuring information security in a distributed computing educational environment. Keywords: distributed computing environment, education, information security, electronic educational resources
</Content>
<field id="content">
The intensive development of computing technology and distributed computing systems technologies has led to the emergence of new active and interactive forms of education in which students have the opportunity of wide access to electronic educational resources. At the same time, new threats and vulnerabilities have emerged, which can be classified into 3 groups: integrity and confidentiality of information in a distributed computing educational environment, protection of intellectual property of the electronic educational resources, and security of the learning management system. The report discusses the mechanisms for ensuring information security in a distributed computing educational environment. Keywords: distributed computing environment, education, information security, electronic educational resources
</field>
<field id="summary">
The report discusses the mechanisms for ensuring information security in a distributed computing educational environment.
</field>
<PrimaryAuthor>
<FirstName>Anatoly</FirstName>
<FamilyName>Minzov</FamilyName>
<Email>926-565-0570@mail.ru</Email>
<Affiliation>Dubna State University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Nadezhda</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@uni-dubna.ru</Email>
<Affiliation>Dubna State University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Evgenia</FirstName>
<FamilyName>Cheremisina</FamilyName>
<Email>chere@uni-dubna.ru</Email>
<Affiliation>Dubna State University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nadezhda</FirstName>
<FamilyName>Tokareva</FamilyName>
<Email>tokareva@uni-dubna.ru</Email>
<Affiliation>Dubna State University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Innovative IT Education</Track>
</abstract>
<abstract>
<Id>187</Id>
<Title>Distributed data processing of COMPASS experiment</Title>
<Content>
Implementation of COMPASS data processing in the distributed environment has started in 2015. Since the summer of 2017, data processing system works in production mode, distributing jobs to two traditional Grid sites: CERN and JINR. There are two storage elements at CERN: EOS for short-term storage and Castor for long-term storage. Processing management services are deployed at JINR Cloud Service. During last year, the system was replenished by FTS and Harvester services, and Monte-Carlo processing chain. Status, details of implementation, workflow, data management, statistics, and infrastructure overview are presented in this report.
</Content>
<field id="content">
Implementation of COMPASS data processing in the distributed environment has started in 2015. Since the summer of 2017, data processing system works in production mode, distributing jobs to two traditional Grid sites: CERN and JINR. There are two storage elements at CERN: EOS for short-term storage and Castor for long-term storage. Processing management services are deployed at JINR Cloud Service. During last year, the system was replenished by FTS and Harvester services, and Monte-Carlo processing chain. Status, details of implementation, workflow, data management, statistics, and infrastructure overview are presented in this report.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Artem</FirstName>
<FamilyName>Petrosyan</FamilyName>
<Email>artem.petrosyan@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>188</Id>
<Title>PIK Data Centre status update</Title>
<Content>
In the framework of the PIK nuclear reactor reconstruction project, a PIK Data Centre was commissioned in 2017. While the main purpose of the Centre is storage and processing of PIK experiments data, its capacity is also used by other scientific groups at PNPI and outside for solving problems in different areas of science such as computational biology and condensed matter physics. PIK Data Centre is an integral part of computing facilities of NRC "Kurchatov Institute". The PIK Computing Centre has a heterogeneous structure and consists of several types of computing nodes suitable for a wide range of tasks and two independent data storage systems, all of which are interconnected with a fast InfiniBand network. The engineering infrastructure provides redundant main power and two independent UPS installations for computing equipment and for cooling system. In this talk we will highlight the results and challenges of one year and a half of successful operation.
</Content>
<field id="content">
In the framework of the PIK nuclear reactor reconstruction project, a PIK Data Centre was commissioned in 2017. While the main purpose of the Centre is storage and processing of PIK experiments data, its capacity is also used by other scientific groups at PNPI and outside for solving problems in different areas of science such as computational biology and condensed matter physics. PIK Data Centre is an integral part of computing facilities of NRC "Kurchatov Institute". The PIK Computing Centre has a heterogeneous structure and consists of several types of computing nodes suitable for a wide range of tasks and two independent data storage systems, all of which are interconnected with a fast InfiniBand network. The engineering infrastructure provides redundant main power and two independent UPS installations for computing equipment and for cooling system. In this talk we will highlight the results and challenges of one year and a half of successful operation.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
<Track>
Computations with Hybrid Systems (CPU, GPU, coprocessors)
</Track>
</abstract>
<abstract>
<Id>189</Id>
<Title>
Federated storage initiatives at NRC "Kurchatov Institute"
</Title>
<Content>
Several R&D projects in Federated Storage techniques have emerged in the last years with a goal of exploring the evolution of distributed storage in Exabyte era, which is defined by storage demands of HL-LHC and several other international scientific collaborations. In this talk we will report on Federated Storage initiatives at NRC "Kurchatov Institute" including participation in the DataLake project and deployment of a storage federation for in-house computing resources.
</Content>
<field id="content">
Several R&D projects in Federated Storage techniques have emerged in the last years with a goal of exploring the evolution of distributed storage in Exabyte era, which is defined by storage demands of HL-LHC and several other international scientific collaborations. In this talk we will report on Federated Storage initiatives at NRC "Kurchatov Institute" including participation in the DataLake project and deployment of a storage federation for in-house computing resources.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Andrey</FirstName>
<FamilyName>Kiryanov</FamilyName>
<Email>globus@pnpi.nw.ru</Email>
<Affiliation>PNPI</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Research Data Infrastructures</Track>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>190</Id>
<Title>
Precision Luminosity Measurement with the CMS detector for HL-LHC
</Title>
<Content>
The High Luminosity upgrade of the LHC (HL-LHC) is foreseen to increase the instantaneous luminosity by a factor of five to seven times the LHC nominal design value. The resulting, unprecedented requirements for background monitoring and luminosity measurement create the need for new high-precision instrumentation at CMS, using radiation hard detector technologies. This contribution presents the strategy for bunch-by-bunch online luminosity measurement based on various detector technologies. A main component of the system is the Tracker Endcap Pixel Detector (TEPX) with an additional 75 kHz of dedicated triggers for online measurement of luminosity and beam-induced background. Real-time implementations of algorithms such as pixel cluster counting on an FPGA are explored for online processing of the resulting data. The potential of the exploitation of the Outer Tracker, the Hadron Forward calorimeter and muon trigger objects will also be discussed.
</Content>
<field id="content">
The High Luminosity upgrade of the LHC (HL-LHC) is foreseen to increase the instantaneous luminosity by a factor of five to seven times the LHC nominal design value. The resulting, unprecedented requirements for background monitoring and luminosity measurement create the need for new high-precision instrumentation at CMS, using radiation hard detector technologies. This contribution presents the strategy for bunch-by-bunch online luminosity measurement based on various detector technologies. A main component of the system is the Tracker Endcap Pixel Detector (TEPX) with an additional 75 kHz of dedicated triggers for online measurement of luminosity and beam-induced background. Real-time implementations of algorithms such as pixel cluster counting on an FPGA are explored for online processing of the resulting data. The potential of the exploitation of the Outer Tracker, the Hadron Forward calorimeter and muon trigger objects will also be discussed.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Gabriella</FirstName>
<FamilyName>Pasztor</FamilyName>
<Email>gabriella.pasztor@cern.ch</Email>
<Affiliation>MTA-ELTE &amp; Eötvös University, Budapest</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Gabriella</FirstName>
<FamilyName>Pasztor</FamilyName>
<Email>gabriella.pasztor@cern.ch</Email>
<Affiliation>MTA-ELTE &amp; Eötvös University, Budapest</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
</abstract>
<abstract>
<Id>191</Id>
<Title>
Anomaly detection and breakdown prediction in RF power source output: a review of approaches
</Title>
<Content>
Linear accelerators are complex machines potentially confronted with significant downtimes periods due to anomalies and subsequent breakdowns in one or more components. The need for reliable operations of linear accelerators is critical for the spread of this technique in medical environment. At CERN, where LINACs are used for particle research, similar issues are encountered, such as the appearance of jitters in plasma sources (2MHz RF generators), that can have significant impact on the subsequent beam quality in the accelerator. The “SmartLINAC” project was established as an effort to increase LINACs’ reliability by means of early anomaly detection and prediction in its operations, down to the component level. The research described in this article reviews the different techniques used to detect anomalies, from their earlier signals, using data from 2MHz RF generators. This research is an important step forward in the SmartLINAC project, but represents only its beginning. The authors used four different techniques in an effort to determine the most appropriate one to detect anomalies on the generators’ data. The main challenge came from the nature of the data having a high noise-to-signal ratio and presenting several kinds of anomalies from different sources, and from the lack of available exhaustive and precise labelling. The techniques are based on different approaches using machines learning and statistics. This research allowed us to understand better the nature of the data we are working with. Through it, we encountered characteristics present in the data we hadn’t forecast, allowing us to start addressing the project’s objectives, not only identifying and differentiating possible anomalies, but also forecasting to some extent potential breakdowns.
</Content>
<field id="content">
Linear accelerators are complex machines potentially confronted with significant downtimes periods due to anomalies and subsequent breakdowns in one or more components. The need for reliable operations of linear accelerators is critical for the spread of this technique in medical environment. At CERN, where LINACs are used for particle research, similar issues are encountered, such as the appearance of jitters in plasma sources (2MHz RF generators), that can have significant impact on the subsequent beam quality in the accelerator. The “SmartLINAC” project was established as an effort to increase LINACs’ reliability by means of early anomaly detection and prediction in its operations, down to the component level. The research described in this article reviews the different techniques used to detect anomalies, from their earlier signals, using data from 2MHz RF generators. This research is an important step forward in the SmartLINAC project, but represents only its beginning. The authors used four different techniques in an effort to determine the most appropriate one to detect anomalies on the generators’ data. The main challenge came from the nature of the data having a high noise-to-signal ratio and presenting several kinds of anomalies from different sources, and from the lack of available exhaustive and precise labelling. The techniques are based on different approaches using machines learning and statistics. This research allowed us to understand better the nature of the data we are working with. Through it, we encountered characteristics present in the data we hadn’t forecast, allowing us to start addressing the project’s objectives, not only identifying and differentiating possible anomalies, but also forecasting to some extent potential breakdowns.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Yann</FirstName>
<FamilyName>Donon</FamilyName>
<Email>yann.donon@ssau.ru</Email>
<Affiliation>Samara National research University</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Alexander</FirstName>
<FamilyName>Kupriyanov</FamilyName>
<Email>akupr@ssau.ru</Email>
<Affiliation>Samara National Research University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Alberto</FirstName>
<FamilyName>Di Meglio</FamilyName>
<Email>alberto.di.meglio@cern.ch</Email>
<Affiliation>CERN openlab</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Rustam</FirstName>
<FamilyName>Paringer</FamilyName>
<Email>rusparinger@gmail.com</Email>
<Affiliation>Samara National Research University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Pavel</FirstName>
<FamilyName>Serafimovich</FamilyName>
<Email>paulserch05@gmail.com</Email>
<Affiliation>Samara National Research University</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Syomic</FirstName>
<FamilyName>Sergey</FamilyName>
<Email>sergey.syomik@gamil.com</Email>
<Affiliation>Samara National Research University</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Yann</FirstName>
<FamilyName>Donon</FamilyName>
<Email>yann.donon@ssau.ru</Email>
<Affiliation>Samara National research University</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Triggering, Data Acquisition, Control Systems</Track>
<Track>Research Data Infrastructures</Track>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
<Track>Machine Learning Algorithms and Big Data Analytics</Track>
</abstract>
<abstract>
<Id>192</Id>
<Title>Containerized services for FEL data processing</Title>
<Content>
Modern Free Electron Laser (FEL) facilities generate huge amounts of data and require sophisticated and computationally expensive analysis. For example, recent experiments at European XFEL have generated more than 360 Tb of raw data in five days. Efficient analysis of this data is a challenging task which requires productive use of existing methods and software for data analysis over scalable computing infrastructure. Additional challenge is that different pieces of software are optimized for diverse computing architectures (parallel MPI computing, GPU, SMP and etc) and require various software environments. In this report we present our experience of setting up experimental data analysis workflow in containerized computing infrastructure. We take individual software packages for certain analysis steps to set up loosely coupled virtualized microservices and use Kubernetes software to orchestrate multiple containers as a scalable data processing workflow. This approach brings us flexibility in setting up software environments inside containers and allows easy parallelization of data processing.
</Content>
<field id="content">
Modern Free Electron Laser (FEL) facilities generate huge amounts of data and require sophisticated and computationally expensive analysis. For example, recent experiments at European XFEL have generated more than 360 Tb of raw data in five days. Efficient analysis of this data is a challenging task which requires productive use of existing methods and software for data analysis over scalable computing infrastructure. Additional challenge is that different pieces of software are optimized for diverse computing architectures (parallel MPI computing, GPU, SMP and etc) and require various software environments. In this report we present our experience of setting up experimental data analysis workflow in containerized computing infrastructure. We take individual software packages for certain analysis steps to set up loosely coupled virtualized microservices and use Kubernetes software to orchestrate multiple containers as a scalable data processing workflow. This approach brings us flexibility in setting up software environments inside containers and allows easy parallelization of data processing.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Anton</FirstName>
<FamilyName>Teslyuk</FamilyName>
<Email>anthony.teslyuk@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Sergey</FirstName>
<FamilyName>Bobkov</FamilyName>
<Email>thereisalogin@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Viacheslav</FirstName>
<FamilyName>Ilyin</FamilyName>
<Email>ilyin0048@gmail.com</Email>
<Affiliation>National Research Center "Kurchatov Institute"</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Anton</FirstName>
<FamilyName>Teslyuk</FamilyName>
<Email>anthony.teslyuk@grid.kiae.ru</Email>
<Affiliation>NRC "Kurchatov Institute"</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>
Computing for Large Scale Facilities (LHC, FAIR, NICA, SKA, PIC, XFEL, ELI, etc.)
</Track>
</abstract>
<abstract>
<Id>193</Id>
<Title>Cloud integration within DIRAC Interware</Title>
<Content>
Computing clouds are widely used by many organizations in science, business, and industry. They provide flexible access to various physical computing resources. In addition computing clouds allows better resource utilization. Today, many scientific organizations have their own private cloud used for both: hosting services and performing computations. In many cases, private clouds are not 100% loaded and could be used as work nodes for distributed computations. If connect clouds together it would be possible to use them together for computational tasks. So the idea to integrate several private clouds appeared. Cloud bursting approach may be used for the integration of resources. But in order to provide access to the united cloud for all participants, the extensive configuration would be required on all clouds. We studied the possibility to unite clouds by integrating them using distributed workload management system – DIRAC Interware. Two approaches for virtual machines spawning were evaluated: usage of OCCI interface and OpenNebula RPC interface. They were tested and both approaches allowed to complete computing jobs on various clouds based on OpenNebula interware.
</Content>
<field id="content">
Computing clouds are widely used by many organizations in science, business, and industry. They provide flexible access to various physical computing resources. In addition computing clouds allows better resource utilization. Today, many scientific organizations have their own private cloud used for both: hosting services and performing computations. In many cases, private clouds are not 100% loaded and could be used as work nodes for distributed computations. If connect clouds together it would be possible to use them together for computational tasks. So the idea to integrate several private clouds appeared. Cloud bursting approach may be used for the integration of resources. But in order to provide access to the united cloud for all participants, the extensive configuration would be required on all clouds. We studied the possibility to unite clouds by integrating them using distributed workload management system – DIRAC Interware. Two approaches for virtual machines spawning were evaluated: usage of OCCI interface and OpenNebula RPC interface. They were tested and both approaches allowed to complete computing jobs on various clouds based on OpenNebula interware.
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nikita</FirstName>
<FamilyName>Balashov</FamilyName>
<Email>balashov.nikita@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Ruslan</FirstName>
<FamilyName>Kuchumov</FamilyName>
<Email>kuchumovri@gmail.com</Email>
<Affiliation>Saint Petersburg State University</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Nikolay</FirstName>
<FamilyName>Kutovskiy</FamilyName>
<Email>kut@jinr.ru</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</PrimaryAuthor>
<PrimaryAuthor>
<FirstName>Andrei</FirstName>
<FamilyName>Tsaregorodtsev</FamilyName>
<Email>atsareg@in2p3.fr</Email>
<Affiliation>CPPM-IN2P3-CNRS</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Igor</FirstName>
<FamilyName>Pelevanyuk</FamilyName>
<Email>gavelock@gmail.com</Email>
<Affiliation>JINR</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Distributed Computing. GRID & Cloud Computing</Track>
</abstract>
<abstract>
<Id>194</Id>
<Title>
GEM detectors for the Upgrade of the CMS Muon Forward system
</Title>
<Content>
The CMS experiment is one of the two general purpose experiments at the LHC pp collider. For LHC Phase-2, the instantaneous luminosity delivered to the experiment will reach 5 × 1034 cm−2s−1, resulting in high particle fluxes that requires the detectors to be upgraded. The forward regions, corresponding to the endcaps of the detectors, are the most affected parts. In the CMS experiment, to cope with the higher event rates and larger radiation doses, triple-layer Gas Electron Multipliers (GEM) will be installed in the muon endcaps. Triple-GEM chambers will complement the existing Cathode Strip Chambers, leading to a better identification of the muon tracks and a reduction of the trigger rate due to the suppression of fake candidates. In addition, the forward coverage will be further extended. For the first ring of the muon endcaps, 144 GEM chambers are being built in production sites spread in 7 countries around the world. For the first time, such detectors will have large sizes of the order of 1 m2, thus high requirements on the uniformity across the detector are needed. Before the final installation in the CMS detector, to test their integrity, quality and performance, the GEM chambers undergo multiple quality control tests. This talk gives an introduction to GEM detectors and presents results of the performance tests.
</Content>
<field id="content">
The CMS experiment is one of the two general purpose experiments at the LHC pp collider. For LHC Phase-2, the instantaneous luminosity delivered to the experiment will reach 5 × 1034 cm−2s−1, resulting in high particle fluxes that requires the detectors to be upgraded. The forward regions, corresponding to the endcaps of the detectors, are the most affected parts. In the CMS experiment, to cope with the higher event rates and larger radiation doses, triple-layer Gas Electron Multipliers (GEM) will be installed in the muon endcaps. Triple-GEM chambers will complement the existing Cathode Strip Chambers, leading to a better identification of the muon tracks and a reduction of the trigger rate due to the suppression of fake candidates. In addition, the forward coverage will be further extended. For the first ring of the muon endcaps, 144 GEM chambers are being built in production sites spread in 7 countries around the world. For the first time, such detectors will have large sizes of the order of 1 m2, thus high requirements on the uniformity across the detector are needed. Before the final installation in the CMS detector, to test their integrity, quality and performance, the GEM chambers undergo multiple quality control tests. This talk gives an introduction to GEM detectors and presents results of the performance tests.
</field>
<field id="summary">
This talk gives an introduction to GEM detectors and presents results of the performance tests.
</field>
<PrimaryAuthor>
<FirstName>Ulrich</FirstName>
<FamilyName>Goerlach</FamilyName>
<Email>ulrich.goerlach@iphc.cnrs.fr</Email>
<Affiliation>UNISTRA</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ulrich</FirstName>
<FamilyName>Goerlach</FamilyName>
<Email>ulrich.goerlach@iphc.cnrs.fr</Email>
<Affiliation>UNISTRA</Affiliation>
</Speaker>
<ContributionType>Plenary</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>195</Id>
<Title>
CMS Drift Tubes at High-Luminosity LHC: chamber longevity and upgrade of the detector electronics
</Title>
<Content>
Drift Tubes (DT) equip the barrel region of the CMS muon spectrometer serving both as tracking and triggering detector. At High-Luminosity LHC (HL-LHC) they will be challenged to operate at background rates and withstand integrated doses well beyond the specifications for which they were initially designed. Longevity studies show that, though a certain degree of ageing is expected, a replacement of the DT chambers is not needed for CMS to operate successfully at HL-LHC. On the other hand, the on-board readout and trigger electronics which presently equip the chambers are not expected to cope with the harsh HL-LHC conditions. For this reason, they will be replaced with time-to-digital converters (TDCs) streaming hits to a back-end electronics system where trigger segments reconstruction and readout event matching will be performed. This new architecture will allow to operate local reconstruction on the trigger electronics exploiting the full detector granularity and the ultimate DT cell resolution. Already over the second LHC long shutdown, a slice-test system consisting of four DT chambers will operate using the upgraded electronics, as an early test of the HL-LHC DT setup. In this document we outline the present knowledge about the DT detector longevity. Furthermore, we describe the prototype electronics and back-end demonstrators, as well as the state of the art of the local trigger algorithms that are being designed to run in the upgraded DT system. Performance measurements of the upgraded DT trigger, based on simulations, will be presented, highlighting their impact on the CMS muon trigger at large. The status of the operation of the DT slice-test will be also covered, with emphasis on the status of the implementation of the trigger algorithms in hardware.
</Content>
<field id="content">
Drift Tubes (DT) equip the barrel region of the CMS muon spectrometer serving both as tracking and triggering detector. At High-Luminosity LHC (HL-LHC) they will be challenged to operate at background rates and withstand integrated doses well beyond the specifications for which they were initially designed. Longevity studies show that, though a certain degree of ageing is expected, a replacement of the DT chambers is not needed for CMS to operate successfully at HL-LHC. On the other hand, the on-board readout and trigger electronics which presently equip the chambers are not expected to cope with the harsh HL-LHC conditions. For this reason, they will be replaced with time-to-digital converters (TDCs) streaming hits to a back-end electronics system where trigger segments reconstruction and readout event matching will be performed. This new architecture will allow to operate local reconstruction on the trigger electronics exploiting the full detector granularity and the ultimate DT cell resolution. Already over the second LHC long shutdown, a slice-test system consisting of four DT chambers will operate using the upgraded electronics, as an early test of the HL-LHC DT setup. In this document we outline the present knowledge about the DT detector longevity. Furthermore, we describe the prototype electronics and back-end demonstrators, as well as the state of the art of the local trigger algorithms that are being designed to run in the upgraded DT system. Performance measurements of the upgraded DT trigger, based on simulations, will be presented, highlighting their impact on the CMS muon trigger at large. The status of the operation of the DT slice-test will be also covered, with emphasis on the status of the implementation of the trigger algorithms in hardware.
</field>
<field id="summary">
The status of the operation of the DT slice-test will be also covered, with emphasis on the status of the implementation of the trigger algorithms in hardware.
</field>
<PrimaryAuthor>
<FirstName>Ulrich</FirstName>
<FamilyName>Goerlach</FamilyName>
<Email>ulrich.goerlach@iphc.cnrs.fr</Email>
<Affiliation>UNISTRA</Affiliation>
</PrimaryAuthor>
<Speaker>
<FirstName>Ulrich</FirstName>
<FamilyName>Goerlach</FamilyName>
<Email>ulrich.goerlach@iphc.cnrs.fr</Email>
<Affiliation>UNISTRA</Affiliation>
</Speaker>
<ContributionType>None</ContributionType>
<Track>Detector & Nuclear Electronics</Track>
</abstract>
<abstract>
<Id>196</Id>
<Title>
Cloud based distributed scientific computing infrastructure for research and education
</Title>
<Content>
The main focus of the advanced research infrastructures is to offer huge volumes of data to international research collaborations with hundreds of participants and their simultaneous need for unique, extremely high-performance computing facilities and sharing data through intense use of the cloud computing paradigm. GÉANT Association, together with its National Research and Education Network (NREN) partners, provides users with unmatched, highly reliable high-speed connectivity layer with very fast access to data all over the world. GÉANT – NREN ecosystem services offer for the research and education community convenient, fast and reliable security access to European High-Performance Computing (HPC) facilities, cloud services from European Open Science Cloud (EOSC) Service Catalogue, access to scientific data and publications. The next stage of development will be ensured by: •	increasing of data transfer rate and throughput of a scientific and educational network by restructuring the GEANT backbone and regional networks ensuring connectivity speeds up to 100 Gbps to smaller European NRENs including to Eastern Europe NRENs from Eastern Partnership Programme; •	provisioning of virtual channels between compute nodes with guaranteed bandwidth; • improving connections between European communities and third countries / regions enabling international research and education and advancing Open Science policy; •	maintaining collection and evaluation of user requirements for future extreme services with the large research infrastructure projects (e.g. CERN LHC2, SKA, ESS, ITER, ELI, Exascale) •	implementation of distributed hybrid European Open Science Cloud (EOSC) with high quality of service (QoS) networking support that is preparing for integration in future to GEANT heterogenous cloud infrastructure where NRENS (including Eastern Europe NRENs) will provide resources and integrated NRENs – GEANT networking platform will support adaptive / elastic connectivity. The article describes approaches and results of advanced research e-Infrastructure implementation in Research and Educational Networking Association of Moldova (RENAM) as a part of regional e-Infrastructure that is developing due to support of EU GN4-3 and EaPConnect projects by the Eastern Europe NRENs. Key words: GEANT-NREN e-infrastructure, regional e-Infrastructure & services, HPC, integrated GEANT-NRENs Cloud Initiative, Open Science, EOSC
</Content>
<field id="content">
The main focus of the advanced research infrastructures is to offer huge volumes of data to international research collaborations with hundreds of participants and their simultaneous need for unique, extremely high-performance computing facilities and sharing data through intense use of the cloud computing paradigm. GÉANT Association, together with its National Research and Education Network (NREN) partners, provides users with unmatched, highly reliable high-speed connectivity layer with very fast access to data all over the world. GÉANT – NREN ecosystem services offer for the research and education community convenient, fast and reliable security access to European High-Performance Computing (HPC) facilities, cloud services from European Open Science Cloud (EOSC) Service Catalogue, access to scientific data and publications. The next stage of development will be ensured by: •	increasing of data transfer rate and throughput of a scientific and educational network by restructuring the GEANT backbone and regional networks ensuring connectivity speeds up to 100 Gbps to smaller European NRENs including to Eastern Europe NRENs from Eastern Partnership Programme; •	provisioning of virtual channels between compute nodes with guaranteed bandwidth; • improving connections between European communities and third countries / regions enabling international research and education and advancing Open Science policy; •	maintaining collection and evaluation of user requirements for future extreme services with the large research infrastructure projects (e.g. CERN LHC2, SKA, ESS, ITER, ELI, Exascale) •	implementation of distributed hybrid European Open Science Cloud (EOSC) with high quality of service (QoS) networking support that is preparing for integration in future to GEANT heterogenous cloud infrastructure where NRENS (including Eastern Europe NRENs) will provide resources and integrated NRENs – GEANT networking platform will support adaptive / elastic connectivity. The article describes approaches and results of advanced research e-Infrastructure implementation in Research and Educational Networking Association of Moldova (RENAM) as a part of regional e-Infrastructure that is developing due to support of EU GN4-3 and EaPConnect projects by the Eastern Europe NRENs. Key words: GEANT-NREN e-infrastructure, regional e-Infrastructure & services, HPC, integrated GEANT-NRENs Cloud Initiative, Open Science, EOSC
</field>
<field id="summary"/>
<PrimaryAuthor>
<FirstName>Nicolai</FirstName>
<FamilyName>Iliuha</FamilyName>
<Email>nicolai.iliuha@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</PrimaryAuthor>
<Co-Author>
<FirstName>Peter</FirstName>
<FamilyName>Bogatencov</FamilyName>
<Email>bogatencov@renam.md</Email>
<Affiliation>RENAM, Moldova</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Grigore</FirstName>
<FamilyName>Secrieru</FamilyName>
<Email>secrieru@renam.md</Email>
<Affiliation>Vasile</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Nichita</FirstName>
<FamilyName>Degteariov</FamilyName>
<Email>ndegteariov@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Co-Author>
<Co-Author>
<FirstName>Grigorii</FirstName>
<FamilyName>Horos</FamilyName>
<Email>grigorii.horos@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Co-Author>
<Speaker>
<FirstName>Nicolai</FirstName>
<FamilyName>Iliuha</FamilyName>
<Email>nicolai.iliuha@renam.md</Email>
<Affiliation>RENAM</Affiliation>
</Speaker>
<ContributionType>Sectional</ContributionType>
<Track>Research Data Infrastructures</Track>
</abstract>
</AbstractBook>
